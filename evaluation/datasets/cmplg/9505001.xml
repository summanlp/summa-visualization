<?xml version='1.0'?>
<!DOCTYPE MINIMAL-DOC SYSTEM "mini.dtd">
<MINIMAL-DOC>
<TITLE>Response Generation in Collaborative Negotiation1</TITLE>
<ABSTRACT>
<P>
In collaborative planning activities, since the agents are autonomous
and heterogeneous, it is inevitable that conflicts arise in their
beliefs during the planning process. In cases where such conflicts are
relevant to the task at hand, the agents should engage in collaborative negotiation as an attempt to square away the
discrepancies in their beliefs. This paper presents a computational
strategy for detecting conflicts regarding proposed beliefs and for
engaging in collaborative negotiation to resolve the conflicts that
warrant resolution. Our model is capable of selecting the most
effective aspect to address in its pursuit of conflict resolution in
cases where multiple conflicts arise, and of selecting appropriate
evidence to justify the need for such modification. Furthermore, by
capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions, our model can successfully
handle embedded negotiation subdialogues.
</P>
</ABSTRACT>
<BODY>
<DIV ID="1" DEPTH="1" R-NO="1"><HEADER>  Introduction </HEADER>
<P>
In collaborative consultation dialogues, the consultant and the
executing agent collaborate on developing a plan to achieve the
executing agent's domain goal. Since agents are autonomous and
heterogeneous, it is inevitable that conflicts in their beliefs arise
during the planning process. In such cases, collaborative agents
 should attempt to square away <REF/> the conflicts by engaging in collaborative negotiation to determine what should
constitute their shared plan of actions and shared beliefs.
Collaborative negotiation differs from non-collaborative negotiation
and argumentation mainly in the attitude of the participants,
since collaborative agents are not self-centered, but act in a way as
to benefit the agents as a group. Thus, when facing a conflict, a
collaborative agent should not automatically reject a belief with
which she does not agree; instead, she should evaluate the belief and
the evidence provided to her and adopt the belief if the evidence is
convincing.  On the other hand, if the evaluation indicates that the
agent should maintain her original belief, she should attempt to
provide sufficient justification to convince the other agent to adopt
this belief if the belief is relevant to the task at hand.
</P>
<P>
This paper presents a model for engaging in collaborative negotiation
to resolve conflicts in agents' beliefs about domain knowledge. Our
model 1) detects conflicts in beliefs and initiates a negotiation
subdialogue only when the conflict is relevant to the current task, 2)
selects the most effective aspect to address in its pursuit of
conflict resolution when multiple conflicts exist, 3) selects
appropriate evidence to justify the system's proposed modification of
the user's beliefs, and 4) captures the negotiation process in a
recursive Propose-Evaluate-Modify cycle of actions, thus
enabling the system to handle embedded negotiation subdialogues.
</P>
</DIV>
<DIV ID="2" DEPTH="1" R-NO="2"><HEADER>  Related Work </HEADER>
<P>
Researchers have studied the analysis and generation of arguments
 <REF/>,<REF/>,<REF/>,<REF/>,<REF/>,<REF/>; however, agents engaging in argumentative dialogues are solely
interested in winning an argument and thus exhibit different behavior
from collaborative agents. Sidner sid_aaaiws92,sid_aaai94
formulated an artificial language for modeling collaborative discourse
using proposal/acceptance and proposal/rejection sequences; however,
her work is descriptive and does not specify response generation
strategies for agents involved in collaborative interactions.
</P>
<P>
Webber and Joshi web_jos_coling82 have noted the
importance of a cooperative system providing support for its
responses. They identified strategies that a system can adopt in
justifying its beliefs; however, they did not specify the criteria
under which each of these strategies should be selected. Walker
wal_coling94 described a method of determining when to
include optional warrants to justify a claim based on factors such as
communication cost, inference cost, and cost of memory
retrieval. However, her model focuses on determining when to include
informationally redundant utterances, whereas our model determines
whether or not justification is needed for a claim to be convincing
and, if so, selects appropriate evidence from the system's private
beliefs to support the claim.
</P>
<P>
 Caswey et al. <REF/>,<REF/> introduced the idea  of utilizing a belief revision mechanism <REF/> to predict whether a set of evidence is sufficient to change a user's existing
belief and to generate responses for information retrieval dialogues
in a library domain. They argued that in the library dialogues they
analyzed, ``in no cases does negotiation extend beyond the initial
 belief conflict and its immediate resolution.'' <REF/>.  However, our analysis of naturally-occurring
 consultation dialogues <REF/>,<REF/> shows that in other domains conflict resolution does extend beyond a single exchange of
conflicting beliefs; therefore we employ a recursive model for
collaboration that captures extended negotiation and represents the
structure of the discourse.  Furthermore, their system deals with a
single conflict, while our model selects a focus in its pursuit of
conflict resolution when multiple conflicts arise. In addition, we
provide a process for selecting among multiple possible pieces of
evidence.
</P>
</DIV>
<DIV ID="3" DEPTH="1" R-NO="3"><HEADER>  Features of Collaborative Negotiation </HEADER>
<P>
Collaborative negotiation occurs when conflicts arise among agents
 developing a shared plan during collaborative planning. A collaborative agent is driven by the goal of developing a
plan that best satisfies the interests of all the agents as a group,
instead of one that maximizes his own interest. This results in
several distinctive features of collaborative negotiation: 1) A
collaborative agent does not insist on winning an argument, and may
change his beliefs if another agent presents convincing justification
for an opposing belief. This differentiates collaborative negotiation
from argumentation
 <REF/>,<REF/>,<REF/>,<REF/>. 2) Agents involved in collaborative negotiation are open and honest with one
another; they will not deliberately present false information to other
agents, present information in such a way as to mislead the other
agents, or strategically hold back information from other agents for
later use. This distinguishes collaborative negotiation from
non-collaborative negotiation such as labor negotiation
 <REF/>. 3) Collaborative agents are interested in others' beliefs in order to decide whether to revise their own beliefs so as
 to come to agreement <REF/>. Although agents involved in argumentation and non-collaborative negotiation take other agents'
beliefs into consideration, they do so mainly to find weak points in
their opponents' beliefs and attack them to win the argument.
</P>
<P>
In our earlier work, we built on Sidner's proposal/acceptance and
 proposal/rejection sequences <REF/> and developed a model  that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions <REF/>. This model views collaborative planning as agent A proposing a set of
actions and beliefs to be incorporated into the shared plan being
developed, agent B evaluating the proposal to determine whether
or not he accepts the proposal and, if not, agent B proposing a set of
modifications to A's original proposal. The proposed
modifications will again be evaluated by A, and if conflicts arise,
she may propose modifications to B's previously proposed
modifications, resulting in a recursive process.  However, our
research did not specify, in cases where multiple conflicts arise, how
an agent should identify which part of an unaccepted proposal to
address or how to select evidence to support the proposed
modification. This paper extends that work by incorporating into the
modification process a strategy to determine the aspect of the
proposal that the agent will address in her pursuit of conflict
resolution, as well as a means of selecting appropriate evidence to
justify the need for such modification.
</P>
</DIV>
<DIV ID="4" DEPTH="1" R-NO="4"><HEADER>  Response Generation in Collaborative Negotiation </HEADER>
<P>
In order to capture the agents' intentions conveyed by their
utterances, our model of collaborative negotiation utilizes an
enhanced version of the dialogue model described in
 <REF/> to represent the current status of the interaction. The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for
the user's later execution, the problem-solving level which
contains the actions being performed to construct the domain plan, the
belief level which consists of the mutual beliefs pursued during
the planning process in order to further the problem-solving
intentions, and the discourse level which contains the
communicative actions initiated to achieve the mutual beliefs
 <REF/>. This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for
engaging in collaborative negotiations.
</P>
<DIV ID="4.1" DEPTH="2" R-NO="1"><HEADER>    Evaluating Proposed Beliefs
</HEADER>
<P>
Our system maintains a set of beliefs about the domain and about the
user's beliefs. Associated with each belief is a strength that
represents the agent's confidence in holding that belief. We model the
strength of a belief using endorsements, which are explicit
records of factors that affect one's certainty in a hypothesis
 <REF/>, following <REF/>,<REF/>. Our endorsements are based on the semantics of the utterance used to
convey a belief, the level of expertise of the agent conveying the
belief, stereotypical knowledge, etc.
</P>
<P>
The belief level of the dialogue model consists of mutual beliefs
proposed by the agents' discourse actions. When an agent proposes a
new belief and gives (optional) supporting evidence for it, this set
of proposed beliefs is represented as a belief tree, where the belief
represented by a child node is intended to support that represented by
its parent. The root nodes of these belief trees (top-level beliefs)
contribute to problem-solving actions and thus affect the domain plan
being developed. Given a set of newly proposed beliefs, the system
must decide whether to accept the proposal or to initiate a
negotiation dialogue to resolve conflicts.  The evaluation of proposed
beliefs starts at the leaf nodes of the proposed belief trees since
acceptance of a piece of proposed evidence may affect acceptance of
the parent belief it is intended to support. The process continues
until the top-level proposed beliefs are evaluated. Conflict
resolution strategies are invoked only if the top-level proposed
beliefs are not accepted because if collaborative agents agree on a
belief relevant to the domain plan being constructed, it is irrelevant
whether they agree on the evidence for that belief
 <REF/>. 
</P>
<P>
In determining whether to accept a proposed belief or evidential
relationship, the evaluator first constructs an evidence set
containing the system's evidence that supports or attacks _bel and
the evidence accepted by the system that was proposed by the user as
support for _bel. Each piece of evidence contains a belief _beli, and an evidential relationship supports(_beli,_bel). Following Walker's weakest link
 assumption <REF/> the strength of the evidence is the weaker of the strength of the belief and the strength of the
evidential relationship. The evaluator then employs a simplified
 version of Galliers' belief revision mechanism,<REF/> to compare the strengths of the evidence that supports and attacks _bel. If the strength of one set of
evidence strongly outweighs that of the other, the decision to accept
or reject _bel is easily made. However, if the difference in their
strengths does not exceed a pre-determined threshold, the evaluator
has insufficient information to determine whether to adopt _bel and
therefore will initiate an information-sharing subdialogue
 <REF/> to share information with the user so that each of them can knowledgably re-evaluate the user's original proposal. If,
during information-sharing, the user provides convincing support for a
belief whose negation is held by the system, the system may adopt the
belief after the re-evaluation process, thus resolving the conflict
without negotiation.
</P>
<DIV ID="4.1.1" DEPTH="3" R-NO="1"><HEADER>    Example
</HEADER>
<P>
To illustrate the evaluation of proposed beliefs, consider the
following utterances:
S:S:
   #2U:
</P>
<P>
   to (5) #1
  I think Dr. Smith is teaching AI next semester. 
</P>
<P>
   to (6) #2
  Dr. Smith is not teaching AI. 
</P>
<P>
   to (7)
  He is going on sabbatical next year. 
</P>
<P>
 Figure <CREF/> shows the belief and discourse levels of  the dialogue model that captures utterances (<CREF/>) and  (<CREF/>). The belief evaluation process will start with the belief at the leaf node of the proposed belief tree, On-Sabbatical(Smith,next year)). The system will first gather its
evidence pertaining to the belief, which includes 1) a warranted
 belief that Dr. Smith has postponed his sabbatical until 1997 (Postponed-Sabbatical(Smith,1997)), 2) a warranted belief that
Dr. Smith postponing his sabbatical until 1997 supports the belief
that he is not going on sabbatical next year (supports(Postponed-Sabbatical(Smith,1997),
<EQN/>On-Sabbatical(Smith,next year)), 3) a strong belief that
Dr. Smith will not be a visitor at IBM next year (<EQN/>visitor(Smith, IBM, next year)), and 4) a warranted belief
that Dr. Smith not being a visitor at IBM next year supports the
belief that he is not going on sabbatical next year (supports(<EQN/>visitor(Smith, IBM, next year),
<EQN/>On-Sabbatical(Smith, next year)), perhaps because Dr. Smith
has expressed his desire to spend his sabbatical only at IBM). The
belief revision mechanism will then be invoked to determine the
system's belief about On-Sabbatical(Smith, next year) based on
the system's own evidence and the user's statement. Since beliefs (1)
and (2) above constitute a warranted piece of evidence against the
proposed belief and beliefs (3) and (4) constitute a strong piece of
evidence against it, the system will not accept On-Sabbatical(Smith, next year).
</P>
<P>
The system believes that being on sabbatical implies a faculty member
is not teaching any courses; thus the proposed evidential relationship
will be accepted.  However, the system will not accept the top-level
proposed belief, <EQN/>Teaches(Smith, AI), since the system has
a prior belief to the contrary (as expressed in utterance (1)) and the
only evidence provided by the user was an implication whose antecedent
was not accepted.
</P>
</DIV>
</DIV>
<DIV ID="4.2" DEPTH="2" R-NO="2"><HEADER>  Modifying Unaccepted Proposals </HEADER>
<P>
The collaborative planning principle in
 <REF/>,<REF/> suggests that ``conversants must provide evidence of a detected discrepancy in belief as soon as
possible.'' Thus, once an agent detects a relevant conflict, she must
notify the other agent of the conflict and initiate a negotiation
subdialogue to resolve it -- to do otherwise is to fail in her
responsibility as a collaborative agent. We capture the attempt to
resolve a conflict with the problem-solving action Modify-Proposal, whose goal is to modify the proposal to a form that
will potentially be accepted by both agents. When applied to belief
modification, Modify-Proposal has two specializations: Correct-Node, for when a proposed belief is not accepted, and Correct-Relation, for when a proposed evidential relationship is not
 accepted. Figure <CREF/> shows the problem-solving  recipes for Correct-Node and its subaction, Modify-Node, that is responsible for the actual modification of the proposal. The
 applicability conditions  of Correct-Node specify that the action can only be invoked when _s1 believes that _node is not acceptable while
_s2 believes that it is (when _s1 and _s2 disagree about the
proposed belief represented by _node). However, since this is a
collaborative interaction, the actual modification can only be
performed when both _s1 and _s2 believe that _node is not
acceptable -- that is, the conflict between _s1 and _s2 must have
been resolved. This is captured by the applicability condition and
precondition of Modify-Node. The attempt to satisfy the
precondition causes the system to post as a mutual belief to be
achieved the belief that _node is not acceptable, leading the system
to adopt discourse actions to change _s2's beliefs, thus initiating a
 collaborative negotiation subdialogue. 
</P>
<DIV ID="4.2.1" DEPTH="3" R-NO="1"><HEADER>  Selecting the Focus of Modification </HEADER>
<P>
When multiple conflicts arise between the system and the user
regarding the user's proposal, the system must identify the aspect of
the proposal on which it should focus in its pursuit of conflict
resolution. For example, in the case where Correct-Node is
selected as the specialization of Modify-Proposal, the system
must determine how the parameter _node in Correct-Node should
be instantiated. The goal of the modification process is to resolve
the agents' conflicts regarding the unaccepted top-level proposed
beliefs. For each such belief, the system could provide evidence
against the belief itself, address the unaccepted evidence proposed by
the user to eliminate the user's justification for the belief, or
both. Since collaborative agents are expected to engage in effective
and efficient dialogues, the system should address the unaccepted
belief that it predicts will most quickly resolve the top-level
conflict. Therefore, for each unaccepted top-level belief, our process
for selecting the focus of modification involves two steps:
identifying a candidate foci tree from the proposed belief tree, and
selecting a focus from the candidate foci tree using the heuristic
``attack the belief(s) that will most likely resolve the conflict
about the top-level belief.''  A candidate foci tree contains the
pieces of evidence in a proposed belief tree which, if disbelieved by
the user, might change the user's view of the unaccepted top-level
proposed belief (the root node of that belief tree). It is identified
by performing a depth-first search on the proposed belief tree. When a
node is visited, both the belief and the evidential relationship
between it and its parent are examined. If both the belief and
relationship were accepted by the evaluator, the search on the current
branch will terminate, since once the system accepts a belief, it is
irrelevant whether it accepts the user's support for that belief
 <REF/>. Otherwise, this piece of evidence will be included in the candidate foci tree and the system will continue to
search through the evidence in the belief tree proposed as support for
the unaccepted belief and/or evidential relationship.
</P>
<P>
Once a candidate foci tree is identified, the system should select the
focus of modification based on the likelihood of each choice changing
 the user's belief about the top-level belief. Figure <CREF/> shows our algorithm for this selection process. Given an unaccepted
belief (_bel) and the beliefs proposed to support it, Select-Focus-Modification will annotate _bel with 1) its focus of
modification (_bel.focus), which contains a set of beliefs (_bel
and/or its descendents) which, if disbelieved by the user, are
predicted to cause him to disbelieve _bel, and 2) the system's
evidence against _bel itself (_bel.s-attack).
</P>
<P>
Select-Focus-Modification determines whether to attack _bel's
supporting evidence separately, thereby eliminating the user's reasons
for holding _bel, to attack _bel itself, or both.  However, in
evaluating the effectiveness of attacking the proposed evidence for
_bel, the system must determine whether or not it is possible to
successfully refute a piece of evidence (i.e., whether or not the
system believes that sufficient evidence is available to convince the
user that a piece of proposed evidence is invalid), and if so, whether
it is more effective to attack the evidence itself or its
support. Thus the algorithm recursively applies itself to the evidence
proposed as support for _bel which was not accepted by the system
 (step <CREF/>). In this recursive process, the algorithm annotates each unaccepted belief or evidential relationship proposed
to support _bel with its focus of modification (_beli.focus) and
the system's evidence against it (_beli.s-attack). _beli.focus
contains the beliefs selected to be addressed in order to change the
user's belief about _beli, and its value will be nil if the system
predicts that insufficient evidence is available to change the user's
belief about _beli.
</P>
<P>
 Based on the information obtained in step <CREF/>, Select-Focus-Modification decides whether to attack the evidence  proposed to support _bel, or _bel itself (step <CREF/>). Its preference is to address the unaccepted evidence, because McKeown's
focusing rules suggest that continuing a newly introduced topic (about
which there is more to be said) is preferable to returning to a
 previous topic <REF/>. Thus the algorithm first considers whether or not attacking the user's support for _bel is sufficient to
 convince him of <EQN/>_bel (step <CREF/>). It does so by gathering (in cand-set) evidence proposed by the user as direct
support for _bel but which was not accepted by the system and which
the system predicts it can successfully refute (i.e., _beli.focus
is not nil). The algorithm then hypothesizes that the user has changed
his mind about each belief in cand-set and predicts how this
 will affect the user's belief about _bel (step <CREF/>). If the user is predicted to accept <EQN/>_bel under this hypothesis,
the algorithm invokes Select-Min-Set to select a minimum subset
of cand-set as the unaccepted beliefs that it would actually
pursue, and the focus of modification (_bel.focus) will be the union
of the focus for each of the beliefs in this minimum subset.
</P>
<P>
If attacking the evidence for _bel does not appear to be sufficient
to convince the user of <EQN/>_bel, the algorithm checks whether
directly attacking _bel will accomplish this goal. If providing
evidence directly against _bel is predicted to be successful, then
 the focus of modification is _bel itself (step <CREF/>). If directly attacking _bel is also predicted to fail, the algorithm
considers the effect of attacking both _bel and its unaccepted
proposed evidence by combining the previous two prediction processes
 (step <CREF/>). If the combined evidence is still predicted to fail, the system does not have sufficient evidence to change the
user's view of _bel; thus, the focus of modification for _bel is nil
 (step <CREF/>). and    <CREF/> of the algorithm invoke a function, Predict, that makes use of the belief revision mechanism
 <REF/> discussed in Section <CREF/> to predict the user's acceptance or unacceptance of _bel based on the system's
knowledge of the user's beliefs and the evidence that could be
 presented to him <REF/>. The result of Select-Focus-Modification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user's belief about
the unaccepted top-level belief.  Thus, the negations of these beliefs
will be posted by the system as mutual beliefs to be achieved in order
to perform the Modify actions.
</P>
</DIV>
<DIV ID="4.2.2" DEPTH="3" R-NO="2"><HEADER>  Selecting Justification for a Claim </HEADER>
<P>
Studies in communication and social psychology have shown that
evidence improves the persuasiveness of a message
 <REF/>,<REF/>,<REF/>,<REF/>. Research on the quantity of evidence indicates that there is no
optimal amount of evidence, but that the use of high-quality evidence
 is consistent with persuasive effects <REF/>. On the other  hand, Grice's maxim of quantity <REF/> specifies that one should not contribute more information than is
 required. Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an
intended mutual belief.
</P>
<P>
To convince the user of a belief, _bel, our system selects
appropriate justification by identifying beliefs that could be used to
support _bel and applying filtering heuristics to them. The system
must first determine whether justification for _bel is needed by
predicting whether or not merely informing the user of _bel will be
sufficient to convince him of _bel. If so, no justification will be
presented. If justification is predicted to be necessary, the system
will first construct the justification chains that could be used to
support _bel. For each piece of evidence that could be used to
directly support _bel, the system first predicts whether the user
will accept the evidence without justification. If the user is
predicted not to accept a piece of evidence (evidi), the system
will augment the evidence to be presented to the user by posting
evidi as a mutual belief to be achieved, and selecting propositions
that could serve as justification for it. This results in a recursive
process that returns a chain of belief justifications that could be
used to support _bel.
</P>
<P>
Once a set of beliefs forming justification chains is identified, the
system must then select from this set those belief chains which, when
presented to the user, are predicted to convince the user of
_bel. Our system will first construct a singleton set for each such
justification chain and select the sets containing justification
which, when presented, is predicted to convince the user of _bel.  If
no single justification chain is predicted to be sufficient to change
the user's beliefs, new sets will be constructed by combining the
single justification chains, and the selection process is repeated.
This will produce a set of possible candidate justification chains,
and three heuristics will then be applied to select from among
them. The first heuristic prefers evidence in which the system is most
confident since high-quality evidence produces more attitude change
 than any other evidence form <REF/>.  Furthermore, the system can better justify a belief in which it has high confidence
should the user not accept it. The second heuristic prefers evidence
that is novel to the user, since studies have shown that evidence is
most persuasive if it is previously unknown to the hearer
 <REF/>,<REF/>.  The third heuristic is based on Grice's maxim of quantity and prefers justification chains that contain the
fewest beliefs.
</P>
</DIV>
<DIV ID="4.2.3" DEPTH="3" R-NO="3"><HEADER>  Example </HEADER>
<P>
 After the evaluation of the dialogue model in Figure <CREF/>, Modify-Proposal is invoked because the top-level proposed belief is not accepted.  In selecting the focus of modification, the system will
first identify the candidate foci tree and then invoke the Select-Focus-Modification algorithm on the belief at the root node of
the candidate foci tree. The candidate foci tree will be identical to
 the proposed belief tree in Figure <CREF/> since both the top-level proposed belief and its proposed evidence were rejected
during the evaluation process. This indicates that the focus of
modification could be either <EQN/>Teaches(Smith,AI) or On-Sabbatical(Smith, next year) (since the evidential relationship
between them was accepted).  When Select-Focus-Modification is
applied to <EQN/>Teaches(Smith,AI), the algorithm will first be
recursively invoked on On-Sabbatical(Smith, next year) to
determine the focus for modifying the child belief (step 3.1 in
 Figure <CREF/>). Since the system has two pieces of evidence against On-Sabbatical(Smith, next year), 1) a warranted piece of
evidence containing Postponed-Sabbatical(Smith,1997) and supports(Postponed-Sabbatical(Smith,1997),<EQN/>On-Sabbatical(Smith,
next year)), and 2) a strong piece of evidence containing <EQN/>visitor(Smith,IBM,next year) and supports(<EQN/>visitor(Smith,IBM,next
year),<EQN/>On-Sabbatical(Smith,next year)), the evidence is
predicted to be sufficient to change the user's belief in On-Sabbatical(Smith,next year), and hence <EQN/>Teaches(Smith,AI); thus, the focus of modification will be
On-Sabbatical(Smith,next year). The Correct-Node
specialization of Modify-Proposal will be invoked since the
focus of modification is a belief, and in order to satisfy the
 precondition of Modify-Node (Figure <CREF/>), MB(S,U, <EQN/>On-Sabbatical(Smith,next year)) will be posted as a mutual
belief to be achieved.
</P>
<P>
Since the user has a warranted belief in On-Sabbatical(Smith,next
year) (indicated by the semantic form of utterance
 (<CREF/>)), the system will predict that merely informing the user of the intended mutual belief is not sufficient to change his
belief; therefore it will select justification from the two available
pieces of evidence supporting <EQN/>On-Sabbatical(Smith,next
year) presented earlier.  The system will predict that either piece
of evidence combined with the proposed mutual belief is sufficient to
change the user's belief; thus, the filtering heuristics are applied.
The first heuristic will cause the system to select Postponed-Sabbatical(Smith, 1997) and supports(Postponed-Sabbatical(Smith, 1997),<EQN/>On-Sabbatical(Smith,
next year)) as support, since it is the evidence in which the system
is more confident.
</P>
<P>
 The system will try to establish the mutual beliefs as an attempt to satisfy the precondition of Modify-Node. This will cause the system to invoke Inform discourse actions to generate the following utterances:
</P>
<P>
S:S:
   #2
</P>
<P>
   to (8) #1
  Dr. Smith is not going on sabbatical next year.
</P>
<P>
   to (9)
  He postponed his sabbatical until 1997. 
</P>
<P>
If the user accepts the system's utterances, thus satisfying
the precondition that the conflict be resolved, Modify-Node can
be performed and changes made to the original proposed beliefs.
Otherwise, the user may propose modifications to the system's proposed
modifications, resulting in an embedded negotiation subdialogue.
</P>
</DIV>
</DIV>
</DIV>
<DIV ID="5" DEPTH="1" R-NO="5"><HEADER>  Conclusion </HEADER>
<P>
This paper has presented a computational strategy for engaging in
collaborative negotiation to square away conflicts in agents'
beliefs. The model captures features specific to collaborative
negotiation. It also supports effective and efficient dialogues by
identifying the focus of modification based on its predicted success
in resolving the conflict about the top-level belief and by using
heuristics motivated by research in social psychology to select a set
of evidence to justify the proposed modification of beliefs.
Furthermore, by capturing collaborative negotiation in a cycle of Propose-Evaluate-Modify actions, the evaluation and modification
processes can be applied recursively to capture embedded negotiation
subdialogues.
</P>
</DIV>
<DIV ID="6" DEPTH="1" R-NO="6"><HEADER>  Acknowledgments </HEADER>
<P>
Discussions with Candy Sidner, Stephanie Elzer, and Kathy McCoy have
been very helpful in the development of this work. Comments from the
anonymous reviewers have also been very useful in preparing the final
version of this paper.
</P>
<DIV ID="6.1" DEPTH="2" R-NO="1"><HEADER>Bibliography </HEADER>
<P>
James Allen.
1991.
Discourse structure in the TRAINS project.
In Darpa Speech and Natural Language Workshop.
</P>
<P>
Lawrence Birnbaum, Margot Flowers, and Rod McGuire.
1980.
Towards an AI model of argumentation.
In Proceedings of the National Conference on Artificial
  Intelligence, pages 313-315.
</P>
<P>
Alison Cawsey, Julia Galliers, Brian Logan, Steven Reece, and Karen
  Sparck Jones.
1993.
Revising beliefs and intentions: A unified framework for agent
  interaction.
In The Ninth Biennial Conference of the Society for the Study of
  Artificial Intelligence and Simulation of Behaviour, pages 130-139.
</P>
<P>
Jennifer Chu-Carroll and Sandra Carberry.
1994.
A plan-based model for response generation in collaborative
  task-oriented dialogues.
In Proceedings of the Twelfth National Conference on Artificial
  Intelligence, pages 799-805.
</P>
<P>
Jennifer Chu-Carroll and Sandra Carberry.
1995.
Generating information-sharing subdialogues in expert-user
  consultation.
In Proceedings of the 14th International Joint Conference on
  Artificial Intelligence.
To appear.
</P>
<P>
Jennifer Chu-Carroll.
1995.
A Plan-Based Model for Response Generation in Collaborative
  Consultation Dialogues.
Ph.D. thesis, University of Delaware.
Forthcoming.
</P>
<P>
Paul R. Cohen.
1985.
Heuristic Reasoning about Uncertainty: An Artificial
  Intelligence Approach.
Pitman Publishing Company.
</P>
<P>
Robin Cohen.
1987.
Analyzing the structure of argumentative discourse.
Computational Linguistcis, 13(1-2):11-24, January-June.
</P>
<P>
Columbia University Transcripts.
1985.
Transcripts derived from audiotape conversations made at Columbia
  University, New York, NY.
Provided by Kathleen McKeown.
</P>
<P>
Julia R. Galliers.
1992.
Autonomous belief revision and communication.
In Gardenfors, editor, Belief Revision. Cambridge University
  Press.
</P>
<P>
H. Paul Grice.
1975.
Logic and conversation.
In Peter Cole and Jerry L. Morgan, editors, Syntax and Semantics
  3: Speech Acts, pages 41-58. Academic Press, Inc., New York.
</P>
<P>
Barbara J. Grosz and Candace L. Sidner.
1990.
Plans for discourse.
In Cohen, Morgan, and Pollack, editors, Intentions in
  Communication, chapter 20, pages 417-444. MIT Press.
</P>
<P>
Dale Hample.
1985.
Refinements on the cognitive model of argument: Concreteness,
  involvement and group scores.
The Western Journal of Speech Communication, 49:267-285.
</P>
<P>
Aravind K. Joshi.
1982.
Mutual beliefs in question-answer systems.
In N.V. Smith, editor, Mutual Knowledge, chapter 4, pages
  181-197. Academic Press.
</P>
<P>
Lynn Lambert and Sandra Carberry.
1991.
A tripartite plan-based model of dialogue.
In Proceedings of the 29th Annual Meeting of the Association for
  Computational Linguistics, pages 47-54.
</P>
<P>
Brian Logan, Steven Reece, Alison Cawsey, Julia Galliers, and Karen
  Sparck Jones.
1994.
Belief revision and dialogue management in information retrieval.
Technical Report 339, University of Cambridge, Computer Laboratory.
</P>
<P>
Joseph A. Luchok and James C. McCroskey.
1978.
The effect of quality of evidence on attitude change and source
  credibility.
The Southern Speech Communication Journal, 43:371-383.
</P>
<P>
Mark T. Maybury.
1993.
Communicative acts for generating natural language arguments.
In Proceedings of the National Conference on Artificial
  Intelligence, pages 357-364.
</P>
<P>
Kathleen R. McKeown.
1985.
Text Generation : Using Discourse Strategies and Focus
  Constraints to Generate Natural Language Text.
Cambridge University Press.
</P>
<P>
Donald D. Morley.
1987.
Subjective message constructs: A theory of persuasion.
Communication Monographs, 54:183-203.
</P>
<P>
Richard E. Petty and John T. Cacioppo.
1984.
The effects of involvement on responses to argument quantity and
  quality: Central and peripheral routes to persuasion.
Journal of Personality and Social Psychology, 46(1):69-81.
</P>
<P>
Martha E. Pollack.
1986.
A model of plan inference that distinguishes between the beliefs of
  actors and observers.
In Proceedings of the 24th Annual Meeting of the Association for
  Computational Linguistics, pages 207-214.
</P>
<P>
Alex Quilici.
1992.
Arguing about planning alternatives.
In Proceedings of the 14th International Conference on
  Computational Linguistics, pages 906-910.
</P>
<P>
Rachel Reichman.
1981.
Modeling informal debates.
In Proceedings of the 7th International Joint Conference on
  Artificial Intelligence, pages 19-24.
</P>
<P>
John C. Reinard.
1988.
The empirical study of the persuasive effects of evidence, the status
  after fifty years of research.
Human Communication Research, 15(1):3-59.
</P>
<P>
Rodney A. Reynolds and Michael Burgoon.
1983.
Belief processing, reasoning, and evidence.
In Bostrom, editor, Communication Yearbook 7, chapter 4, pages
  83-104. Sage Publications.
</P>
<P>
Candace L. Sidner.
1992.
Using discourse to negotiate in collaborative activity: An artificial
  language.
In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent
  Systems, pages 121-128.
</P>
<P>
Candace L. Sidner.
1994.
An artificial discourse language for collaborative negotiation.
In Proceedings of the Twelfth National Conference on Artificial
  Intelligence, pages 814-819.
</P>
<P>
SRI Transcripts.
1992.
Transcripts derived from audiotape conversations made at SRI
  International, Menlo Park, CA.
Prepared by Jacqueline Kowtko under the direction of Patti Price.
</P>
<P>
Katia Sycara.
1989.
Argumentation: Planning other agents' plans.
In Proceedings of the 11th International Joint Conference on
  Artificial Intelligence, pages 517-523.
</P>
<P>
Marilyn Walker and Steve Whittaker.
1990.
Mixed initiative in dialogue: An investigation into discourse
  segmentation.
In Proceedings of the 28th Annual Meeting of the Association for
  Computational Linguistics, pages 70-78.
</P>
<P>
Marilyn A. Walker.
1992.
Redundancy in collaborative dialogue.
In Proceedings of the 15th International Conference on
  Computational Linguistics, pages 345-351.
</P>
<P>
Marilyn A. Walker.
1994.
Discourse and deliberation: Testing a collaborative strategy.
In Proceedings of the 15th International Conference on
  Computational Linguistics.
</P>
<P>
Bonnie Webber and Aravind Joshi.
1982.
Taking the initiative in natural language data base interactions:
  Justifying why.
In Proceedings of COLING-82, pages 413-418.
</P>
<P>
Steve Whittaker and Phil Stenton.
1988.
Cues and control in expert-client dialogues.
In Proceedings of the 26th Annual Meeting of the Association for
  Computational Linguistics, pages 123-130.
</P>
<P>
Robert S. Wyer, Jr.
1970.
Information redundancy, inconsistency, and novelty and their role in
  impression formation.
Journal of Experimental Social Psychology, 6:111-127.
</P>
<P>
R. Michael Young, Johanna D. Moore, and Martha E. Pollack.
1994.
Towards a principled representation of discourse plans.
In Proceedings of the Sixteenth Annual Meeting of the Cognitive
  Science Society, pages 946-951.
</P>
<DIV ID="6.1.1" DEPTH="3" R-NO="1"><HEADER>Footnotes</HEADER>
<P>
  This
material is based upon work supported by the National Science
Foundation under Grant No. IRI-9122026.
  The notion of shared plan has
 been used in <REF/>,<REF/>. 
  For details on
how our model determines the acceptance of a belief using the ranking
 of endorsements proposed by Galliers, see <REF/>. 
  The strength of a belief is classified as: warranted, strong, or weak, based on the endorsement of
the belief.
   A recipe <REF/> is a template for performing actions. It contains the applicability conditions for
performing an action, the subactions comprising the body of an action,
etc.
  Applicability conditions are
conditions that must already be satisfied in order for an action to be
reasonable to pursue, whereas an agent can try to achieve unsatisfied
preconditions.
  This subdialogue is
considered an interrupt by Whittaker, Stenton, and Walker
 <REF/>,<REF/>, initiated to negotiate the truth of a piece of information.  However, the utterances they classify as
interrupts include not only our negotiation subdialogues,
generated for the purpose of modifying a proposal, but also
clarification subdialogues, and information-sharing subdialogues
 <REF/>, which we contend should be part of the evaluation process. 
  In collaborative dialogues, an agent should
reject a proposal only if she has strong evidence against it. When an
agent does not have sufficient information to determine the acceptance
of a proposal, she should initiate an information-sharing
subdialogue to share information with the other agent and re-evaluate
 the proposal <REF/>. Thus, further research is needed to determine whether or not the focus of modification for a rejected
belief will ever be nil in collaborative dialogues.
  Walker wal_coling94 has shown the
importance of IRU's (Informationally Redundant Utterances) in
efficient discourse. We leave including appropriate IRU's for future
work.
  Only MB(S,U,Postponed-Sabbatical(Smith, 1997)) will be proposed as
justification because the system believes that the evidential
relationship needed to complete the inference is held by a
stereotypical user.
</P>
</DIV>
</DIV>
</DIV>
</BODY>
</MINIMAL-DOC>
