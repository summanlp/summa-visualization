
Feeding training data to statistical representations of language
has become a popular past-time for computational
linguists, but our understanding of what constitutes
a sufficient volume of data remains shadowy.
For example, Brown et al. (1992) used over 500 million
words of text to train their language model.
Is this enough?  Could devouring even more data
further improve the accuracy of the model learnt?
In this paper I explore a number of issues in the analysis of
 data requirements for statistical NLP systems. A framework for viewing such systems is proposed and a sample
of existing works are compared within this framework.
Finally, the first steps toward a theory of
data requirements are made by establishing
an upper bound on the expected error rate of a class of
statistical language learners as a function of the volume
of training data.
