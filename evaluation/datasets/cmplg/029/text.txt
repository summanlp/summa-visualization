
Statistical approaches to natural language processing
are becoming increasingly popular, being applied
to a wide variety of tasks.  For example,
Weischedel et al. (1993) explores part-of-speech
tagging, parsing and acquisition of lexical frames.
Nonetheless, all these tasks share some important
characteristics, not the least of which is the requirement
for a sizable corpus of training data.  One
question which has largely been ignored is how much
data is enough?  For example, given a limited body
of training data, it is essential to know which
statistical NLP methods are likely to be accurate before
pursuing any one.  Also, given a particular
method, when will acquiring further training data cease
to improve the system accuracy?  Currently, the
field is conspicuously lacking a general theory of
data requirements for statistical NLP.

In this paper, I present the first steps towards the
development of such a theory.  I begin by formulating
a framework for statistical NLP systems
designed to capture some of the elements crucial
to data requirements analysis.  I will
then review a sample of existing approaches, showing
how they fit into the framework, and where they vary from it.
Even though several of these introduce complexities which
are not captured by the framework, reasoning in the framework
still supports some important insights into these systems.
Finally, I present some preliminary work on establishing
a closed form upper bound on data requirements for a class
of statistical NLP systems. This latter work owes much to Mark
Johnson, Brown University, who is responsible for several key
 mathematical ideas in Section . 

Statistical NLP systems are designed to make choices;
hopefully in an informed manner.  To do this they use
indicators, upon which their choices are conditioned.
The purpose of computing statistics is to inductively establish
the relationship between the indicators and the choices to be made.
Consider for example a next word predictor which attempts to predict
the next word on the basis of the preceding word.  To do this it must
have an understanding of the relationship  between the indicator (the
preceding word) and the choice (the next word).  It is possible
to acquire this understanding by computing statistics over a large
corpus, a process called training.  Once trained, the system may then
be applied to a new text and its accuracy evaluated.  This paper
is concerned with the dependence of a system's accuracy
on the size of the training corpus.  In the following section, the
notions of indicators, choices and training data will
be made more formal.

A statistical NLP system deals with a certain linguistic
universe.  Formally, there is a set of linguistic events

from which every training example and every
test instance will be drawn.  In the next word predictor,
this need only be the set of all pairs of words which may
be adjacent in text.

Let V be a finite set of values that we would like
to assign to a given linguistic input.  This defines
the range of possible answers that the analyser can
make.  In the predictor, this is the set of words plus
an end of sentence symbol.
Let 


be the random
variable describing the distribution of values that
linguistic events take on.  We also require a set of
indicators, B, to use in selecting a value for a
given linguistic event.  I will refer to each element of
B as a bin. In the predictor, the set of bins is
the set of words plus a start of sentence symbol.
Let 


be
the random variable describing the distribution of bins
into which linguistic events fall.

The task of the analyser is to choose which value
is most likely given only the
indicator.  Therefore, it is a function 


The task of the learning algorithm is to acquire
this function by computing statistics on the training set.

Putting these components together, we can define a
Statistical Processor, S as a
tuple 


 ,
where:



is the set of all possible linguistic
events

B and V are finite sets, the bins and values
respectively

A is the trained analysis function



Amongst all such statistical processors, there is a
special class in which we are interested.  Define a
probabilistic analyser to be a statistical processor
which computes a function 


such that 


for all 

and then computes A as:



The problem of acquiring A is thus transformed into
one of estimating the function p using the
training corpus. Generally, p(b,v) is viewed as an estimate
of the probability 


 .

Formally, a training corpus, c,
of m instances, is an element from


where each pair (b,v) is sampled
according to the random variables I and J from

 .
For probabilistic analysers, there are a variety of
methods by which an appropriate function p can be
estimated from a corpus; one simple example being the
Maximum Likelihood Estimate.  Regardless of
the learning algorithm used, each possible training
corpus, c, results in the acquisition of some function, Pc.
Our aim is to explore the dependence
of the expected accuracy of Pc on the magnitude
of m.

Surprisingly, it is not always obvious how many training
instances have been used to train a statistical
method.  It is not generally sufficient
to report the size of the corpus in words.  A system
which collects word associations using a window of
cooccurrence 10 words wide will find 819 instances
in a 100 word corpus, while one collecting the objects
of the preposition on from the same corpus,
would most likely find only a few instances.
Therefore, before any conclusions can be drawn about data
requirements, the training corpus must be measured in terms of
instances.

Each of these instance falls into a particular bin
by virtue of its associated indicator.  In choosing the indicators,
we have implicitly defined equivalence classes for instances.  The
statistical processor will treat every instance in a bin identically.
Further, once the bins are chosen, the greater the number of
training instances that fall into a bin, the greater our
confidence in the statistical inference made by the processor
for test cases in that bin.  For instance, the next word predictor
is more likely to be correct when the preceding word is common
than when it is a rare word.

It is not always obvious how many bins a
given statistical method employs.  Often multiple
indicators are used.
For instance, a trigram tagger uses the tags of the two
preceding words and the current word to choose a new tag.
In this case, 


where T is the tagset
and W is the vocabulary.

This example demonstrates an important point.  By choosing to take
into account the tags of two preceding words, the trigram
tagger requires |T| times as many bins as a bigram
tagger (where 


 ).  With more bins, the
trigram tagger is sensitive to a broader range of context and
thus can in principle achieve a greater accuracy.  However,
because there are more bins, there are fewer training instances
in each bin.  Thus, statistical estimation will be less accurate.
In practice, high accuracy requires at least a few training
instances per bin.  Thus increasing the number of indicators
may actually decrease the overall accuracy.

For probabilistic analysers it is useful to define the number
of slots, L, to be 

|B|(|V|-1), which is the number of
independent parameters needed to define the function p.

For any non-trivial general statistical processor the
indicators used cannot perfectly represent the entire
linguistic event space.  Thus, in general there exist
values 


 ,
for which
both 


 and 


for some 

 .
Suppose without loss of generality that

A(b) = v1.  The analyser will be in
error with probability at least 


 .
This is the root of a rather difficult problem in
statistical NLP because no matter how inaccurate a
trained statistical processor is, the inaccuracy may
be due to the imperfect representation of 

  by B.   

Probabilistic analysers always select just one value for each bin,
the one which maximises p.  Let 


This is the probability of the analyser being in error
on a randomly selected element of 

 .
Let


be any function which minimises the expected
error rate and 


 .
Given B and V, 


is the smallest possible
expected error rate.  Any probabilistic analyser
which achieves an accuracy close to this is
unlikely to benefit from further training data.

Unless large volumes of manually annotated data exist,
measuring the size of 


in any given statistical
processor presents a difficult challenge.  Hindle
and Rooth (1993) have attempted a similar task using human
subjects on the problem of prepositional phrase
attachment.  Subjects were given only the preposition
and the preceding verb and noun and then were
asked to select the attachment.  This was precisely
the task facing their statistical processor.  The subjects
could only perform the attachment correctly in around 86%
of cases.  If we assume that the subjects incorrectly analysed
the remaining 14% of cases because these cases depended on
knowledge of the wider context, then any statistical learning
algorithm based only on these indicators cannot do better
than 86%.  Of course, if there is insufficient training data
the system may do considerably worse.

Assuming that human performance on the task
accurately reflects the value of 


 is the only means known at present to estimate the value
of 


 .
Unfortunately, this approach is expensive to apply
and makes a number of questionable
psychological assumptions.  For example,
it assumes that humans can accurately reproduce parts
of their language analysis behaviour on
command.  It may also suffer when representational
aspects of the analysis task cannot be explained
easily to experimental subjects.  A worthwhile goal
for future research is to establish a statistical
method for estimating or bounding  


using
language data.

In this section, I show how a number of existing statistical
NLP works fit into the framework, including a
tagger, a sense disambiguator and three syntactic analysers.
 For each, I consider how the various
elements of the general statistical processor are instantiated.

Weischedel et al. (1993) uses (among other experiments)
a trigram hidden Markov model to tag text for
part of speech.  The training data is four million
words of the University of Pennsylvania Treebank,
tagged with a set of 47 different tags.  I shall regard
B as consisting of the two previous tags (


 ),
while V is simply the tagset.  The system
also takes into account lexical tag frequencies (that is,


 ).
I will assume however that data sparseness does not affect
the lexical tag frequency estimates.  Since the trigram estimates and
the lexical tag frequencies are combined as independent
factors, ignoring the lexical component does not seem unreasonable.
The situation is further complicated
because probability is maximised over a sequence of
words, rather than for a single word.  The framework needs to be extended
to capture these mechanisms, but for the moment the approximations
I have made may be useful.
Since every word in the corpus (bar the first two) is used for
training, we have m=4 million and 


The accuracy is reported to be
around 97%, which is approximately the accuracy of
human taggers using the whole context.

Yarowsky (1992) describes a sense disambiguation system
which uses a 100 word window of
cooccurrences.  He uses a mutual information-like measure
which combines the cooccurrence statistics
for all words in each category of Roget's thesaurus.
 The result is a profile of contexts for a category
which can be used to estimate how likely each category
is within a certain context.  Comparing the
different possible categories for the word provides
a broad sense discrimination.  The training corpus is
Grolier's encyclopedia which contains on the order
of 10 million words.  Each of these provides 100
training instances (every other word in the window),
so 


billion.  Since the evidence from
each word in the context is combined independently,
it is reasonable to regard B as simply the set of distinct
words in Grolier's. Again, further work is needed to make this
approximation unnecessary.  V is the set of Roget's categories
(

|V| = 1042), so assuming the vocabulary is around
100,000, 


million.
  The average accuracy reported is 92%.

Hindle and Rooth (1993) propose a system to syntactically
disambiguate prepositional phrase
attachments.  Unambiguous examples of attachments are
used to find lexical associations (a likelihood
ratio) between prepositions and the nouns or verbs
they attach to.  They cyclically apply this
technique, adding disambiguated attachments into the
training set, until all the training data (ambiguous
or not) has been used.  This approach can be
approximated by a probabilistic analyser.
Each association value is ascribed
to a pair (w,p) where w is a verb or noun
and p is a preposition.  Thus B is a product of
two indicator spaces: the set of verbs and nouns and
the set of prepositions.  Assuming they used 10,000
nouns and verbs (5,000 of each) and 100
prepositions, |B| = 1 million.  The analyser computes
a probability for each of two possible attachments,
nominal and verbal, so V is binary.
The training set consists of 754,000 noun attachments
and 468 thousand verb ones giving m = 1.22 million.
  The accuracy reported is close to 80%, while human subjects given the
same indications could achieve 85-88% accuracy.
If the latter figure reflects the optimal error
rate, it appears there is still room for improvement
by adding training data or changing the statistical
measures.

Lauer (1994) describes a system for syntactically analysing
compound nouns.  Two-word compounds
extracted from Grolier's encyclopedia were used to
measure mutual information between every pair of
thesaurus categories (using Roget's thesaurus) and
the results used to select a bracketing for three-word
compounds.  Since an association value is computed
for every pair of thesaurus categories, |B|is equal to 


 .
There are only two possible
bracketings to choose from, so again V is binary.  The training
corpus consists of about 35,000 two-word
compounds, giving 

m = 35,000 and 


million.
 The accuracy reported is 75%.

Resnik and Hearst (1993) aim to enhance Hindle and
Rooth's (1993) work by incorporating
information about the head noun of the prepositional
phrase in question.  Thus B is now a product of
three spaces: the set of nouns and verbs, the set of
prepositions and the set of nouns.  To reduce the data
requirement, a freely available on-line thesaurus,
called WordNet is used (Beckwith et al., 1991).
WordNet groups words into synsets, categories of synonymous
words.  These synsets are arranged in a taxonomy, so that
every word is also provided with a list of hypernyms.
The system then adds together the frequency counts for nouns
within a synset, providing more data about each.
This reduces the number of bins, since it is the synsets
which are taken as indicators rather than individual words.
If we assume roughly 1000 synsets,
1000 verbs and 100 prepositions, then 

|B|
=  200 million.  V is still binary.  Their training corpus
is an ``order of magnitude smaller than'' Hindle and Rooth's,
so m is around 100,000.  Unlike Hindle
and Rooth's, their corpus is parsed, which should give
better results.  Interestingly, they combine
evidence from large groups of synsets within WordNet's
hypernym hierarchy using a t-test.  This causes
the effective number of synsets for nouns to be reduced,
perhaps by as much as a factor of 10 (thus 


million).
I will therefore assume that 


million.  Even
given the additional information about the head
noun of the prepositional phrase, the accuracy reported
fails to improve on that of Hindle and Rooth,
being 78%.  It is possible that insufficient training
data is the cause of this shortfall.

 Table  shows a summary of the above systems, ordered on the ratio m:L.  A strong
correlation is evident between the value of this ratio
and the success rate.  This suggests that the success
of a statistically based system is strongly dependent
on the confidence permitted by the training set size
as measured by this ratio.

The model formulated above and the empirical data presented
support a number of qualitative
inferences about the potential of systems given a fixed
training set size.  Because training data will
always be limited, such reasoning is an important part
of system design.  Therefore before turning to
some quantitative analyses, I will examine a few such
inferences.

The most important of these is in regard to linguistic
sophistication, that is the degree to which the
system uses knowledge of the patterns of language.
 This kind of knowledge is extremely important,
since it often allows just the right distinctions to
be made.  More simplistic systems will inevitably
assign one choice to two different inputs
because their linguistic knowledge fails to support a
distinction.  Therefore, it seems desirable to
incorporate as much linguistic sophistication as
possible.  While this is a tempting direction to take
for improving system performance, there is a
barrier.

Consider, for instance, the effect on data requirements
of incorporating new indicators.  Each indicator
increases the number of distinctions which the system
can make.  For example, Resnik and Hearst (1993)
take into account the object of the preposition. In
doing so, they distinguish cases which Hindle and Rooth (1993)
did not.  As a result, the number of cases their system
considers is substantially larger than those considered
by Hindle and Rooth's.  In terms of the framework, Resnik
and Hearst have many more bins than Hindle and Rooth.

It is easy to see that incorporating a new indicator
increases the number of bins combinatorially.
The size of B is multiplied by the
range of the new indicator.  This results in the ratio m:Lfalling by the same factor, which, as I have argued above, can be
detrimental to the overall accuracy.

The situation is worse still if the training set is not hand annotated.
In this case, introducing the new indicator creates additional ambiguity
in the training set since the value of the new indicator must be
determined for each training example.  This
effectively decreases the number of training instances
resulting in a further decrease in m:L.

Thus, linguistic sophistication presents a trade-off
between accuracy and data sparseness.  It is a
balance between poor modeling of the language and insufficient
data for accurate statistics.  If we are to
strike a satisfactory compromise, we need a strong
theory of data requirements and ways to make more
economic use of data.

One such method is termed conceptual association,
as defined in Resnik and Hearst (1993).  By
collecting statistics based on concepts rather than
individual words, the number of bins is usually
reduced.  The idea is to generalise findings about
words to cover other words which have the same
meaning.  The advantages of this approach are extensively
argued in Resnik (1993) and the method is
used in Lauer (1994).  While concepts can help, the
ambiguity introduced (namely what concept does a
given word belong to) may undermine the increased accuracy.
  Further work is needed to establish the
effects on data requirements of employing this strategy.

A novel extension to this approach that has not yet
been employed, would be to collect statistics at
various levels of granularity.  Statistics computed on
counts of individual words would provide fine sensitivity,
while statistics computed on counts of a small set of
semantic primitives (such as ANIMATE, ABSTRACT, etc.)
would provide the coarsest evidence.  As many levels
as desired between these two extremes could be employed
in this way.  The level used to make
each choice could then be selected according
to the degree of confidence available at each level.
If insufficient data has been seen to allow a confident
selection at one level, a coarser grained level
would be tried.  Resnik and Hearst (1993) seem to be
simulating this when they perform a t-test across
all levels of the WordNet hierarchy.

In this section I shall establish some lower bounds
on the accuracy of a simple training scheme within
the framework developed.  The mathematics presented in
 Sections  through  was for the most part developed by Mark Johnson of Brown
University and completed by the author.  I wish to thank
him for his many communications in this regard.

Let 


 ,
the training instances in a corpus c that fall into bin b.

Let 


 ,
the frequency of the value v in the set of
training instances, t.

Let 


 ,
the most common value for instances from a corpus c in a bin b.
Where several values have equal frequencies, one should be
chosen at random.

Define the learning algorithm such that:

Since each bin has only one value with non-zero
probability, V is effectively a binary set (either
the instance has the non-zero value or it does not).
Thus, L = |B|.  Notice also that the value assigned
highest probability by Pc is the one most
frequently falling into the bin.  That is,


 .

Two possible cases arise when the analyser is faced
with making a decision on the basis of some
indication, b.  Either the corpus contains no occurrences
of (b, v) for any value 

(Case A)
or there is some training data which falls into the
bin (Case B).

Case A arises when none of the training instances fall
into the bin.  Let pb denote 

In Case B, we have at least one instance in the corpus
for the given bin.  Let 


An obvious question is why systems such as Lauer (1994)
and Resnik and Hearst (1993) work at all
given that far less than one instance is expected for
each slot.  One possible answer is that different bins
have widely differing frequencies.  The system quickly
learns about the most frequent cases at the
expense of less frequent ones.

This can be modeled by considering the different
distributions, pb defined for Case A above.
In that analysis, the probability of encountering an
empty bin was maximised over all possible distributions.
However, if something is known about the distribution,
in principle a tighter bound is possible.  For example,
suppose some fraction of the bins have very low
probability.  That is, 


such that


for some small c.
Let 


and 


 .
Now:

Now the second term is maximised when 




So letting 


 :

So knowing a pair of values c and 

is a useful,
if primitive, means of lowering the upper
bound on data requirements.  Since the distribution
of bins does not depend on the values we are
seeking to learn, it should be possible to develop
simple techniques for estimating values of c and

 .

A great deal of work remains to be done.  I will mention
only a few directions where the work begs
to be extended.  First, the mathematical model doesn't
capture several aspects of existing models, such as maximising
probabilities over sequences of words and combining evidence
from multiple sources.  Second, the simple learning algorithm
presented differs from those used in practice in several
ways.  It would be useful to explore the
relationship between the algorithm I have proposed
and others in existing statistical methods (for
example, the backing off method in Katz, 1987).  Third,
smoothing is frequently used to alleviate data
sparseness (see Dagan et al., 1993), but the model does
not include any means to represent the process
of smoothing.  Finally, almost all statistical NLP
systems deal with some noise in the training data.
This is especially important in systems like Yarowsky
(1992) where training is unsupervised.  The
mathematical results need to be extended to reflect
noisy training data and to support reasoning about
the sensitivity of data requirements to noise.

In this paper I have indicated the lack of a general
theory of data requirements within the field of
statistical NLP.  As a first step in the development
of such a theory I have presented a framework
for statistical NLP systems.  I have shown how several
prominent works in the field fit this model and demonstrated
a number of mathematical results which
support inferences about data requirements.  I believe
this represents a significant first step along the
road to a better understanding of when and how statistical
NLP methods can be applied.

Without Mark Johnson's interest and collaboration, this paper
would not exist.  Many of the key ideas originate with him
and I am indebted to him for his patience and attention.
The work has also benefited from assistance from Richard Buckland,
Robert Dale, Mark Dras, Mike Johnson and Steven Sommer.
Financial support is gratefully acknowledged from the Australian
Government and the Microsoft Institute.

1
 Beckwith, R., Fellbaum, C., Gross, D. and Miller, G.
(1991) WordNet: A lexical database organized on psycholinguistic
principles, in Zernik, Uri (ed.) Lexical Acquisition:
Exploring On-line Resources to Build a Lexicon, Lawrence Erlbaum,
pp211-32

2
 Brown, P. F., Della Pietra, S. A., Della Pietra, V. J.,
Lai, J. C. and Mercer, R. L. (1992) An Estimate of an Upper
Bound for the Entropy of English Computational Linguistics
18(1) pp31-40

3
 Dagan, I., Marcus, S. and Markovitch, S.
(1993) Contextual Word Similarity and Estimation
from Sparse DataProceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
Columbus, Ohio. see also cmp-lg/9405001

4
 Dras, Mark and Lauer, Mark (1993) Lexical Preference Estimation Incorporating Case
Information Proceedings of the Natural Language
Processing Workshop, Australian Joint Conference on
Artificial Intelligence, Melbourne, Australia.

5
 Hindle, Don and Mats Rooth (1993) Structural Ambiguity and Lexical Relations
Computational Linguistics Vol. 19(1), Special Issue
on Using Large Corpora I, pp 103-20

6
 Katz, S. M. (1987) Estimation of
probabilities from sparse data for the language model
of a speech recognizer IEEE Transactions on Acoustics,
Speech, and Signal Processing, Vol. 35(3)

7
 Lauer, Mark (1994) Conceptual
Association for Compound Noun Analysis Proceedings
of the 32nd Annual Meeting of the Association for
Computational Linguistics, Student Session, Las Cruces,
New Mexico. cmp-lg/9409002

8
 Resnik, P. (1993) Selection and
Information: A Class-Based Approach to Lexical
Relationships PhD dissertation, University of
Pennsylvania, Philadelphia, PA.

9
 Resnik, Philip and Marti Hearst (1993) Structural Ambiguity and Conceptual Relations
Proceedings of the Workshop on Very Large Corpora:
Academic and Industrial Perspectives, June 22, Ohio
State University, pp 58-64

10
 Weischedel, R., Meteer, M., Schwartz, R.,
Ramshaw, L. and Palmucci, J. (1993) Coping
with Ambiguity and Unknown Words through
Probabilistic Models Computational Linguistics
Vol. 19(2), Special Issue on Using Large
Corpora II, pp 359-82

11
 Yarowsky, David  (1992) Word-Sense
Disambiguation Using Statistical Models of Roget's
Categories Trained on Large Corpora Proceedings of
COLING-92, Nantes, France, pp 454-60

  This
paper has been published in the Proceedings of the Second
Conference of the Pacific Association for Computational
Linguistics, Brisbane, Australia, 1995.
  Unless a more accurate statistical
processor based on the same indicators already exists.
  Brackets indicate measured on different data
and/or under different conditions.
  Reported in Resnik(1993).
  Reported in Dras and Lauer(1993).
  Some stemming is performed, so it is the number
of stems in the vocabulary that we want.
  I have allowed all the training
examples as instances, even though some are
acquired by cyclic refinement.
