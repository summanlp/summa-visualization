<?xml version='1.0'?>
<!DOCTYPE MINIMAL-DOC SYSTEM "mini.dtd">
<MINIMAL-DOC>
<TITLE>A Robust Parser Based on Syntactic Information</TITLE>
<ABSTRACT>
<P>
An extragrammatical sentence is what a normal parser fails to analyze. 
It is important to recover it using only syntactic information
although results of recovery are better if semantic factors are considered.
A general algorithm for least-errors recognition,  
which is based only on syntactic information, 
was proposed by G. Lyon to deal with the extragrammaticality. 
We extended this algorithm to recover extragrammatical sentence into grammatical one
in running text.
Our robust parser with recovery mechanism
- extended general algorithm for least-errors recognition -
can be easily  scaled up and modified because it utilize only
syntactic information.
To upgrade this robust parser
we proposed heuristics through the analysis on the Penn treebank corpus.
The experimental result shows 68% <EQN/>
77% accuracy in error recovery.  
</P>
</ABSTRACT>
<BODY>
<DIV ID="1" DEPTH="1" R-NO="1"><HEADER>  Introduction </HEADER>
<P>
Extragrammatical sentences include patently ungrammatical constructions as well as
utterances that may be grammatically acceptable but are beyond the syntactic coverage
 of a parser, and any other difficult ones that are encountered in parsing <REF/>. 
</P>
<P>
 		I am sure this is what he means.		This is, I am sure, what he means.		 		The progress of machine does not stop even a day.		Not even a day does the progress of machine stop.        
</P>
<P>
Above examples show that people are used to write same meaningful sentences differently.
In addition, people are prone to mistakes in writing sentences.
So, the bulk of written sentences are open to the extragrammaticality.
</P>
<P>
 In the Penn treebank  tree-tagged  corpus<REF/>, for instance, about 80 percents of the rules are concerned with peculiar sentences which include
inversive, elliptic, parenthetic, or emphatic phrases.
For example, 
we can drive a rule VP  
<!-- MATH: $\rightarrow$ -->
<EQN/> vb  NP  comma  rb  comma  PP   from the following sentence.
</P>
<P>
The same jealousy can breed confusion, however, in the absence of any authorization bill this year.
</P>
<P>
(
(S
 (NP The/dt
  (ADJP same/jj) jealousy/nn) can/md
 (VP breed/vb
  (NP confusion/nn) ,/, however/rb ,/,
  (PP in/in
   (NP
    (NP the/dt absence/nn)
    (PP of/in
     (NP any/dt authorization/nn bill/nn))
    (NP this/dt year/nn)))))
./.)
</P>
<P>
A robust parser is one that can analyze these extragrammatical sentences without failure. 
However, if we try to preserve robustness 
by adding such rules whenever we encounter an extragrammatical sentence,
the rulebase will grow up rapidly, and thus processing and maintaining the excessive 
number of rules will become inefficient and impractical.
Therefore, extragrammatical sentences should be handled by some recovery mechanism(s)
rather than by a set of additional rules.
</P>
<P>
Many researchers have attempted several techniques to deal with extragrammatical sentences
 such as Augmented Transition Network(ATN) <REF/>,  network-based semantic grammar <REF/>, partial pattern matching <REF/>,  conceptual case frame <REF/>, and    multiple cooperating methods <REF/>. Above mentioned techniques take into account 
various semantic factors 
depending on specific domains on question
in recovering extragrammatical sentences. 
Whereas they can provide even better solutions intrinsically,
they are usually ad-hoc and  are lack of extensibility.
Therefore, it is important to recover extragrammatical sentences
using syntactic factors only, which are independent of any particular system
and any particular domain.
</P>
<P>
 Mellish <REF/> introduced some chart-based techniques using only syntactic information for extragrammatical sentences.
This technique has an advantage that there is no repeating work 
for the chart to prevent
the parser from generating the same edge as the previously existed edge.
Also, because the recovery process runs when a normal parser terminates 
unsuccessfully, the performance of the normal parser does not decrease
in case of handling grammatical sentences.
However, his experiment was not based on the errors in running texts
but on artificial ones which were randomly generated by human.
Moreover, only one word error was considered though
several word errors can occur simultaneously in the running text.
</P>
<P>
 A general algorithm for least-errors recognition <REF/>, proposed by G. Lyon,  is to find out the least number of errors necessary to successful parsing and
recover them.
Because this algorithm is also syntactically oriented and based on a chart, it has the same
advantage as that of Mellish's parser.
When the original parsing algorithm terminates unsuccessfully, the algorithm begins to
assume errors of insertion, deletion and mutation of a word.
For any input, including grammatical and extragrammatical sentences, 
this algorithm can generate the resultant parse tree.
At the cost of the complete robustness, however, this algorithm degrades the
efficiency of parsing, and generates many intermediate edges.
</P>
<P>
In this paper, we present a robust parser with a recovery mechanism.
We extend the general algorithm for least-errors recognition
to adopt it as the recovery mechanism in our robust parser.
Because our robust parser handle extragrammatical sentences with  
this syntactic information oriented recovery mechanism,
it
can be independent of a particular system or particular domain.
Also, we present the heuristics to reduce the number of edges
so that we can upgrade the performance of our parser.
</P>
<P>
This paper is organized as follows :
We first review a general algorithm for least-errors recognition.
Then we present the extension of this algorithm, and 
the heuristics adopted by the robust parser. 
Next, we describe the implementation of the system and the result of the experiment of
parsing real sentences. 
Finally, we make conclusion with
future direction.
</P>
</DIV>
<DIV ID="2" DEPTH="1" R-NO="2"><HEADER>  Algorithm and Heuristics </HEADER>
<DIV ID="2.1" DEPTH="2" R-NO="1"><HEADER>    General algorithm for least-errors recognition
</HEADER>
<P>
 The general algorithm for least-errors recognition <REF/>,  which is based on Earley's algorithm,
assumes that sentences may have insertion, deletion, and
mutation errors of terminal symbols.
The objective of this algorithm is to parse
input string with the least number of errors.
</P>
<P>
A state used in this algorithm is quadruple (p, j, f, e), where
p is a production number in grammar, 
j marks a position in RHS(p),
f is a start position  of the state in input string, and
 e is an error value. A final state (p, 
<!-- MATH: $\underline{p}$ -->
<EQN/>+1, f, e) denotes recognition of a phrase
RHS(p) with e errors where 
<!-- MATH: $\underline{p}$ -->
<EQN/>
is a number of components in rule p.
A stateset S(i), where i is the position of the input, is an ordered set of states.
States within a stateset are ordered by ascending value of j,
within a p within a f ; f takes descending value.
</P>
<P>
When adding to statesets, if state (p, j, f, e) is a candidate for admission 
to a stateset which already has a similar member (p, j, f, e') and
e' <EQN/>
e, then (p, j, f, e) is rejected.
However, if e' ] e, then (p, j, f, e') is replaced
by (p, j, f, e).
</P>
<P>
The algorithm works as follows :
A procedure SCAN is carried out for each state in S(i).
SCAN checks various correspondences of input token t(i) against 
terminal symbols in RHS of rules.
Once SCAN is done, COMPLETER substitutes all final states of S(i)
into all other analyses which can use them as components.
</P>
<P>
SCAN
SCAN handles states of S(i), checking each input terminal against requirements 
of states in S(i) and various error hypotheses.
 Figure <CREF/> shows how SCAN processes. 
</P>
<P>
Let c(p,j) be j-th component of RHS(p) and
t(i) be i-th word of input string.
<ITEMIZE>
<ITEM>perfect match :   
                If c(p,j) = t(i) then add (p, j+1, f, e) to 
                S(i+1) if possible.
</ITEM>
<ITEM>insertion-error hypothesis : 
                Add (p, j, f, e+
<!-- MATH: $\alpha_{insertion}$ -->
<EQN/>) to S(i+1) if possible.
<!-- MATH: $\alpha_{insertion}$ -->
<EQN/>
is the cost of an insertion-error for a terminal symbol.
</ITEM>
<ITEM>deletion-error hypothesis :
                If c(p,j) is terminal, then add (p, j+1, f, e+
<!-- MATH: $\alpha_{deletion}$ -->
<EQN/>) 
                to S(i) if possible. 
<!-- MATH: $\alpha_{deletion}$ -->
<EQN/>
is the cost of a deletion-error for a terminal symbol.
</ITEM>
<ITEM>mutation-error hypothesis :       
                If c(p,j) is terminal but not equal to t(i),
                then add (p, j+1, f, e+
<!-- MATH: $\alpha_{mutation}$ -->
<EQN/>) to S(i+1) if possible.
<!-- MATH: $\alpha_{mutation}$ -->
<EQN/>
is the cost of a mutation-error for a terminal 
         symbol. 
</ITEM>
</ITEMIZE>
</P>
<P>
COMPLETER
COMPLETER handles substitution of final states in S(i) 
like that of original Earley's algorithm.
Each final state means the recognition of a nonterminal.
</P>
</DIV>
<DIV ID="2.2" DEPTH="2" R-NO="2"><HEADER>  Extension of least-errors recognition algorithm </HEADER>
<P>
 The algorithm in section <CREF/> can analyze any input string with the least number of errors.
But this algorithm 
can handle only the errors of terminal symbols
because it doesn't consider the errors of nonterminal nodes.
In the real text, however, the insertion, deletion, or inversion of a
phrase  -  namely, nonterminal node -  
occurs more frequently.
So, we extend the original algorithm in order to handle the errors of 
nonterminal symbols as well.
</P>
<P>
In our extended algorithm, the same SCAN as that of the original algorithm
is used,
while COMPLETER is modified and extended.
 Figure <CREF/> shows the processing of extended-COMPLETER.  In figure <CREF/>, [NP] denotes the final state  whose rule has NP as its LHS.
In other words, it means the recognition of a noun phrase.
</P>
<P>
extended-COMPLETER
        If there is a final state 
<!-- MATH: ${\it s' = (p', \underline{p'}+1, k, e')}$ -->
<EQN/>
in S(i), 
<ITEMIZE>
<ITEM>phrase perfect match
                If there exists a state 
<!-- MATH: ${\it s'' = (p, j, x, e)}$ -->
<EQN/>
in S(k) , k [ i and
<!-- MATH: $c(p,j) = LHS(p')$ -->
c(p,j) = LHS(p')                then add 
<!-- MATH: $s = (p, j+1, x, e+e')$ -->
s = (p, j+1, x, e+e') into S(i).
</ITEM>
<ITEM>phrase insertion-error hypothesis
                If there exists a state 
<!-- MATH: $s'' = (p, j, x, e)$ -->
s'' = (p, j, x, e) in S(k) 
                then add 
<!-- MATH: $s = (p, j, x, e+\beta_{insertion})$ -->
<EQN/>
into S(i) if possible.
<!-- MATH: $\beta_{insertion}$ -->
<EQN/>
is the cost of a insertion-error for a nonterminal symbol.
</ITEM>
<ITEM>phrase deletion-error hypothesis
                If there exists a state 
<!-- MATH: $s'' = (p, j, x, e)$ -->
s'' = (p, j, x, e) in S(k) 
                and c(p,j) is a nonterminal
                then add 
<!-- MATH: $s = (p, j+1, x, e+\beta_{deletion})$ -->
<EQN/>
into S(k) if possible.
<!-- MATH: $\beta_{deletion}$ -->
<EQN/>
is the cost of a deletion-error for a nonterminal symbol.
</ITEM>
<ITEM>phrase mutation-error hypothesis
                                  If there exists a state 
<!-- MATH: $s'' = (p, j, x, e)$ -->
s'' = (p, j, x, e) in S(k) and 
                c(p,j) is a nonterminal but  not equal to L(p')                then add 
<!-- MATH: $s = (p, j+1, x, e+\beta_{mutation})$ -->
<EQN/>
into S(i) if possible.
<!-- MATH: $\beta_{mutation}$ -->
<EQN/>
is the cost of a mutation-error for a nonterminal symbol.
</ITEM>
</ITEMIZE>
</P>
<P>
The extended least-errors recognition algorithm can handle 
not only terminal errors but also nonterminal errors.
</P>
</DIV>
<DIV ID="2.3" DEPTH="2" R-NO="3"><HEADER>  Heuristics </HEADER>
<P>
The robust parser using the extended least-errors recognition algorithm
overgenerates many error-hypothesis edges during parsing process.
To cope with this problem, we adjust error values according to the following
heuristics.
Edges with more error values are regarded as less important ones,
so that those edges are processed later than those of less error values.
</P>
<P>
<ITEMIZE>
<ITEM>Heuristics 1: error types
                The analysis on 3,538 sentences of the Penn treebank corpus WSJ
                shows that there are 498 sentences with phrase deletions and
                224 sentences with phrase insertions.
                So, we assign less error value to the deletion-error hypothesis edge than 
                to the insertion- and  mutation-errors.
 		<EQN/>
[		<EQN/>
 		<EQN/>
[ <EQN/>
[ <EQN/>		<EQN/>
[ <EQN/>
where  <EQN/>
is the error cost of a  terminal symbol, <EQN/>
is the error
            cost of a nonterminal symbol. 
</ITEM>
<ITEM>Heuristics 2: fiducial nonterminal
        People often make mistakes in writing English.
        These mistakes usually take place rather between small constituents
        such as a verbal phrase, an adverbial phrase and noun phrase
        than within small constituents themselves.
        The possibility of error occurrence within noun phrases are lower
        than between a noun phrase and a verbal phrase, a preposition phrase, an adverbial
        phrase. So, we assume some phrases, for example noun phrases,  as  fiducial nonterminals, which 
        means error-free nonterminals. When handling  sentences, the robust
        parser assings more error values(
<!-- MATH: $\delta_{1}$ -->
<EQN/>)
to the error hypothesis edge 
        occurring within a fiducial nonterminal.
</ITEM>
<ITEM>Heuristics 3: kinds of terminal symbols
        Some terminal symbols like punctuation symbols,
        conjunctions and particles are often misused.
        So, the robust parser assigns less error values(
<!-- MATH: $-\delta_{2}$ -->
<EQN/>)
to the error hypothesis 
        edges with these symbols than to the other terminal symbols.
</ITEM>
<ITEM>Heuristics 4: inserted phrases between commas or parentheses
                Most of inserted phrases are surrounded by commas or parentheses. For example,
 a.
They're active , generally  , at night or on damp, cloudy days.
 b.
All refrigerators , whether they are defrosted manually or not , need to be cleaned.
                 c.
I was a last-minute ( read interloping ) attendee at a French journalism convention <EQN/>
We will assign less error values(
<!-- MATH: $-\delta_{3}$ -->
<EQN/>)
to the insertion-error hypothesis edges 
                of nonterminals  which are embraced by  comma or parenthesis.
</ITEM>
</ITEMIZE>
<!-- MATH: $\delta_{1}$ -->
<EQN/>
and 
<!-- MATH: $\delta_{2}$ -->
<EQN/>
are weights for the  error of terminal nodes,
                and 
<!-- MATH: $\delta_{3}$ -->
<EQN/>
is a weight for the error of nonterminal nodes.
</P>
<P>
The error value e of an edge is calculated as follows.
All error values are additive.
        The error value e for a rule  
<!-- MATH: $X \rightarrow a_{1} A_{1} a_{2} \cdots a_{i} A_{j}$ -->
<EQN/>,
        where a is a terminal node and A is a nonterminal node, is
</P>
<P>
1.
<!-- MATH: $e  =  \sum_{1}^{i} e_{T} + \sum_{1}^{j} e_{NT}$ -->
<EQN/>
</P>
<P>
2.
<!-- MATH: $e_{T} = \left\{ \begin{array}{ll}
\alpha + \delta_{1} - \delta_{2}   \mbox{if terminal error}\\
                                                                0                                                                 \mbox{otherwise}
                                   \end{array}
                                   \right.$ -->
<EQN/>
3.
<!-- MATH: $e_{NT} = \left\{ \begin{array}{ll}
\beta -  \delta_{3}  + e_{child}   \mbox{if nonterminal}\\
                                                                                                                            \mbox{error}\\
                                      e_{child}                                                                      \mbox{otherwise}
                                      \end{array}
                                      \right.$ -->
<EQN/>where 
<!-- MATH: $\alpha \in \{\alpha_{insertion},~\alpha_{deletion},~\alpha_{mutation}\}$ -->
<EQN/>,
<!-- MATH: $\beta \in \{\beta_{insertion}, \beta_{deletion}\}$ -->
<EQN/>
and
echild is an error value of a child edge.
</P>
<P>
By these heuristics, our robust parser can process only plausible edges first, 
instead of processing all generated edges at the same time,
so that  we can enhance the performance of the robust parser and result in
the great reduction in the number of resultant trees.
</P>
</DIV>
</DIV>
<DIV ID="3" DEPTH="1" R-NO="3"><HEADER>  Implementation and Evaluation </HEADER>
<DIV ID="3.1" DEPTH="2" R-NO="1"><HEADER>  The robust parser </HEADER>
<P>
Our robust parsing system is composed of two modules.
One module is a normal parser
which is the bottom-up chart parser.
The other is a robust parser with the error recovery mechanism proposed herein. 
At first, an input sentence is processed by  the normal parser.
If the sentence is within the grammatical coverage of the system,
the normal parser succeed to analyze it.
Otherwise, the normal parser fails, and then the robust parser starts to execute
with edges generated by the normal parser.
The result of the robust parser is the parse trees which are within the grammatical 
coverage of the system.
 The overview of the system is shown in  figure <CREF/>. 
</P>
</DIV>
<DIV ID="3.2" DEPTH="2" R-NO="2"><HEADER>  Experimental result </HEADER>
<P>
To show usefulness of the robust parser proposed in this paper, 
we made some experiments.
</P>
<P>
<ITEMIZE>
<ITEM>Rule
                        We can derive 4,958 rules and their frequencies out of 14,137 sentences 
                        in the Penn treebank tree-tagged corpus, the Wall Street Journal.
                        The average frequency of each rule is 48 times in the corpus. 
                        Of these rules, 
                        we remove rules which occurs fewer times than the average frequency in the corpus,
                        and then only 192 rules are left.
                        These removed rules are almost for peculiar sentences and the left rules 
                        are very general rules.
                        We can show that our robust parser can compensate for lack of rules 
                        using only 192 rules with the recovery mechanism and heuristics.
</ITEM>
<ITEM>Test set
                        First,  1,000 sentences are selected randomly from the WSJ corpus, which we have
                        referred to in proposing the robust parser.
                        Of these sentences, 410 are failed in normal parsing,
                        and are processed again by the robust parser.
                        To show the validity of these heuristics, 
                        we compare the result of the robust parser using  heuristics  with one not using
                        heuristics.
                        Second, to show the adaptability of our robust parser,
                        same experiments are carried out on
                        1,000 sentences from the ATIS corpus in Penn treebank, which we haven't referred to 
                        when we propose the robust parser.
                        Among 1,000 sentences from the ATIS, 465 sentences are processed by the robust parser
                        after the failure of the normal parsing.
</ITEM>
<ITEM>Parameter adjustment
                        We chose the best parameters of heuristics by executing several experiments. 
 		 <EQN/>
: 		  10.2 		 		 <EQN/>
: 		 15.0 		 <EQN/>
: 		 10.4 		 		 <EQN/>
: 		 20.0 		 <EQN/>
: 		 10.8		 <EQN/>
: 		 0.01 		 		 <EQN/>
: 		 5.0 		 <EQN/>
: 		 1.0                        
</ITEM>
</ITEMIZE>
</P>
<P>
Accuracy is measured as the percentage of constituents in the test sentences which do not
 cross any Penn treebank constituents <REF/>.  Table <CREF/> shows the results of the robust parser on WSJ.  In table <CREF/>, 5th, 6th and  7th raw mean that the percentage of sentences  which have no crossing constituents, less than one crossing and less than two crossing respectively.
With heuristics, our robust parser can enhance the processing time and reduce the
number of edges. 
Also, the accuracy is improved from 72.8% to 77.1%  
even if the heuristics differentiate edges and prefer some edges.
It shows that the proposed heuristics is valid in parsing the real sentences.
The experiment says that
our robust parser with heuristics can recover perfectly
about 23 sentences out of 100 sentences which are just failed in normal parsing,
as the percentage of no-crossing sentences is about 23.28%.
</P>
<P>
 Table  <CREF/> is the results of the robust parser on ATIS  which we did not refer to before.
The accuracy of the result on ATIS is lower than WSJ because 
the parameters of the heuristics are  adjusted not by ATIS itself but  by WSJ.
However, the percentage of sentences with constituents crossing less than 2 is higher than the WSJ,
as sentences of ATIS are more or less simple.
</P>
<P>
The experimental results of our robust parser show high accuracy in recovery
even though 96% of total rules are removed.
It is impossible to construct complete grammar rules in the real parsing system to
succeed in analyzing every real sentence.
So, parsing systems are likely to have extragrammatical sentences which cannot be analyzed
by the systems.
Our robust parser can recover these extragrammatical sentences
with 68 <EQN/>
77% accuracy.
</P>
<P>
It is very interesting that parameters of heuristics reflect the characteristics of the test corpus.
For example, if people tend to  write sentences with inserted phrases, 
then the parameter 
<!-- MATH: $\beta_{insertion}$ -->
<EQN/>
must increase.
Therefore we can get better results if the parameter are fitted 
to the  characteristics of the corpus.
</P>
</DIV>
</DIV>
<DIV ID="4" DEPTH="1" R-NO="4"><HEADER>  Conclusion </HEADER>
<P>
In this paper, we have presented the robust parser with the extended least-errors
recognition algorithm as the recovery mechanism.
This robust parser can easily be 
scaled up and applied to various domains
because this parser depends only on syntactic factors.
To enhance the performance of the robust parser for extragrammatical sentences, 
we proposed several heuristics.
The heuristics assign the error values to each error-hypothesis edge,
and  edges which has less error  values are processed first.
So, not all the generated edges are processed by the robust parser, 
but
the most plausible parse trees can be generated first.
The accuracy of the recovery in our robust parser is about 68% <EQN/>
77%.
Hence, this parser is suitable for systems in real application areas.
</P>
<P>
Our short term goal is to propose an automatic method that can learn
parameter values of heuristics by analyzing the corpus.
We expect that automatically learned values of parameters can upgrade the performance of the parser.
</P>
</DIV>
<DIV ID="5" DEPTH="1" R-NO="5"><HEADER>  Acknowledgement </HEADER>
<P>
This work was supported(in part) by Korea Science and Engineering Foundation(KOSEF)
through Center for Artificial Intelligence Research(CAIR), the Engineering Research Center(ERC)
of Excellence Program.
</P>
<DIV ID="5.1" DEPTH="2" R-NO="1"><HEADER>Bibliography </HEADER>
<P>
[Black, 1991] E. Black et al.
A Procedure for quantitatively comparing the syntactic coverage of English grammars.
Proceedings of Fourth  DARPA Speech and Natural Language Workshop, 
                        pp. 306-311, 1991.
</P>
<P>
[Carbonell and Hayes, 1983] J. G. Carbonell and P. J. Hayes.
Recovery Strategies for Parsing Extragrammatical Language.
American Journal of Computational
  Linguistics, vol. 9, no. 3-4, pp. 123-146, 1983.
</P>
<P>
[Hayes and Carbonell, 1981] P. Hayes and J. Carbonell.
Multi-strategy Construction-Specific Parsing for Flexible Data Base Query Update.
Proceedings of the 7th International Joint Conference on Artificial
                        Intelligence, pp. 432-439, 1981.
</P>
<P>
[Hayes and Mouradian, 1981] P. J. Hayes and G. V. Mouradian.
Flexible Parsing.
American Journal of Computational Linguistics, 
                        vol. 7, no. 4, pp. 232-242, 1981.
</P>
<P>
[Hendrix, 1977] G. Hendrix. 
Human Engineering for Applied Natural Language Processing.
Proceedings of the 5th International Joint Conference
                         on Artificial Intelligence, pp. 183-191, 1977.
</P>
<P>
[Kwasny and Sondheimer, 1981] S. Kwasny and N. Sondheimer. 
Relaxation Techniques for Parsing Grammatically Ill-Formed Input 
              in Natural Language Understanding Systems.
American Journal of Computational Linguistics, 
                         vol. 7, no. 2, pp. 99-108, 1981.
</P>
<P>
[Lyon, 1974] G. Lyon.
Syntax-Directed Least-Errors Analysis for Context-Free Languages.
Communications of the ACM, vol. 17, no. 1, pp. 3-14, 1974.
</P>
<P>
[Marcus, 1991] M. P. Marcus.
Building very Large natural language corpora : the Penn Treebank, 1991.
</P>
<P>
[Mellish, 1989] C. S. Mellish. 
Some Chart-Based Techniques for Parsing Ill-Formed Input.
Association for Computational Linguistics,
                pp. 102-109, 1989.
</P>
<P>
[Schank  et al. , 1980] R. C. Schank, M. Lebowitz and L. Brinbaum.
An Intergrated Understander.
American Journal of Computational Linguistics, 
                        vol. 6, no. 1, pp. 13-30, 1980.
</P>
<DIV ID="5.1.1" DEPTH="3" R-NO="1"><HEADER>Footnotes</HEADER>
<P>
  Lyon said that e is an error count
<!-- MATH: $\alpha_{insertion},~\alpha_{deletion},~\alpha_{mutation}$ -->
<EQN/>
are all strictly
        1 in Lyon's original paper.
  In fact, there are cases 
that an inserted phrase cannot be constructed to form a nonterminal node. 
 In phrase insertion-error hypothesis of figure <CREF/>,  the original  sentence is ``Other countries, including West Germany, may have <EQN/>'', 
where the inserted phrase VP is surrounded by commas.
So, the substring( comma VP comma ) should be dealt with as a constituent
in extended-COMPLETER.
In fact, we implemented the algorithm to allow substring insertions as well as
insertions of nonterminal nodes.
  We know that the phrase mutation-error hypothesis is not meaningful in the real text
                        because we cannot find out any example of phrase mutation-error in the corpus.
                  So we didn't implement the phrase mutation-error hypothesis.
</P>
</DIV>
</DIV>
</DIV>
</BODY>
</MINIMAL-DOC>
