<?xml version='1.0'?>
<!DOCTYPE MINIMAL-DOC SYSTEM "mini.dtd">
<MINIMAL-DOC>
<TITLE>The Use of Knowledge Preconditions in Language
Processing1</TITLE>
<ABSTRACT>
<P>
If an agent does not possess the knowledge needed to perform an
action, it may privately plan to obtain the required information on
its own, or it may involve another agent in the planning process by
engaging it in a dialogue.  In this paper, we show how the
requirements of knowledge preconditions can be used to account for
information-seeking subdialogues in discourse.  We first present an
axiomatization of knowledge preconditions for the SharedPlan model of
 collaborative activity <REF/>, and then provide an analysis of information-seeking subdialogues within a general
framework for discourse processing.  In this framework, SharedPlans
and relationships among them are used to model the intentional
component of Grosz and Sidner's <REF/> theory of
discourse structure.
</P>
</ABSTRACT>
<BODY>
<DIV ID="1" DEPTH="1" R-NO="1"><HEADER>  Introduction </HEADER>
<P>
 For an agent  to be able to perform an action, it must satisfy both the physical and
 knowledge preconditions of that action <REF/>,<REF/>. For example, for an agent to pick up a particular tower of blocks, it
must (1) know how to pick up towers in general, (2) be able to
identify the tower in question, and (3)  have satisfied the (physical)
preconditions or constraints associated with picking up towers (e.g.,
it must have a free hand).  These conditions must hold whether the
agent is planning an action on its own or is involved in a
collaborative planning effort with other agents.
</P>
<P>
In this paper, we provide an axiomatization of knowledge preconditions
for the SharedPlan model of collaborative activity
 <REF/>,<REF/>,<REF/>.  This model draws upon  past work <REF/>,<REF/>, but adapts it to the collaborative situation.  We briefly describe the SharedPlan framework
 in Section <CREF/>, and then, in Section <CREF/>, present our  axiomatization of knowledge preconditions.  In Section <CREF/>, we demonstrate the use of knowledge preconditions in accounting for
 information-seeking subdialogues, such as those in Figure <CREF/>.  We then compare our approach to the alternative accounts
 <REF/>,<REF/>,<REF/>. 
</P>
</DIV>
<DIV ID="2" DEPTH="1" R-NO="2"><HEADER>    SharedPlans
</HEADER>
<P>
The SharedPlan formalism is a mental-state model of collaborative
plans with roots in Pollack's <REF/> work on
single-agent plans.  For a group of agents GR to have a full
SharedPlan (FSP) for an act 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
they must satisfy the
 requirements given in Figure <CREF/>.  When the agents have satisfied only a subset of these requirements, they are said to have a
 partial SharedPlan (PSP).  The bracketed  terms in Figure    <CREF/> indicate the operators used by Grosz and Kraus <REF/> to formalize each requirement.
</P>
<P>
 Requirement (1) in Figure <CREF/> refers to the agents' recipe <REF/> for  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Recipes are modeled in Grosz
and Kraus's definitions as sets of constituent acts and constraints.
To perform an act 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
an agent must perform each constituent act
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 's recipe according to the constraints of that recipe.
Actions themselves may be further decomposed into act-types and
parameters.  We will represent an action 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
as a term of the
form 
<!-- MATH: $\bar{\alpha}(p_1,\ldots,p_n)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
where 
<!-- MATH: $\bar{\alpha}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
represents
the act-type of the action and the pi its parameters.
</P>
</DIV>
<DIV ID="3" DEPTH="1" R-NO="3"><HEADER>    Knowledge Preconditions
</HEADER>
<P>
Grosz and Kraus <REF/> use the operators BCBA
(read ``believes can bring about'') and MBCBAG (read ``mutually
believe can bring about group'') to formalize respectively
 requirements (2b) and (3a) in Figure <CREF/>.  Although these operators are intended to specify the conditions under which an agent
is able to perform an action, their definitions explicitly require
only that an agent satisfy the physical preconditions or constraints
associated with an action to be able to perform it.  Because an agent
is not truly capable of performing an act unless it possesses the
appropriate knowledge, the definitions of BCBA and MBCBAG must be
augmented with an axiomatization of knowledge preconditions.  The
following observations made by Morgenstern <REF/>,
but recast in our terminology, must be represented in such an
axiomatization:
1.
Agents need to know recipes for the acts they
perform.
2.
All agents have some primitive acts in their
repertoire.
3.
Agents must be able to identify the parameters of the acts they
perform.
4.
Agents may know only some descriptions of an act.
5.
Agents know that the knowledge necessary for complex acts
derives from that necessary for their component acts.
</P>
<P>
Our axiomatization of knowledge preconditions is based on
Morgenstern's observations, but adapted to the requirements of
 individual and shared mental-state plans.  We use the predicates has.recipe and id.params to
represent explicitly observations (1) and (3) above.  The remaining
observations are implicitly represented by the way in which these two
knowledge precondition relations are defined.  Observation (2) is
modeled as the base case of has.recipe, and observation (5) is
modeled by the use of has.recipe within the recursive plan
definitions.
</P>
<P>
Observation (4) requires that the knowledge precondition relations be
intensional, rather than extensional; within their scope it should not
be possible to freely substitute one representation of an action for
another.  We thus define has.recipe and id.params to
hold of action descriptions, rather than actions.  Action
descriptions are intensional objects; one action description can be
substituted for another only if the descriptions are the same.  For
example, although 555-1234 and phone-
<!-- MATH: $number(speech$ -->
number(speech-lab) may
be extensionally equivalent, the descriptions 
<!-- MATH: $\rule[1.2ex]{.05em}{.24em}^{\hspace*{-0.05em}\rule[.6ex]{.25em}{.05em}}\!\,{555}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 -
<!-- MATH: ${1234}\:\rule[1.2ex]{.05em}{.24em}^{\hspace*{-.26em}\rule[.6ex]{.25em}{.05em}}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 and 
<!-- MATH: $\rule[1.2ex]{.05em}{.24em}^{\hspace*{-0.05em}\rule[.6ex]{.25em}{.05em}}\!{phone}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 -
<!-- MATH: ${number(speech}$ -->
number(speech-
<!-- MATH: ${lab)}\:\rule[1.2ex]{.05em}{.24em}^{\hspace*{-.26em}\rule[.6ex]{.25em}{.05em}}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
are not.  By
convention, we will omit the corner quote notation in what follows and
simply take the appropriate arguments of the predicates to represent
action descriptions rather than actions.
</P>
<P>
Although Morgenstern's observations are most naturally expressed
informally in terms of knowledge, we formalize them using belief to
allow for the possibility of an agent's being incorrect.  Although it
is true that an agent cannot successfully act unless its beliefs about
recipes and parameters are correct, having to know the recipes
and parameters is too strong a requirement for collaborating agents
 <REF/>. 
</P>
<DIV ID="3.1" DEPTH="2" R-NO="1"><HEADER>    Determining Recipes: has.recipe
</HEADER>
<P>
For an agent to be able to perform an act 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
it must know how to perform 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ;
i.e., it must have a recipe for the act.
The relation 
<!-- MATH: $has.recipe(G,\alpha,R,T)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is used to represent that
agent G has a recipe 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
for an act 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
at time T.  It is
formalized as follows:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
         (1)    [
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
        (2)    
</P>
<IMAGE TYPE="FIGURE"/>
<P>
         (2a) 		
</P>
<IMAGE TYPE="FIGURE"/>
<P>
         (2a1) 		    
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
         (2a2) 				
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Clause (1) of the definition models Morgenstern's second observation,
namely that agents do not need a recipe to perform a basic-level
action, i.e., one executable at will
 <REF/>. For non-basic-level actions (Clause (2)), the agent of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
(either a single agent (2a1) or a
group of agents (2a2)) must believe that some set of acts, 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and constraints, 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
constitute a recipe for
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
</P>
</DIV>
<DIV ID="3.2" DEPTH="2" R-NO="2"><HEADER>    Identifying Parameters: id.params
</HEADER>
<P>
An agent must also be able to identify the parameters of an act
</P>
<IMAGE TYPE="FIGURE"/>
<P>
to be able to perform it.  For example, if an agent is told
 ``remove the flywheel,'' as in the dialogue of Figure <CREF/>, the agent must be able to identify the flywheel in question.  The
relation 
<!-- MATH: $id.params(G,\alpha,T)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is used to represent that agent Gcan identify the parameters of act 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
at time T.  If 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 is of the form 
<!-- MATH: $\bar{\alpha}(p_1,...,p_n)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
then
<!-- MATH: $id.params(G,\alpha,T)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is true if G can identify each of the pi.
To do so, G must have a description of each pi that is suitable
for 
<!-- MATH: $\bar{\alpha}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
The relation id.params is defined as
follows:
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
The ability to identify an object is highly context dependent.  For
example, as Appelt points out <REF/>, ``the
description that one must know to carry out a plan requiring the
identification of `John's residence' may be quite different depending
on whether one is going to visit him, or mail him a letter.''  The
function 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in the above definition is an oracle function
intended to model the context-dependent nature of parameter
identification.  This function returns a suitable identification
 constraint <REF/> for a parameter pi in the context of an act-type 
<!-- MATH: $\bar{\alpha}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
For example, in the case of
sending a letter to John's residence, the constraint produced by the
oracle function would be that John's residence be described by a
postal address.
</P>
<P>
The relation 
<!-- MATH: $has.sat.descr(G,P,C,T)$ -->
has.sat.descr(G,P,C,T) holds of an agent G, a
parameter description P, an identification constraint C, and a
time T, if G has a suitable description, as determined by C, of
the object described as P at time T.  To formalize this relation,
we utilize Kronfeld's <REF/> notion of an
individuating set.  An agent's individuating set for an object is a
maximal set of terms such that each term is believed by the agent to
denote that object.  For example, an agent's individuating set for
John's residence might include its postal address as well as an
identifying physical description such as ``the only yellow house on
Cherry Street.''  To model individuating sets, we introduce a function
IS(G,P,T); the function returns an agent G's individuating set at
time T for the object that G believes can be described as P.
This function is based on similar elements of the formal language that
Appelt and Kronfeld <REF/> introduce as part
of their theory of referring.  The function returns a set that
contains P as well as the other descriptions that G has for the
object that it believes P denotes.
</P>
<P>
For an agent to suitably identify a parameter described as P, the
agent must have a description, 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
of the parameter such that
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is of the appropriate sort.  For example, for an agent to
visit John's residence, it is not sufficient for the agent to believe
that the description ``John's residence'' refers to the place where
John lives.  Rather, the agent needs another description of John's
residence, one such as ``the only yellow house on Cherry Street,''
that is appropriate for the purpose of visiting him.  To model an
agent's ability to identify a parameter (described as P) for some
purpose, we thus require that the agent have an individuating set for
the parameter that contains a description 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
such that
</P>
<IMAGE TYPE="FIGURE"/>
<P>
satisfies the identification constraint that derives from
the purpose.  The definition of has.sat.descr is thus as
 follows:     
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
 [
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
The predicate 
<!-- MATH: ${\it suff\!.for.id}(C,P^\prime)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is true if the
constraint C applies to the parameter description 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
The
oracle function 
<!-- MATH: ${\cal F}(\bar{\alpha},p_i)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in id.params
produces the appropriate identification constraint on pi given
<!-- MATH: $\bar{\alpha}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
</P>
</DIV>
</DIV>
<DIV ID="4" DEPTH="1" R-NO="4"><HEADER>    The Role of Knowledge Preconditions in Language Processing
</HEADER>
<P>
We now show how the requirements of knowledge preconditions can be
used in discourse processing.  Our model of discourse processing is
based on the theory of discourse structure proposed by Grosz and
Sidner <REF/>.  According to their theory,
discourse structure consists of three interrelated components: a
linguistic structure, an attentional state, and an intentional
structure.  The linguistic structure consists of discourse
 segments and an embedding relationship among  them; the bold rule in Figure    <CREF/> indicates the linguistic structure of that discourse.  Attentional state is an abstraction of
the discourse participants' focus of attention; it serves as a record
of those entities that are salient at any point in the discourse.
Intentional structure is comprised of discourse segment purposes and
their interrelationships, particularly that of dominance.  A
discourse segment purpose, or DSP, is a
Gricean-like <REF/> intention that leads to the
initiation of a discourse segment.  One DSP is dominated by another if
the satisfaction of the first provides part of the satisfaction of the
second.
</P>
<P>
Intentional structure plays a central role in discourse processing; an
agent's comprehension of the utterances in a discourse relies on the
 recognition of this structure <REF/>.  Grosz and Sidner <REF/> proposed SharedPlans to provide a
basis for recognizing intentional structure.  They argued that
discourses are fundamentally collaborative, and hence that a model of
shared plans provides a more appropriate basis for discourse
processing than a model of single-agent plans.  However, the
connection between SharedPlans and intentional structure was never
specified.
</P>
<DIV ID="4.1" DEPTH="2" R-NO="1"><HEADER>  SharedPlans as Intentional Structure </HEADER>
<P>
We have developed a model of discourse processing that provides that
 connection <REF/>.  Figure 3 illustrates the role of SharedPlans in modeling intentional structure.  Each segment of a
discourse has an associated SharedPlan.  The purpose of the segment is
 taken to be intention that (Int.Th <REF/>) the discourse participants form that plan.  This intention is held by the
agent who initiates the segment.  In what follows, we will refer to
that participant as the initiating conversational participant or ICP;
 the other participant is the OCP <REF/>.  Dominance relationships between DSPs are modeled using subsidiary
relationships between SharedPlans.  One plan is subsidiary to another
if the completion of the first plan contributes to the completion of
the second.  Subsidiary relationships are discussed in more detail in
 Section <CREF/> 
</P>
<P>
The utterances of a discourse are understood in terms of their
contribution to the SharedPlans associated with the segments of the
discourse.  Those segments that have been completed at the time of
processing an utterance have a full SharedPlan (FSP) associated with
them (e.g., segment (2) in Figure 3), while those that have not have a
partial SharedPlan (PSP) (e.g., segments (1) and (3) in Figure 3).
</P>
<P>
<!-- MATH: $\psfig{figure=struct-ijcai.eps,width=3.37in}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Figure 3: Modeling Intentional Structure
</P>
<P>
For each utterance of a discourse, an agent must determine whether the
utterance begins a new segment of the discourse, contributes to the
 current segment, or completes it <REF/>.  For an utterance to begin a new segment, it must indicate the initiation of a
subsidiary plan.  This case is described in further detail below.  For
an utterance to contribute to the current segment, it must advance the
partial SharedPlan associated with the segment towards completion.
That is, it must establish one of the beliefs or intentions required
for the discourse participants to have a full SharedPlan, but missing
from their current partial SharedPlan.  For an utterance to complete
the current segment, it must indicate that the purpose of that segment
has been satisfied.  For that to be the case, the SharedPlan
associated with the segment must be an FSP rather than a PSP.  That
is, all of the beliefs and intentions required of an FSP, as indicated
 in Figure <CREF/>, must have been established over the course of the segment.
</P>
<P>
A detailed description of the implemented algorithms used in modeling
 each of these cases can be found elsewhere <REF/>.  Here, we focus on the use of knowledge preconditions in accounting for the
initiation of information-seeking subdialogues.  We use the dialogue
 in Figure <CREF/> as an example and assume the role of the Expert  (participant ``E'') in analyzing the discourse.  The dialogue in Figure    <CREF/> was extracted from a larger discourse in which the Expert and
Apprentice (participant ``A'') are collaborating on removing the pump
of an air compressor.  We thus take the purpose of the larger
discourse to be
</P>
<P>
   DSP1=
</P>
<IMAGE TYPE="FIGURE"/>
<P>
        where ac1 represents the air compressor the agents        are working on. 
</P>
</DIV>
<DIV ID="4.2" DEPTH="2" R-NO="2"><HEADER>    Accounting for the Initiation of New Discourse Segments
</HEADER>
<P>
To make sense of an utterance, an agent must provide an
explanation for it in the form of an answer to the question, ``Why did
 the speaker say that to me?'' <REF/>.  An OCP must provide a similar explanation for an ICP's initiation of a new
discourse segment.  This explanation takes the form of an answer to
the question ``Why does the ICP want to engage in a segment with
purpose DSPj at this point in our discourse?''; i.e., ``How is
DSPj related to what we were talking about before?''  Subsidiary
relationships between SharedPlans provide the basis for modeling the
OCP's reasoning.
</P>
<P>
One plan is subsidiary to another if the completion of the first plan
contributes to the completion of the second.  The most basic example
of this relationship occurs within the FSP definition itself.  As
 indicated in Figure <CREF/>, a full plan for an act  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 includes full plans for each subact in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 's recipe as components
(requirements (2c) and (3b)).  The plans for the subacts thus
contribute to the plan for 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and are therefore subsidiary to
it.
</P>
<P>
Subsidiary relationships may also arise in response to the other
requirements of the FSP definition.  For example, as discussed in
 Section <CREF/>, the BCBA operator used to model requirement (2b) specifies that to be able to perform an act 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
an agent must (1) have a recipe for 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
(
<!-- MATH: $has.recipe$ -->
has.recipe), (2) be
able to identify the parameters of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
(
<!-- MATH: $has.sat.descr$ -->
has.sat.descr), and
(3) have satisfied the constraints associated with performing
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
The first of these requirements provides an explanation for
 the first subdialogue in Figure <CREF/>. 
</P>
<P>
 The purpose of this subdialogue is represented as      DSP2=
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 		  Achieve(has.recipe(a,				
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and can be glossed as ``the Apprentice
intends that the agents collaborate on his obtaining a recipe for the
act of removing the flywheel of the air compressor.''  To account for
the Apprentice's initiation of this subdialogue, the Expert must
determine the relationship of DSP2 to the purpose of the agents'
preceding discourse, namely DSP1. In this case, the Expert can
reason that the Apprentice wants to engage in the subdialogue to
obtain a recipe for the act of removing the flywheel so that he will
be able to perform that act as part of the agents' SharedPlan to
remove the pump.  The plan in DSP2 is thus subsidiary to that in
DSP1 by virtue of a knowledge precondition requirement of the
latter plan.
</P>
<P>
 Figure <CREF/> illustrates this analysis.  Each box in the figure corresponds to a discourse segment and contains the SharedPlan
used to model the segment's purpose.  The SharedPlans are labeled so
as to be co-indexed with the DSPs discussed above.  The arrows
indicate subsidiary relationships between SharedPlans, as explained by
the text that adjoins them.  When plan Pj is subsidiary to plan Pi,
DSPj is dominated by DSPi.
</P>
<P>
The information represented within each SharedPlan in
 Figure <CREF/> is separated into two parts.  Those beliefs and intentions that have been established at the time of the analysis
are shown above the dotted line, while those that remain to be
established, but that are used in determining subsidiary
relationships, are shown below the line.  The index in square brackets
to the right of each constituent indicates the FSP requirement from
which the constituent arose.
</P>
<P>
 As indicated in Figure <CREF/>, the initiation of the second  subdialogue in Figure <CREF/> is explained similarly. This time, however, it is the need to identify parameters of acts
(requirement (2) above) that leads to the initiation of the
subdialogue.  In addition, the parameter in question is a parameter of
an act in a subsidiary individual plan of the Apprentice's.
</P>
</DIV>
<DIV ID="4.3" DEPTH="2" R-NO="3"><HEADER>  Discussion </HEADER>
<P>
In this paper, we have shown that information-seeking subdialogues may
be explained on the basis of knowledge precondition requirements.  Our
account of such subdialogues fits within a general framework for
discourse processing in which the purpose of a subdialogue is modeled
using a SharedPlan and is related to the purposes of other
subdialogues based on the requirements of the FSP definition.
 Elsewhere <REF/>, we show that correction and subtask subdialogues, among others, may also be accounted for in this manner.
</P>
<P>
In contrast, alternative plan-based accounts of dialogue understanding
introduce multiple types of plans to account for the utterances in a
discourse.  For example, Litman and Allen <REF/>,
propose the use of two types of plans to model clarification and
correction subdialogues: discourse plans and domain
plans.  Domain plans represent knowledge about a task, while
discourse plans represent conversational relationships between
utterances and plans.  Litman and Allen provide operators for the
following discourse plans:
<ITEMIZE>
<ITEM>INTRODUCE-PLAN: introduce a new plan for discussion
</ITEM>
<ITEM>CONTINUE-PLAN: execute the next action in a plan
</ITEM>
<ITEM>TRACK-PLAN: talk about the execution of an action
</ITEM>
<ITEM>MODIFY-PLAN: introduce a new plan by modifying a previous one
</ITEM>
<ITEM>CORRECT-PLAN: correct a plan
</ITEM>
<ITEM>IDENTIFY-PARAMETER: identify a parameter of an action in a plan
</ITEM>
</ITEMIZE>
</P>
<P>
Under our approach, the recognition of discourse plans is unnecessary.
The fact that a speaker is using an utterance to, for example,
introduce a plan, or track a plan, or identify a parameter, need not
be explicitly recognized for the purposes of utterance interpretation.
Furthermore, we would argue that such facts are not intended to be
recognized (cf. Grice <REF/>).  Rather, they simply fall
out of recognizing the relationship of an utterance to the current
discourse structure, i.e., the currently active SharedPlans.  For
example, INTRODUCE-PLAN corresponds to initiating a new discourse
segment, CONTINUE- or TRACK-PLAN to contributing to the current
segment, and IDENTIFY-PARAMETER to initiating a new segment to satisfy
a 
<!-- MATH: $has.sat.descr$ -->
has.sat.descr knowledge precondition requirement.  Although the
initiation of a new SharedPlan corresponds to the initiation of a new
discourse segment under our approach, it is the SharedPlan that must
be recognized and not a discourse plan that refers to that SharedPlan.
</P>
<P>
Lambert and Carberry <REF/> have extended
Litman and Allen's approach by introducing a third type of plan.  Problem-solving plans, such as BUILD-PLAN and INSTANTIATE-VARS, are
used to model the process by which agents construct domain plans.
Under our approach, the need to explicitly recognize problem-solving
plans is also avoided.  The fact that an agent is building a plan or
instantiating a variable is a byproduct of understanding an utterance
by relating it to the current discourse structure.  BUILD-PLAN
corresponds to initiating a new discourse segment to satisfy a
<!-- MATH: $has.recipe$ -->
has.recipe knowledge precondition requirement, while
INSTANTIATE-VARS corresponds to initiating one to satisfy a
<!-- MATH: $has.sat.descr$ -->
has.sat.descr requirement.  Unlike Lambert and Carberry's approach,
however, and Litman and Allen's as well, our approach actually
recognizes this structure.  The other approaches are essentially
utterance-to-utterance based and thus do not recognize discourse
segments as separate units.
</P>
<P>
Ramshaw <REF/> has added a different third type of
plan, exploration plans, to Litman and Allen's two types.
Exploration plans are intended to model the process by which agents
explore courses of actions.  Although we have not yet incorporated
such reasoning into our model, we hypothesize that the exploration of
plans can be modeled, without the introduction of a new plan type, by
reasoning about an agent's potential intentions and the process
by which they become full-fledged intentions
 <REF/>,<REF/>. 
</P>
<P>
These alternative approaches share an important property that
distinguishes them from our approach; they take a data-structure
view of plans, rather than a mental phenomenon view
 <REF/>.  Whereas data-structure plans are essentially ``recipes-for-action,'' mental phenomenon plans are a ``structured
collection of beliefs and intentions''
 <REF/>, pg. 77].  Data-structure plans thus describe what an agent is doing with an utterance, but not why the agent is doing it.  For
example, although the constraints of Litman and Allen's
IDENTIFY-PARAMETER discourse plan force the plan to be related to
another plan that involves the parameter to be identified,
IDENTIFY-PARAMETER does not explain why this information is desired;
it does not capture that agents need to know parameters to be
 able to perform acts involving them.  It thus fails to model the essential knowledge precondition nature of identifying a parameter.  Although it is
possible to impose a mental phenomenon interpretation on top of a
data-structure plan, doing so does not result in a mental phenomenon
 plan <REF/>.  Saying that G1 intends to IDENTIFY a PARAMETER fails to address why G1 intends to do so.
</P>
<P>
The need to explain an utterance is not unique to interpretation.
Moore and Paris <REF/> have shown that a similar
need exists in generation.  In particular, they have argued that
RST-based text plans must be augmented with intentional structure.
Otherwise, a system has no record of why it said what it did and is
thus unable to respond effectively if a hearer does not understand or
accept its utterances.
</P>
</DIV>
</DIV>
<DIV ID="5" DEPTH="1" R-NO="5"><HEADER>  Conclusion </HEADER>
<P>
In this paper, we have presented an axiomatization of knowledge
preconditions for the SharedPlan model of collaborative activity
 <REF/>.  We have also shown how the requirements of knowledge preconditions can be used to account for information-seeking
subdialogues in discourse.  Our account of this phenomenon fits within
a general framework for discourse processing in which SharedPlans and
relationships among them are used to model the intentional component
of Grosz and Sidner's <REF/> theory of discourse
structure.  Unlike the alternative approaches, our approach recognizes
and makes use of discourse structure.  In addition, it does not require
the introduction of new plan types.
</P>
<DIV ID="5.1" DEPTH="2" R-NO="1"><HEADER>Bibliography </HEADER>
<P>
D. Appelt and A. Kronfeld.
A computational model of referring.
In Proceedings of IJCAI-87, pages 640-647, Milan, Italy, 1987.
</P>
<P>
D. E. Appelt.
Some pragmatic issues in the planning of definite and indefinite noun
  phrases.
In Proceedings of the 23rd Annual Meeting of the ACL, pages
  198-203, Chicago, IL, 1985.
</P>
<P>
M. E. Bratman, D. J. Israel, and M. E. Pollack.
Plans and resource-bounded practical reasoning.
Computational Intelligence, 14:349-355, 1988.
</P>
<P>
H. P. Grice.
Utterer's meaning and intentions.
Philosophical Review, 68(2):147-177, 1969.
</P>
<P>
B. J. Grosz and S. Kraus.
Collaborative plans for group activities.
In Proceedings of IJCAI-93, pages 367-373, Chambery, Savoie,
  France, 1993.
</P>
<P>
B. J. Grosz and C. L. Sidner.
Attention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175-204, 1986.
</P>
<P>
B. J. Grosz and C. L. Sidner.
Plans for discourse.
In P. R. Cohen, J. L. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 417-444. MIT Press, Cambridge, MA, 1990.
</P>
<P>
B. J. Grosz [Deutsch].
The structure of task-oriented dialogs.
In IEEE Symposium on Speech Recognition: Contributed Papers,
  pages 250-253, Pittsburgh, PA, 1974.
</P>
<P>
J. R. Hobbs.
Ontological promiscuity.
In Proceedings of the 23rd Annual Meeting of the ACL, pages
  61-69, Chicago, IL, 1985.
</P>
<P>
A. Kronfeld.
Donnellan's distinction and a computational model of reference.
In Proceedings of the 24th Annual Meeting of the ACL, pages
  186-191, New York, NY, 1986.
</P>
<P>
L. Lambert and S. Carberry.
A tripartite plan-based model of dialogue.
In Proceedings of the 29th Annual Meeting of the ACL, pages
  47-54, Berkeley, CA, 1991.
</P>
<P>
D. J. Litman and J. F. Allen.
A plan recognition model for subdialogues in conversations.
Cognitive Science, 11:163-200, 1987.
</P>
<P>
K. E. Lochbaum, B. J. Grosz, and C. L. Sidner.
Models of plans to support communication: An initial report.
In Proceedings of AAAI-90, pages 485-490, Boston, MA, 1990.
</P>
<P>
K. E. Lochbaum.
Using Collaborative Plans to Model the Intentional Structure of
  Discourse.
PhD thesis, Harvard University, 1994.
</P>
<P>
J. D. Moore and C. L. Paris.
Planning text for advisory dialogues: Capturing intentional and
  rhetorical information.
Computational Linguistics, 19(4):651-694, December 1993.
</P>
<P>
R. C. Moore.
A formal theory of knowledge and action.
In J. R. Hobbs and R. C. Moore, editors, Formal Theories of the
  Commonsense World, pages 319-358. Ablex Publishing Corp., Norwood, NJ,
  1985.
</P>
<P>
L. Morgenstern.
Knowledge preconditions for actions and plans.
In Proceedings of IJCAI-87, pages 867-874, Milan, Italy, 1987.
</P>
<P>
L. Morgenstern.
Foundations of a Logic of Knowledge, Action, and Communication.
PhD thesis, New York University, 1988.
</P>
<P>
M. E. Pollack.
Plans as complex mental attitudes.
In P. R. Cohen, J. L. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 78-104. MIT Press, Cambridge, MA, 1990.
</P>
<P>
L. A. Ramshaw.
A three-level model for plan exploration.
In Proceedings of the 29th Annual Meeting of the ACL, pages
  39-46, Berkeley, CA, 1991.
</P>
<P>
C. L. Sidner and D. J. Israel.
Recognizing intended meaning and speakers' plans.
In Proceedings of IJCAI-81, pages 203-208, Vancouver, British
  Columbia, Canada, 1981.
</P>
<DIV ID="5.1.1" DEPTH="3" R-NO="1"><HEADER>Footnotes</HEADER>
<P>
  This work was done as part of my dissertation
research at Harvard University, and was supported by a Bellcore
graduate fellowship and by U S WEST Advanced Technologies.  I would
like to thank Barbara Grosz, Stuart Shieber, and Candy Sidner for
their helpful comments, discussions, and insights on this work.
  Unless otherwise indicated, we will use the term
``agent'' to refer to both individual agents and sets of agents.
  This description of a PSP is
only a rough, though useful, approximation to the formal definition
given by Grosz and Kraus <REF/>.
  A comparison of our
formalization with those of Morgenstern <REF/> and
 Moore <REF/> can be found elsewhere <REF/>. 
  Basic-level actions are by their
nature single-agent actions.
  A more precise account of what it means to be able
to identify an object is beyond the scope of this paper; for further
details, see the discussions by Hobbs <REF/>, Appelt and
Kronfeld <REF/>, and
Morgenstern <REF/>.
  The term discourse segment is a generalization of
the term subdialogue.  Whereas the term discourse segment applies to
all types of discourse, the term subdialogue is reserved for segments
that occur within dialogues.
  We have omitted the time parameters for simplicity of
exposition.
  For
simplicity of exposition, we will take participant ``E'' to be female
and participant ``A'' to be male.
  We describe
 a method for recognizing DSPs of this form elsewhere <REF/>. 
  Although Lambert and
Carberry <REF/> adopt
Pollack's <REF/> terminology in presenting their
theory, their ``plans'' are not mental state plans in Pollack's
sense.
  Although Litman and
Allen describe IDENTIFY-PARAMETER as providing ``a suitable
description for a term instantiating a parameter of an action such
that the hearer is then able to execute the action''
 <REF/>, pg. 173], the IDENTIFY-PARAMETER operator itself does not include a formalization of the last condition,
i.e., that the parameter description should enable the execution of
the action.
</P>
</DIV>
</DIV>
</DIV>
</BODY>
</MINIMAL-DOC>
