<?xml version='1.0'?>
<!DOCTYPE MINIMAL-DOC SYSTEM "mini.dtd">
<MINIMAL-DOC>
<TITLE>DUAL-CODING THEORY AND CONNECTIONIST LEXICAL
SELECTION</TITLE>
<ABSTRACT>
<P>
We introduce the bilingual dual-coding theory as a model for bilingual
mental representation. Based on this model, lexical selection neural
networks are implemented for a connectionist transfer project in machine
translation.
</P>
</ABSTRACT>
<BODY>
<DIV ID="1" DEPTH="1" R-NO="1"><HEADER>  Introduction </HEADER>
<P>
Psycholinguistic knowledge would be greatly helpful, as we believe, in
constructing an artificial language processing system. As for machine
translation, we should take advantage of our understandings of (1) how the
languages are represented in human mind; (2) how the representation is
mapped from one language to another; (3) how the representation and mapping
are acquired by human.
</P>
<P>
 The bilingual dual-coding theory <REF/> partially answers the above questions. It depicts the verbal representations for two different
languages as two separate but connected logogen systems, characterizes the
translation process as the activation along the connections between the
logogen systems, and attributes the acquisition of the representation to
some unspecified statistical processes.
</P>
<P>
 We have explored an information theoretical neural network <REF/> that can acquire the verbal associations in the dual-coding theory.  It
provides a learnable lexical selection sub-system for a connectionist
transfer project in machine translation.
</P>
</DIV>
<DIV ID="2" DEPTH="1" R-NO="2"><HEADER>  Dual-Coding Theory </HEADER>
<P>
There is a well-known debate in psycholinguistics concerning the bilingual
mental representation: independence position assumes that bilingual memory
is represented by two functionally independent storage and retrieval
systems, whereas interdependence position hypothesizes that all information
of languages exists in a common memory store. Studies on cross-language
transfer and cross-language priming have provided evidence for both
 hypotheses <REF/>,<REF/>. 
</P>
<P>
Dual-coding theory explains the coexistence of independent and
interdependent phenomena with separate but connected structures.  The
general dual-coding theory hypothesizes that human represents language with
dual systems -- the verbal system and the imagery system. The elements of
the verbal system are logogens for words in a language. The elements
of the imagery system, called ``imagens'', are connected to the
logogens in the verbal systems via referential connections. Logogens
in a verbal system are also interconnected with associative
connections.  The bilingual dual-coding theory proposes an architecture in
which a common imagery system is connected to two verbal systems, and the
two verbal systems are interconnected to each other via associative
 connections [Figure <CREF/>]. Unlike the within-language associations, which are rich and diverse, these between-language associations involve
primarily translation equivalent terms that are experienced together
frequently.  The interconnections among the three systems explain the
interdependent functional behavior. On the other hand, the different
characteristics of within-language and between-language associations
account for the independent functional behavior.
</P>
<P>
Based on the above structural assumption, dual-coding theory proposes a
parallel set of processing assumptions. Activation of connections between
referentially related imagens and logogens is called referential
processing.  Naming objects and imaging to words are prototypical
examples. Activation of associative connections between logogens is called
associative processing. Lexical translation is an example of
associative processing between two languages.
</P>
</DIV>
<DIV ID="3" DEPTH="1" R-NO="3"><HEADER>  Connectionist Lexical Selection </HEADER>
<DIV ID="3.1" DEPTH="2" R-NO="1"><HEADER>  Lexical Selection </HEADER>
<P>
Lexical selection is the task of choosing target language words that accurately
reflect the meaning of the corresponding source language words.
 It plays an important role in machine translation <REF/>. 
</P>
<P>
A common lexical selection practice involves an intermediate representation.
It disambiguates the source language words to entities in the intermediate
representation, then maps from the entities to the target lexical entries. This
 intermediate representation may be Lexical Concept Structure <REF/>  or interlingua <REF/>. This engineering approach requires great effort in designing the representation and the mapping rules.
</P>
<P>
Currently, there are some efforts in statistical lexical selection. A
target language word W
<!-- MATH: $_{\hbox{t}}$ -->
<EQN/>
can be selected with the
posterior probability Pr(W
<!-- MATH: $_{\hbox{t}}$ -->
<EQN/>W
<!-- MATH: $_{\hbox{s}}$ -->
<EQN/>)
given the source language word W
<!-- MATH: $_{\hbox{s}}$ -->
<EQN/>.
Several target language lexical entries may be selected for a single source
language word. Then the correct selections can be identified by the
 language model of the target language <REF/>. This approach is learnable. However, the accuracy is low. One reason is that it does not use
any structural information of a language.
</P>
<P>
In next subsections, we propose information-theoretical networks
based on the bilingual dual-coding theory for lexical selection.
</P>
</DIV>
<DIV ID="3.2" DEPTH="2" R-NO="2"><HEADER>  Information-Theoretical Networks </HEADER>
<P>
Information-theoretical network is a neural network formalism that is
capable of doing associations between two layers of representations.  The
associations can be obtained statistically according to the network's
experiences.
</P>
<P>
An information-theoretical network has two layers. Each unit of a layer
represents an element in the input or output of a training pattern, which
might be a logogen or a word.  Units in different layers are connected.
The weight of the connection between unit i in one layer and unit
j in the other layer is assigned with the mutual information between
the elements represented by the two units
</P>
<P>
(1) 
<!-- MATH: $w_{ij}   =   I(v_{i}, v_{j})   =   \log (Pr(v_{j} v_{i}) /
Pr(v_{i}))$ -->
 <EQN/> 
</P>
<P>
Each layer also contains a bias unit, which is always activated.
The weight of the connection between the bias unit in one layer and
unit j in the other layer is
</P>
<P>
(2)  
<!-- MATH: $w_{0j}   =   \log Pr(v_{j})$ -->
<EQN/>
</P>
<P>
Both the information-theoretical network and the back-propagation network
compute the posterior probabilities for an association task
 <REF/>,<REF/>. However, only the information-theoretical network is isomorphic to the directly interconnected verbal systems in the
dual-coding theory.  Besides, an information-theoretical network has the
following advantages: (1) it learns fast. The network can learn in a single
pass without gradient decent. (2) it is adaptive. It can incrementally
adapt to new experiences simply by adding new data to the training samples
and modifying the associations according to the changed statistics. These
make the network more psychologically plausible.
</P>
</DIV>
<DIV ID="3.3" DEPTH="2" R-NO="3"><HEADER>  Lexical Selection as an Associative Process </HEADER>
<P>
We tried to map source language f-structures to target language f-structure
 in a connectionist transfer project <REF/>.  Functionally, there were two sub-tasks: 1. finding the target sub-structures, their phrasal
categories and their corresponding source structures; 2. finding the head
of a target structure. The second sub-task is a problem of lexical
selection.  It was first implemented with a back-propagation network.
</P>
<P>
We replaced the back-propagation networks for lexical selection with
information-theoretical networks simulating the associative process in
the dual-coding theory. The networks have two layers of units. Each source
(target) language lexical item is represented by a unit in the input
(output) layer. One network is constructed for each phrasal category (NP, VP,
AP, etc.).
</P>
<P>
The networks works in the following way: for a target-language f-structure
to be generated, the transfer system knows its phrasal category and its
corresponding source-language f-structure from the networks that perform
the sub-task 1. It then activates the lexical selection network for that
phrasal category with the input units that correspond to the heads of the
source language f-structure and its sub-structures. Through the connections
between the two layers, the output units are activated, and the lexical
item that corresponds to the most active output unit is selected as the
head of the target f-structure. The following example illustrates how the
system selects the head anmelden for the German XCOMP sub-structure when
it does the transfer from
</P>
<P>
<!-- MATH: $[_{\em sentence}$ -->
<EQN/>
<!-- MATH: $[_{\em subj}$ -->
<EQN/> I] would 
<!-- MATH: $[_{\em xcomp}$ -->
<EQN/>
<!-- MATH: $[_{\em
subj}$ -->
<EQN/> I] like 
<!-- MATH: $[_{\em xcomp}$ -->
<EQN/>
<!-- MATH: $[_{\em subj}$ -->
<EQN/> I] register 
<!-- MATH: $[_{\em
pp-adj}$ -->
<EQN/> for the conference]]]] to
</P>
<P>
<!-- MATH: $[_{\em sentence}$ -->
<EQN/>
<!-- MATH: $[_{\em subj}$ -->
<EQN/> Ich] werde 
<!-- MATH: $[_{\em xcomp}$ -->
<EQN/>
<!-- MATH: $[_{\em
subj}$ -->
<EQN/> Ich]
<!-- MATH: $[_{\em adj}$ -->
<EQN/> gerne] anmelden 
<!-- MATH: $[_{\em pp-adj}$ -->
<EQN/>
fuer
 der Konferenz]]]. 
</P>
<P>
Since the structure networks find that there is a VP sub-structure of XCOMP
in the target structure whose corresponding input structure is 
<!-- MATH: $[_{\em
xcomp}$ -->
<EQN/>
<!-- MATH: $[_{\em subj}$ -->
<EQN/> I] to register 
<!-- MATH: $[_{\em pp-adj}$ -->
<EQN/> for the
conference]]], it activates the VP lexical selection network's input
units for I, register and conference. By propagating the
activation via the associative connections, the unit for anmelden is
the most active output. Therefore, anmelden is chosen as the head of
the xcomp sub-structure.
</P>
</DIV>
<DIV ID="3.4" DEPTH="2" R-NO="4"><HEADER>  Preliminary Result </HEADER>
<P>
The domain of our work was the Conference Registration Telephony
Conversations.  The lexicon for the task contained about 500 English and
500 German words. There were 300 English/German f-structure pairs available
 from other research tasks <REF/>.  A separate set of 154 sentential f-structures was used to test the generalization performance of
the system. The testing data was collected for an independent task
 <REF/>. 
</P>
<P>
From the 300 sentential f-structure pairs, every German VP sub-structure is
extracted and labeled with its English counterpart. The English
counterpart's head and its immediate sub-structures' heads serve as the
input in a sample of VP association, and the German f-structure's head
become the output of the association.  For the above example, the
association ([input I, register, conference]
<!-- MATH: $[_{output}$ -->
[outputanmelden]) is a sample drawn from the f-structures for the VP network.
The training samples for all the other networks are created in the same
way.
</P>
<P>
The accuracy of our system with information-theoretical network lexical
selection is lower than the one with back-propagation networks (around 84%
versus around 92%) for the training data. However, the generalization
performance on the unseen inputs is better (around 70% versus around
62%).  The information-theoretical networks do not over-learn as the
back-propagation networks. This is partially due to the reduced number of
free parameters in the information-theoretical networks.
</P>
</DIV>
</DIV>
<DIV ID="4" DEPTH="1" R-NO="4"><HEADER>  Summary </HEADER>
<P>
The lexical selection approach discussed here has two advantages. First, it
is learnable. Little human effort on knowledge engineering is
required. Secondly, it is psycholinguistically well-founded in that the
approach adopts a local activation processing model instead of relies upon
symbol passing, as symbolic systems usually do.
</P>
<DIV ID="4.1" DEPTH="2" R-NO="1"><HEADER>Bibliography </HEADER>
<P>
P. F. Brown and et al.
A statistical approach to machine translation.
Computational Linguistics, 16(2):73-85, 1990.
</P>
<P>
A. M. de Groot and G. L. Nas.
Lexical representation of cognates and noncognates in compound
  bilinguals.
Journal of Memory and Language, 30(1), 1991.
</P>
<P>
B. J. Dorr.
Conceptual basis of the lexicon in machine translation.
Technical Report A.I. Memo No. 1166, Artificial Intelligence
  Laboratory, MIT, August, 1989.
</P>
<P>
A. L. Gorin and S. E. Levinson.
Adaptive acquisition of language.
Technical report, Speech Research Department, ATT Bell
  Laboratories, Murray Hill, 1989.
</P>
<P>
A. N. Jain.
Parsec: A connectionist learning architecture for parsing spoken
  language.
Technical Report CMU-CS-91-208, Carnegie Mellon University, 1991.
</P>
<P>
W. E. Lambert, J. Havelka and C. Crosby.
The influence of language acquisition contexts on bilingualism.
Journal of Abnormal and Social Psychology, 56, 1958.
</P>
<P>
S. Nirenberg, V. Raskin and A. B. Tucker.
The structure of interlingua in translator.
In S. Nirenburg, editor, Machine Translation: Theoretical
  and Methodological Issues. Cambridge University Press, Cambridge, England,
  1987.
</P>
<P>
L. Osterholtz and et al.
Janus: a multi-lingual speech to speech translation system.
In Proceedings of the IEEE International Conference on
  Acoustics, Speech and Signal Processing, volume 1, pages 209-212. IEEE,
  1992.
</P>
<P>
A. Paivio.
Mental Representations: A Dual Coding Approach.
Oxford University Press, New York, 1986.
</P>
<P>
J. Pustejovsky and S. Nirenburg.
Lexical selection in the process of language generation.
In Proceedings of the 25th Annual Conference of the Association
  for Computational Linguistics, pages 201-206, Standford University,
  Standford, CA, 1987.
</P>
<P>
A. Robinson.
Practical network design and implementation.
In Cambridge Neural Network Summer School, 1992.
</P>
<P>
Y. Wang and A. Waibel.
Connectionist transfer in machine translation.
In prepare, 1994.
</P>
<DIV ID="4.1.1" DEPTH="3" R-NO="1"><HEADER>Footnotes</HEADER>
<P>
  This work was partly supported
by ARPA and ATR Interpreting Telephony Research Laboratorie.
  Where vi means the event that unit i is activated.
  The f-structures are simplified here for the
sake of conciseness.
</P>
</DIV>
</DIV>
</DIV>
</BODY>
</MINIMAL-DOC>
