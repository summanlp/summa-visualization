
We present a new model of natural language based on the
concept of construction, consisting of a set of
features of form, a set of semantic and pragmatic
conditions describing its application context,
and a description of its meaning.
The model gives us a better
handle on phenomena of real language than
the standard approaches, such as syntax + semantics a la Montague.
It is also an alternative to the standard design based on the
pipeline syntax-semantics-pragmatics (we have little to say about
morphology at this point).
Since this work has been implemented, there is
also a computational argument in favor of this approach.

We claim that a linking of form, meaning and context
is needed to accurately describe NL constructions,
both "standard" and "non-standard". However,
we are not arguing for the unsuitability of syntax for
describing a "core" of language or a universal grammar.
We believe, that such a language
core is small, and that the syntactic descriptions of this
core are naturally paired with their meanings, which produces
a construction-based universal grammar. Furthermore,
new methods are needed
to handle phenomena of real languages. In this paper, our aim is not
to come with linguistic generalizations about universal
structural properties of sentences (although, of course, we have
nothing against them), but to come with an effective method for natural
language understanding, which in addition to computational effectiveness
would also have some linguistic and psychological plausibility. 

The first argument for the linking of forms, meanings and contexts
is that it has been the experience of
of many people working in the field of computational linguistics
that combining syntactic and semantic information increases the
efficiency and effectiveness of processing; e.g.
  has shown that a semantically driven approach is superior to syntax-first approach in processing text in narrow domains.

The second group of arguments is strictly linguistic.
Many linguistic phenomena can
be naturally described by the pairing of their syntactic
form with their semantic features. Such patterns of interdependence
of syntax and semantics are common both for standard constructions,
like NPs, VPs, or Ss, and for more exotic ones, such as sentences
with "let alone". Attempts to account for those phenomena in systems
without such interdependence of syntax and semantics lead to
drastic overgeneralizations.

As an example involving familiar constructions, consider
conjunctions. It is easily seen that
particular conjunctions select clauses with a specific logical
relation between their respective meanings:
   This is a regular chair,
but you can sit on it.
   This is an old chair, but you can sit on it.
In this case, the two phrases conjoined by "but" agree if
the second clause contradicts what is typically believed about entities
mentioned in the first clause. This kind of agreement
cannot be described without access to semantic information
and world knowledge.

Virtually every construction of English displays a similar
interplay of form and meaning.  We can refer the reader to
  for a discussion e.g. of the semantic significance of the
order of modifiers of
NPs, on the semantic conditions on the progressive, etc..
Regarding more exotic constructions,
Fillmore et al. observe that
standard grammars do not handle open idioms
such as
The more you work, the easier it will get or
He doesn't eat fish, let alone shrimp. They note that
each of these expressions "exhibits properties that are not fully
predictable from independently known properties of its
 lexical make-up and its grammatical structure"   , p. 511. 

To show that these types of
constructions (and the standard ones)
can be handled computationally,
  proposed a construction formalism to build "a linguistically motivated grammar (...) compatible
with psycholinguistic results on sentence processing".
(His grammar, however, does not include contextual information, nor
handle dialogs.)

Finally, we note that
context, especially dialog context, changes the range of applicable
constructions. Everybody knows that
Kissinger thinks bananas makes sense as an answer to
the question What is (was) Nixon's favorite fruit.
And, in spoken discourse context,
"fragments" behave like open idioms; e.g.
"afternoon" might mean "in the afternoon", "two" the second of the
choices (e.g. for a meeting time), and "checking" can stand
for "from (the) checking (account)", i.e. semantically be a "source". 

The paper is organized as follows:
In Section 2, we present our NLU system and
an example that illustrates the need for contextual information
in understanding dialogs.
In Section 3 we describe
the formalism of constructions;
Section 4 presents examples of construction (from words to discourse).
Section 5 is about the role of  ontologies in the grammar.
Section 6 contains an example of parsing with a construction
grammar. In Section 7, we argue that construction-based approach
captures the intuition of
compositional semantics, and that it is an effective and reusable
encoding of linguistic knowledge.

We have built a dialog understanding system,  MINCAL,
for a small domain and a specific task, namely, for
scheduling (as well as canceling, and moving) meetings in an on-line
 calendar . This work has later been extended
to other small discourse domains (fast food,
banking, insurance premium quotes etc.).

In  MINCAL,
the grammar consists of about a hundred constructions and
a few hundred lexical entries,
and includes both sentential constructions and discourse constructions.
(Similar numbers for other applications).
The system also contains an encoding of elementary
facts about time and parameters of meetings; this background
knowledge is encoded in about a hundred Prolog clauses.

The system is capable of understanding dialogs such as this:

It contains three main components with the following functions:
1.
The parser
takes an input utterance from the user and a context
information from the discourse model.  It outputs the set of messages
corresponding to the possible meanings of
the uttered phrase.  The components of the parser are: the parser itself,
a lexicon, a set of constructions and a set of filters.
2.
The interpretation module (semantic engine)
translates the messages it receives from the
parser to a set of slots with values, corresponding to the entities recognized
in the phrase,
like [action_name reschedule],
[new_event_place "my manager's
office"]  etc.
3.
The discourse module
gathers the computed slots, the current state
of its internal database and the current context.  From this it computes the
new context, changes the database if necessary, informs the user about the
action
taken, and carries on the dialog.

 MINCAL
parser recognizes Schedule a meeting with Bob  as an instance
of sent(imp), the imperative construction consisting of a verb
and an np, in this case np(event).
  Then, in the context of the task and the question asked,
it parses, the subsequent sentences.
At each step of the dialog, the semantic engine extracts information
from the results of parsing to yield (at the end of the dialog):

At the end of the processing, another component of the
program changes the format of this data, and
schedules the meeting in an on-line calendar, xdiary.

We can make the  following observations.
(a) From the point of view of the operation of the system, we do not
care about the structure of the sentences (provided the parser
can handle them); we care only about their meanings,
and these meanings depend on context.
(b) The issue of ontology is not trivial.
Namely, we have
to distinguish not between three languages
(representations) and corresponding ontologies:
the language/ontology for parsing,
the language/ontology for representing information about the domain,
e.g. the calendar functions, and
the language/ontology for representing information about the
particular application, e.g. xdiary.
A natural question arising is the role of all three
representations in parsing.

We will talk about the interaction of form and meaning in the
next section. Now we want to make a few observations about
the context-dependence of interpreting dialogs.
In the first sentence of the above dialog,
the context is used to prevent
another reading in which with Bob modifies schedule,
as in Dance a tango with Bob!. That is, we use a contextual
rule saying that for the calendar application people do not modify
actions, nor places.
By setting up a set of such domain- and application-specific filters,
we can remove ambiguities of PP attachment in most cases, e.g. in 
   Set up a lunch in the cafeteria with my boss 
   Postpone the interview at 10 to Monday. 

In general, taking the context into account during parsing
allows the parser to focus on certain constructions rather than others,
as well as to compute certain meanings more accurately.
Using context to restrict the
constructions that are triggered during parsing
greatly reduces the number of edges our
parser has to consider, resulting in the increase of speed.
The dependence of meaning on context
is well known, e.g. deixis or deciding the reference of
pronouns. But
there are very natural examples not connected to deixis: in our dialog,
the expression 8 is interpreted only as
a time expression (and not a place or something else),
because of the context of the preceding question
(asking about the time of the meeting).
Similarly,  for computing the meaning of other
time expressions: 4 to 6 is more
likely to mean "beginning at 4 and ending at 6" than "5:56" in the
context of a question about the time of a meeting. But by using the
context the application, we can completely eliminate the second
reading, since all meetings are scheduled in 5 minute increments.

A grammar is a collection of constructions.
Each construction is given by the matrix:



The structure / vehicle

consists of formulas describing presence (or perhaps
absence) of certain features of form within
the construction, e.g.
a list of subconstructions
and the way they have been put together - in all our examples this is
concatenation, but there are
other possibilities, e.g. wrapping.
Since in practice generative grammars
use only taxemes that are "meaningful", i.e. one can associate some
meanings with their presence, we can include in our description of
constructions some of the features used by GB, GPSG or PEG
such as agreement and lexical markings.
 (cf. e.g.   and ). 

The context

consists of a set of semantic and pragmatic constraints limiting the
application of the construction. It can be viewed as a set of
preconditions that must be satisfied in order for a construction
to be used in parsing.
The message 


describes the meaning
of the construction, via a set of syntactic,
semantic and pragmatic constraints.

For example, we can analyze expressions
No, but I'll do it now/send it tomorrow/..., typically given
as an answer to a question whether something has been done,
as a discourse construction.
Its description uses such relations as
previous_utterance/p_utter,
previous_sentence/p_sent (i.e. the propositional content
of the previous utterance),
the message/meaning of S, m(S).



As we can see, the construction applies only in the context of a
previously asked
question, and its message says that the answer to the question is negative,
after which it elaborates the answer with a sentence S. 

There is no agreement on what is context, or, even more important,
on what is not. But, again, from the point of view of an abstract
computing device, a context is a collection of relations that are
interpreted as constraints. A partial list of such relations
can be found in the books on pragmatics
and it includes such relations as
speaker,
hearer,
topic,
presupposed,
assertion, question, command, declaration, 
...     . To this list we can add
previous_utterance,    
current_domain,    
current_application,    
current_question,    
default_value,     
etc.

As we have said before, we see constructions as sets of
constraints on the relation between forms, meanings and contexts.
  argues that this view arises naturally in
computational linguistics, and discusses its implications for
the syntax-semantics interface and parsing.

We do not assume any specific constraint formalism; however
we do assume that the constraints can be inherited, and
they can be nonmonotonic, that is, more specific
constraints might invalidate the inherited ones.
Also, in the process of writing a grammar we do not have to specify
all constraints at once, e.g. for no.but.S construction
there should be a relationship, like shared topic, between
the sentence S and the preceding question, however this constraint
should can be added at any time, and the lack of it should have
no impact on the rest of the grammar. We assume that the
difference of form implies the difference of meanings of two
constructions
 (cf.  p.3), but the opposite does not hold (ambiguity and multiple word senses).

Also we should note that apart from the inheritance hierarchy
grammars of constructions can be also partitioned "horizontally",
each class with its own subtheory.
For example,
  divides constructions into four types:
1.
sentence-type constructions;
2.
constituency constructions, e.g. clauses, phrases, words;
3.
valency constructions (head+dependents); and
4.
substitution constructions (e.g. pro-forms and zeros).

We can hypothesize the existence of a
universal grammar of constructions; e.g.
   p.5, says that "simple clause constructions are associated directly with
structures which reflect scenes basic to human experience".
She then presents arguments supporting this thesis.
Similar arguments can be made for other types of constructions, e.g.
basic discourse constructions (cf. e.g.
 ) and anaphoric reference
 . 

To end this general exposition, we list four basic differences between
construction grammars and other grammars.
(1) Construction grammars are not lexicalized; (2) They are not
head driven (since in real dialogs and texts it is often impossible
to find the head of a clause); (3) Parsing produces only flat, two
level, "semantic" structures - which is important from the point of view
of efficiency; (4) Ontology plays an important role in organizing
the grammar and in guiding the parsing.

The set of core constructions describes
an agent performing an action; the NP is as an
agent and a verb as an action. The English constructions
SV, SVO, SVOO, SVOC (subject-verb-object-complement), ...
could all be described in a similar fashion.
But we show only the SVO construction.



The construction specifies that the action is given by the meaning
of the VP (which we assume would consist of a V and adverbials);
the agent and object are given by the meanings of the two NPs.

For computational reasons it is often convenient to assume that meanings
are expressed as lists of


pairs together
with formulas constraining those values. We put no
restrictions on either the form or the semantics of those formulas;
in particular they can refer to extralinguistic properties.
In the "atomic" case meanings consist simply of "denotations".
For example, for "dog" and "walk" we could have (respectively)


and

.
The SV construction would then
specify the meaning of the verb is combined with the meaning
of the subject (noun) to produce  



Notice that at this point
the exact choice of notation is immaterial, e.g. we could
have written den(dog) for 

.
The point is that constructions explicitly specify how
the meanings of subconstructions contribute to the meaning of
a whole, and this can be done in a number of ways:
by logical formulas, by combining features, functionally, etc.
We will use notation which seems the most easy to read.

Construction grammars make use of
defaults (that can be overridden).
Hence action verbs do not have to be
specified as such, but stative verbs must be specified as stative.
ACTIONS and verbs can be further subdivided, e.g. along the lines
proposed by
  into motion-type, affect-type,
attention-type, etc.. Correspondingly,  the defaults would be
elaborated into AGENT  is  MOVABLE for motion-type actions (walk,
run, etc.); OBJECT  is  TARGET and complement is
MANIP for affect-type actions (e.g.
kick); AGENT  is  PERCEIVER for attention-type verbs; and so on.

Similarly, nouns would form a hierarchy in which there is a general
category ,
with a default meaning "entity",
and many subcategories 

,

,

,

,


(e.g. afternoon), etc.
(see Section 5 below).
The noun phrase can then be defined, as usual, as a noun with
modifiers (MODS), with the order to be specified for each
language separately.
And the default meaning of the NP is a conjunction
of the meaning of the noun with the relation defined by the modifiers.
Notice that the type of a noun phrase (X) is the type of its head
(as usual).



We view languages as collections of constructions which range from
words to discourse. We have seen how the same representation
scheme can be used for different constructions.
In this subsection we apply it to lexical items. For instance,
the verbs "see" and "hit" can be represented as follows:





The lexical entry specifies the semantic type of the verbs
(after
 ), and the subcategorization information.
This information determines the default
semantics of the clause in which the verb appears.
Notice that (a)
even simple words require context to get an interpretation;
we say in 


that the language code is english (but in other
cases it could also be spoken French, etc.); (b)
some pieces of information do not
have to be explicitly specified and can be
replaced by defaults.  E.g.
for "hit" we do not have to say that the subject is an agent;
it is enough to specify the semantic roles of the object and
the complement.

The simplicity of the meanings is a result of a deliberate
simplification.
In reality, the lexical meaning of any word is a much more
complicated matter.  Any review of issues of
of computational lexicography contains a list of the
types of features of lexical entries and inference methods
for natural language understanding.
For instance, in our lexicon
the messages of words may contain many of the attributes
that appear in the explanatory combinatorial dictionary of Melcuk (cf. e.g.
  and  , pp.92-101) and the functions used there. 

We end showing how one can write the matrices of the constructions
pronoun(him) and determiner(the).





Clearly, this is a very sketchy characterization of the definite
article. It says that there is a non-empty discourse context,
that it subcategorizes for a bare NP, and meaning is given
by the attribute definite. Although it captures
the main properties of "the",
we refer the reader to
  p.96-103 and   for a discussion of definite descriptions
and determiners, and for a characterization of
the attribute definite in terms of "mental spaces".

In our construction grammar
the representation of constructions is closely coupled
with a semantic taxonomy. Thus, for instance, not only do we
have an np construction, but also
such constructions as np(place), np(duration),
np(time), np(time(hour)) etc. In other words, the semantic hierarchy
is reflected in the set of linguistic categories. (Notice that
categories are not the same as features).
Hence we do not have a list, but a tree of grammatical
categories.

To be more specific, let us consider temporal categories.
Time is divided into the following categories
minute, hour, weekday, month,  relative,
         day_part, week_part, phase,
and more categories could be added if needed, e.g. century.
Some of these categories are further subdivided:
day_part into afternoon, morning, ...,
or
relative(_) (e.g. in two hours)
into hour, minute, day_part, week_part, weekday.

Note that (a) different constructions can describe the same
object of a given category, for example
hour can be given by a numeral or a numeral  followed by
the word "am" or "pm"; (b) there is a continuum of both categories
and constructions describing objects of a given category,
for instance, we have the following sequence of types of constructions

np ] np(time) ] np(time(month)) ]

]  np(time(month(january))) = january

corresponding to the sequence of categories

entity ] time ] month = time(month) ]

]  january =  time(month(january))

In the first case january is a word (words are constructions,
too), in the second case it is a concept.

We end this sections with two notes. First, we can ask:
what about verbs and their hierarchies? -- At present
we have no hierarchy of actions, and no hierarchy of
verbs; perhaps when related
actions can appear as parameters of a plan such a need would arise.
However np hierarchies help describe arguments of verbs.

Second, we would like to point to another, indirect, argument
in favor of the coupling of grammar and ontology. Namely,
the relationship between ontologies and case systems. For instance,
  list 28 cases grouped in 5 classes (eg. space (direction, orientation, location_to, ...), or
causality (cause, contradiction, effect, ...)).
Clearly, there is a relationship between cases and concepts in
ontological hierarchies.

Parsing with constructions differs a bit from
syntactic parsing. First, the collection of features
that are used to drive parsing is richer, because it contains terms
with semantic and pragmatic interpretation. Second,
semantic and pragmatic information is used during parsing.
Third, descriptions assigned to strings by the parser
are different, namely,
the structural information is lost; instead the meanings/messages
are produced.

In the next example, notice that the knowledge
of application is needed to produce the following interpretation
of a complete sentence.
That is, the interpretation cannot be produced solely from
the edges of the chart produced by the parser; it must e.g. be known
that "set up" is means "schedule".


Let us now consider the processing of fragments, which will also
explain how the interpretation above can be computed,
and how ontology is used in constructions. As noted before,
getting the slots from a fragment is possible only by
using context (such parameters as current_question or
topic),
so that its meaning
can be unambiguously computed.
  Let us assume that the system has asked for the time and date of
an appointment. The user however does not answer exactly to the point:

The preposition on by itself does not carry any meaning,
we treat it as a lexicalized feature.
In (2, 4), to prevent overgeneralizations such as
3 th, the vehicle of the construction must contain the list
of cases if 1 then st, if 2 then nd, ....


The meaning of the pp (0, 4)  is a simple transformation of
the meaning of the np in  (1, 4). But notice that this
happens because we have a specific construction which says that
a combination of on with an np(time(_)) produces
event_time. Altogether, we have encoded a few dozens
various pp constructions, but in a given application
only a fraction of them are used.

In the same context, please note that, because of the close
relationship between the domain ontology and the hierarchy of
constructions, we can also postulate a close relationship
between the type of meaning a construction expresses and
its category (i.e. name), for example, the type of meaning of
np(time(hour)) is [type  event_time].

At the end, we obtain two parses of (0, 6), but they
have the same messages. The difference lies in the category assigned
to pp_list; and in this particular case the choice of
that category is not important.

The idea of compositionality is used
to account for the ability of the language user to understand
the meaning of sentences not encountered before. The new sentence
can be understood, because it is composed of parts (words) that
the user knows, and the meaning of the new sentence is computable
from the meanings of the words that compose it. But the standard
definition of compositionality is formally vacuous
  (i.e. any semantics can be compositional), so the concept of compositionality has to be reexamined.

Given that, note that a grammar of constructions can account
for the ability of the language user to understand
the meaning of novel sentences without separating the language into
syntax, semantics and pragmatics. Namely, the meaning of a larger
construction is a combination of the meanings of its parts
(and the way they are put together). The message of the larger
construction specifies how its meaning depends on the meanings
of the parts.

We view construction grammars as representations of domain-independent
linguistic knowledge. That is why we need a domain-dependent
semantic module to map linguistic representations into concepts of
the domain. The fact that we have been able to use the same
sets of constructions for various domains is an argument for
feasibility of this approach.

Obviously NLU is impossible without access to vast bodies of
background knowledge. The fact that meanings of NL utterances
are classified and described with the help of general ontologies
suggests that linking linguistic and non-linguistic
knowledge for the purpose of reasoning about dialogs and texts
might be possible without the help of a translator program.
Furthermore, it might be possible to modularize those sources
of knowledge.

The innovations we have proposed --
the close coupling of semantic hierarchies with linguistic
categories, the use of
context in representing linguistic knowledge,
and the representation of the grammar as a dictionary of
constructions -- not only facilitate the development of the
grammar, but also its interaction with the inference engine
and the application.

Obviously, the usefulness
of this approach is not limited to natural language interfaces.
We have used parts of the grammar of constructions
for some information retrieval tasks; and we can easily
imagine it being applied to
text skimming and to machine translation in limited domains.

The approach to language understanding we
are advocating has several advantages: it
agrees with the facts of language; it is not restricted to
a "core" of language, but applies to standard and more exotic
constructions, including language fragments;
and it is computationally feasible, as it has been
implemented in a complete working system.

Summarizing,
the most important innovations implemented in the system are
the close coupling of semantic hierarchies with the set of linguistic
categories; the use of
context in representing linguistic knowledge, esp. for discourse
constructions; and a non-lexicalist encoding of the grammar
in a dictionary of constructions.
We have obtained a new language model in which forms cannot
be separated from meanings. We can talk about meaning in
a systematic way, but we do not have compositionality
described as a homomorphism from syntax to semantics.
(This is theoretically interesting, also because
it is closer to intuitions than current models of semantics).
We have validated this model by building
prototypes of natural language interfaces.

T. Copeck, S. Delisle, and S.Szpakowicz.
Parsing and case analysis in tanka.
1992.

R.M.W. Dixon.
A New Approach to English Grammar on Semantic Principles.
Clarendon Press, Oxford, 1991.

G. Fauconnier.
Mental Spaces.
MIT Press, Cambridge, Massachusetts, 1985.

C.J. Fillmore, P.Kay, and M.C. O'Connor.
Regularity and idiomaticity in grammatical constructions.
Language, 64(3):501-538, 1988.

A.E. Goldberg.
Constructions: a construction grammar approach to argument
  structure.
The University of Chicago Press, Chicago, IL, 1994.

K. Jensen.
Parsing strategies in a broad-coverage grammar of english.
Technical Report RC 12147, IBM T.J. Watson Research Center, Yorktown
  Heights, New York, 1986.

D. Jurafsky.
An On-line Computational Model of Sentence Interpretation.
PhD thesis, University of California, Berkeley, 1992.
Report No. UCB/CSD 92/676.

P. Kay.
Anaphoric binding in construction grammar.
In BLS 20, Berkeley, CA, 1994.

R.W. Langacker.
Foundations of Cognitive Grammar II.
Stanford U. Press, Stanford, CA, 1991.

S.L. Lytinen.
Semantics-first natural language processing.
In Proceedings AAAI-91, pages 111-116, Anaheim, CA, 1991.

J.D. McCawley.
The Syntactic Phenomena of English.
University of Chicago Press, Chicago, IL, 1988.

M. McCord, A. Bernth, S. Lappin, and W. Zadrozny.
Natural language processing technology based on slot grammar.
International Journal on Artificial Intelligence Tools,
  1(2):229-297, 1992.

I. Melcuk and A. Polguere.
A formal lexicon in meaning-text theory (or how to do lexica with
  words).
Computational Linguistics, 13(3-4):261-275, 1987.

I. Melcuk.
Dependency Syntax: Theory and Practice.
State University of New York Press, Albany, NY, 1988.

J. Nerbonne.
Computational semantics -- linguistics and processing.
In S. Lappin, editor, The Handbook of Contemporary Semantic
  Theory. Blackwell, Oxford, 1995.

P. Sells.
Lectures on Contemporary Syntactic Theories.
CSLI Lecture Notes (3), Stanford, CA, 1985.

L. Talmy.
Lexicalization patterns: semantic structure in lexical forms.
In T. Shopen, editor, Language Typology and syntactic
  description Vol.III, pages 57-149. 1985.

W. Zadrozny, M. Szummer, S. Jarecki, D.E. Johnson, and
L. Morgenstern.
NL understanding with a grammar of constructions.
Proc. Coling'94, 1994.

W. Zadrozny.
From compositional to systematic semantics.
Linguistic and Philosophy, (4):329-342, 1994.

A.M. Zwicky.
Dealing out meaning: Fundamentals of syntactic constructions.
In BLS 20, Berkeley, CA, 1994.

  Proc. IJCAI'95 Workshop on Context in NLP. Montreal, Aug.1995
  We will use the convention that np, np(x), vp, vp(_)
etc. refer to elements of our construction grammar, while
traditional linguistic/syntactic categories will be denoted
by NP, VP, S etc.
  The language of meanings is
to a large extent an open problem, but
  presents a catalogue of semantic relations which are lexically expressible (in different languages). We believe
that an extension of this list can be used to classify meanings of all
constructions.
  In the following,
we do not show syntactic information, because syntactic features
are standard (e.g.  plural, 3rd, accusative, ...).
We have experimented with different representations for constructions,
and we have built two parsers of constructions, of which one
was a postprocessor to
a standard grammar, the English Slot Grammar (ESG) of McCord,
 and it is described in . 
