
We address the problem of automatically acquiring case frame patterns
(selectional patterns) from large corpus data. The acquisition of
case frame patterns normally involves the following three subproblems:
1) Extracting case frames from corpus data, 2) Generalizing case frame
slots within these case frames, 3) Learning dependencies that exist
between these generalized case frame slots.

In this paper, we propose a method of learning dependencies between
case frame slots. By `dependency' is meant the relation that exists
between case slots which constrains the possible values assumed by
each of those case slots. As illustrative examples, consider the
following sentences.









We see that an `airline company' can be the subject of verb `fly' (the
value of slot `arg1'), when the direct object (the value of slot
`arg2') is an `airplane' but not when it is an `airline
 company'. These examples indicate that the possible values of case slots depend in general on those of
the other case slots: that is, there exist `dependencies' between
different case slots.

The knowledge of such dependencies is useful in various tasks in
natural language processing, especially in analysis of sentences
involving multiple prepositional phrases, such as



Note in the above example that the slot of `from' and that of `to'
should be considered dependent and the attachment site of one of the
prepositional phrases (case slots) can be determined by that of the
other with high accuracy and confidence.

There has been no method proposed to date, however, that learns
dependencies between case slots in the natural language processing
literature. In the past research, the distributional pattern of each
case slot is learned independently, and methods of resolving ambiguity
are also based on the assumption that case slots are independent
 ,,,,,,, or dependencies between at most two case slots are considered
 ,,. Thus, provision of an effective method of learning dependencies between case slots, as well
as investigation of the usefulness of the acquired dependencies in
disambiguation and other natural language processing tasks would be an
important contribution to the field.

In this paper, we view the problem of learning case frame patterns as
that of learning a multi-dimensional discrete joint distribution,
where random variables represent case slots. We then formalize the
dependencies between case slots as the probabilistic
dependencies between these random variables. Since the number of
parameters that exist in a multi-dimensional joint distribution is
exponential if we allow n-ary dependencies in general, it is
infeasible to estimate them with high accuracy with a data size
available in practice. It is also clear that relatively few of these
random variables (case slots) are actually dependent on each other
with any significance. Thus it is likely that the target joint
distribution can be approximated reasonably well by the product of
component distributions of low order, drastically reducing the
number of parameters that need to be considered. This is indeed the
approach we take in this paper.

Now the problem is how to approximate a joint distribution by the
product of lower order component distributions. Recently, Suzuki
proposed an algorithm to approximately learn a multi-dimensional joint
distribution expressible as a `dendroid distribution,' which is both
 efficient and theoretically sound . We employ Suzuki's algorithm to learn case frame patterns as dendroid distributions. We
conducted some experiments to automatically acquire case frame
patterns from the Penn Tree Bank bracketed corpus. Our experimental
results indicate that for some classes of verbs the accuracy achieved
in a disambiguation experiment can be improved by using the acquired
knowledge of dependencies between case slots.

 Suppose that we have data of the type shown in Figure , given by instances of the case frame of verb `fly' automatically
extracted from a corpus, using conventional techniques. As explained
in Introduction, the problem of learning case frame patterns can be
viewed as that of estimating the underlying multi-dimensional joint
distribution which gives rise to such data. In this research, we
assume that case frame instances with the same head are generated by a
joint distribution of type,



where index Y stands for the head, and each of the random variables


represents a case slot. In this paper, we use
`case slots' to mean surface case slots, and we uniformly treat
obligatory cases and optional cases. Thus the number n of the
random variables is roughly equal to the number of prepositions in
English (and less than 100).

These models can be further classified into three types of
probabilistic models according to the type of values each random
variable Xi assumes. When Xi assumes a word or a special symbol
`0' as its value, we refer to the corresponding model


as a `word-based model.'  Here `0' indicates
the absence of the case slot in question. When Xi assumes a
word-class or `0' as its value, the corresponding model is called a
`class-based model.' When Xi takes on 1 or 0 as its value, we
call the model a `slot-based model.'  Here the value of `1'
indicates the presence of the case slot in question, and `0'
 absence. For example, the data in Figure  can be generated by a word-based model, and the data in Figure
  by a class-based model. Suppose for simplicity that there are only 4 possible case slots corresponding respectively to the
subject, direct object, `from' phrase, and `to' phrase. Then,



is given a specific probability value by a word-based model. In
contrast,



is given a specific probability by a class-based model, 
where 


and 


denote word classes. Finally, 



is assigned a specific probability by a slot-based model.

We then formulate the dependencies between case slots as the   probabilistic dependencies between the random variables in each of
these three models. In the absence of any constraints, however, 
the number of
parameters in each of the above three models is exponential 
(even the slot-based model has O(2[n]) parameters ), and thus it is
infeasible to estimate them in practice. A simplifying assumption that
is often made to deal with this difficulty is that random variables
(case slots) are mutually independent.

Suppose for example that in the analysis of the sentence



the following alternative interpretations are given.





We wish to select the more appropriate of the two interpretations. A
heuristic word-based method for disambiguation, in which the
random variables (case slots) are assumed to be dependent, is to
calculate the following values of word-based likelihood and to select
the interpretation corresponding to the higher likelihood value.





If on the other hand we assume that the random variables are   independent, we only need to calculate and compare
 (c.f.) 



and



The independence assumption can also be made in the case of a
class-based model or a slot-based model. For slot-based models, with
the independence assumption, the following probabilities





 are to be compared (c.f.).  

Assuming that random variables (case slots) are mutually independent
would drastically reduce the number of parameters. (Note that under
the independence assumption the number of parameters in a slot-based
model becomes O(n).)  As illustrated in Section 1, this assumption
is not necessarily valid in practice. 

What seems to be true in practice is that some case slots are in fact
dependent but overwhelming majority of them are independent, due
partly to the fact that usually only a few case slots are obligatory
 and most others are optional. Thus the target joint distribution is likely to be approximable by the
product of several component distributions of low order, and thus have
in fact a reasonably small number of parameters. We are thus lead to
the approach of approximating the target joint distribution by such a
simplified model, based on corpus data.

Without loss of generality, any n-dimensional joint distribution can
be written as



for some permutation (

m1,m2,...mn) of 1,2,..,n, where we let

P(Xm1|Xm0) denote 

P(Xm1).

A plausible assumption on the dependencies between random variables is
intuitively that each variable directly depends on at most one
other variable. (Note that this assumption is the simplest among those
that relax the independence assumption.) For example, if a joint
distribution 

P(X1,X2,X3) over 3 random variables 

X1,X2,X3can be written (approximated) as follows, it (approximately) satisfies
such an assumption.



Such distributions are referred to as `dendroid distributions' in the
literature. A dendroid distribution can be represented by a dependency
forest (i.e. a set of dependency trees), whose nodes represent the
random variables, and whose directed arcs represent the dependencies
that exist between these random variables, each labeled with a number
of parameters specifying the probabilistic dependency. (A dendroid
distribution can also be considered as a restricted form of the
 Bayesian network .)  It is not difficult to see that there are 7 and only 7 such representations for the joint distribution

 P(X1,X2,X3) (See Figure ), disregarding the actual numerical values of the probabilistic parameters.

Now we turn to the problem of how to select the best dendroid
distribution from among all possible ones to approximate a target
joint distribution based on input data `generated' by it. This problem
has been investigated in the area of machine learning and related
fields.  A classical method is Chow  Liu's algorithm for estimating
a multi-dimensional joint distribution as a dependency tree, in a way
 which is both efficient and theoretically sound . More recently Suzuki extended their algorithm so that it estimates the
target joint distribution as a dendroid distribution or dependency
 forest , allowing for the possibility of learning one group of random variables to be completely independent of another.
Since many of the random variables (case slots) in case frame patterns
are essentially independent, this feature is crucial in our context,
and we thus employ Suzuki's algorithm for learning our case frame
patterns.

Suzuki's algorithm first calculates the mutual information between all
two nodes (random variables), and it sorts the node pairs in
descending order with respect to the mutual information. It then puts
a link between a node pair with the largest mutual information value
I, provided that I exceeds a certain threshold which depends on
the node pair and adding that link will not create a loop in the
current dependency graph. It repeats this process until no node pair
 is left unprocessed. Figure  shows the detail of this algorithm, where ki denotes the number of possible values
assumed by Xi, N the input data size, and 
denotes the
logarithm to the base 2.
It is easy to see that the number of parameters in a dendroid
distribution is of the order O(n k[2]), where k is the maximum of
all ki, and n is the number of random variables, and the time
complexity of the algorithm is of the order 

.

We will now show how the algorithm works by an illustrative example.
 Suppose that the data is given as in Figure  and there are 4 nodes (random variables) 

Xarg1,Xarg2,Xfrom,Xto.
The values of mutual information and thresholds for all node pairs are
 shown in Table . Based on this calculation the algorithm constructs  the dependency forest shown in Figure    , because the mutual information between Xarg2 and Xto , Xfrom and
Xto are large enough, but not the others. The result indicates
that slot `arg2' and `from' should be considered dependent on `to.'
Note that `arg2' and `from' should also be considered dependent via
`to' but to a somewhat weaker degree.

Suzuki's algorithm is derived from the Minimum Description Length
(MDL) principle
 ,,,, which is a principle for statistical estimation in information theory. It is
known that as a strategy of estimation, MDL is guaranteed to be near
 optimal. In applying MDL, we usually assume that the given data are generated by a probabilistic
model that belongs to a certain class of models and selects a model
within the class which best explains the data. It tends to be the case
usually that a simpler model has a poorer fit to the data, and a more
complex model has a better fit to the data. Thus there is a trade-off
between the simplicity of a model and the goodness of fit to data. MDL
resolves this trade-off in a disciplined way: It selects a model which
is reasonably simple and fits the data satisfactorily as well. In our
current problem, a simple model means a model with less dependencies,
and thus MDL provides a theoretically sound way to learn only those
dependencies that are statistically significant in the given data. An
especially interesting feature of MDL is that it incorporates the
input data size in its model selection criterion. This is reflected,
in our case, in the derivation of the threshold .
Note that
when we do not have enough data (i.e. for small N), the thresholds
will be large and few nodes tend to be linked, resulting in a simple
model in which most of the case slots are judged independent.  This is
reasonable since with a small data size most case slots cannot be
determined to be dependent with any significance.

We conducted some experiments to test the performance of the proposed
method as a method of acquiring case frame patterns. In particular, we
tested to see how effective the patterns acquired by our method are in
a structural disambiguation experiment. We will describe the results
of this experimentation in this section.

In our first experiment, we tried to acquire slot-based case frame
patterns. First, we extracted 181,250 case frames from the Wall
Street Journal (WSJ) bracketed corpus of the Penn Tree Bank
  as training data. There were 357 verbs for which more than 50 case frame examples appeared in the training data.
 Table  shows the verbs that appeared in the data most frequently and the number of their occurrences.

First we acquired the slot-based case frame patterns for all of the
357 verbs. We then conducted a ten-fold cross validation to evaluate
 the `test data perplexity' of the acquired case frame patterns, that is, we used nine tenth of the case
frames for each verb as training data (saving what remains as test
data), to acquire case frame patterns, and then calculated perplexity
using the test data. We repeated this process ten times and calculated
 the average perplexity. Table  shows the average perplexity obtained for some randomly selected verbs. We also
calculated the average perplexity of the `independent slot models'
acquired based on the assumption that each case slot is independent.
 Our experimental results shown in Table  indicate that the use of the dendroid models can achieve up to 
perplexity
reduction as compared to the independent slot models. It seems safe to
say therefore that the dendroid model is more suitable for
representing the true model of case frames than the independent
slot model.

We also used the acquired dependency knowledge in a pp-attachment
disambiguation experiment. We used the case frames of all 357 verbs
as our training data. We used the entire bracketed corpus as training
data because we wanted to utilize as many training data as possible.
We extracted 

(verb,noun1,prep,noun2) or

(verb,prep1,noun1,prep2,noun2) patterns from the WSJ tagged
corpus as test data, using pattern matching techniques such as that
 described in . We took care to ensure that only the part of the tagged (non-bracketed) corpus which does not overlap with
the bracketed corpus is used as test data. (The bracketed corpus does
overlap with part of the tagged corpus.)

We acquired case frame patterns using the training data. Figure
  shows an example of the results, which is part of the case frame pattern (dendroid distribution) for the verb `buy.'  Note
in the model that the slots `for,' 'on,' etc, are dependent on `arg2,'
while `arg1' and `from' are independent.

We found that there were 266 verbs, whose `arg2' slot is dependent
 on some of the other preposition slots. Table  shows 37 of the verbs whose dependencies between arg2 and other case slots
are positive and exceed a certain threshold, i.e. 

P(arg2=1,prep=1) ]
 0.25 . The dependencies found by our method seem to agree with human intuition in most cases.

There were 93 examples in the test data (

(verb,noun1,prep,noun2)pattern) in which the two slots `arg2' and prep of verb are
determined to be positively dependent and their dependencies are
stronger than the threshold of 0.25. We forcibly attached

prep noun2 to verb for these 93 examples. For comparison, we
also tested the disambiguation method based on the independence
 assumption proposed by  on these examples.  Table  shows the results of these experiments, where `Dendroid' stands for the former method and `Independent' the latter.
We see that using the information on dependency we can   significantly improve the disambiguation accuracy on this part of
the data. Since we can use existing methods to perform disambiguation
for the rest of the data, we can improve the disambiguation accuracy
for the entire test data using this knowledge.

Furthermore, we found that there were 140 verbs having
 inter-dependent preposition slots. Table  shows 22out of these 140 verbs such that their case slots have positive dependency that exceeds a certain threshold, i.e.

P(prep1=1,prep2=1) ] 0.25. Again the dependencies found by our
method seem to agree with human intuition.

In the test data (which are of 

verb,prep1,noun1,prep2,noun2pattern), there were 21 examples that involves one of the above 22verbs whose preposition slots show dependency exceeding 0.25. We
forcibly attached both 

prep1 noun1 and 

prep2 noun2 to verbon these 21 examples, since the two slots prep1 and prep2 are
 judged dependent. Table  shows the results of this experimentation, where `Dendroid' and `Independent' respectively
represent the method of using and not using the dependencies. Again,
we find that for the part of the test data in which dependency is
present, the use of the dependency knowledge can be used to improve
the accuracy of a disambiguation method, although our experimental
results are inconclusive at this stage.

We also used the 357 verbs and their case frames used in Experiment 1 
to acquire class-based case frame patterns using the proposed method. We
randomly selected 100 verbs among these 357 verbs and attempted to
acquire their case frame patterns. We generalized the case slots
within each of these case frames using the method proposed by
  to obtain class-based case slots, and then  replaced the word-based case slots in the data with the obtained 
class-based case slots. 
What resulted are class-based case frame examples
 like those shown in Figure . We used these data as input to the learning algorithm and acquired case frame patterns for each of
the 100 verbs. We found that no two case slots are determined as
dependent in any of the case frame patterns. This is because the
number of parameters in a class based model is very large compared to
the size of the data we had available.

Our experimental result verifies the validity in practice of the
assumption widely made in statistical natural language processing that
class-based case slots (and also word-based case slots) are mutually
independent, at least when the data size available is that provided by
the current version of the Penn Tree Bank. This is an empirical
finding that is worth noting, since up to now the independence
assumption was based solely on human intuition, to the best of our
knowledge.

To test how large a data size is required to estimate a class-based
model, we conducted the following experiment. We defined an artificial
class-based model and generated some data according to its
distribution. We then used the data to estimate a class-based model
(dendroid distribution), and evaluated the estimated model by
measuring the number of dependencies (dependency arcs) it has and the
 KL distance between the estimated model and the true model. We repeatedly generated data and observed the
`learning curve,' namely the relationship between the number of
dependencies in the estimated model and the data size used in
estimation, and the relationship between the KL distance between the
estimated and true model and the data size. We defined two other
 models and conducted the same experiments. Figure  shows the results of these experiments for these three artificial models
averaged over 10 trials. (The number of parameters in Model1,
Model2, and Model3 are 18, 30, and 44 respectively, while the
number of dependencies are 1, 3, and 5 respectively.) We see
that to accurately estimate a model the data size required is as large
as 100 times the number of parameters. Since a class-based model
tends to have more than 100 parameters usually, the current data
 size available in the Penn Tree Bank (See Table) is not enough for accurate estimation of the dependencies within case frames
of most verbs.

We conclude this paper with the following remarks.
1.
The primary contribution of research reported in this paper is
that we have proposed a method of learning dependencies between case
  frame slots, which is theoretically sound and efficient, thus
  providing an effective tool for acquiring case dependency
  information. 
2.
For the slot-based model, sometimes case slots are found to be
  dependent. Experimental results demonstrate that using the
  dependency information, when dependency does exist, structural 
  disambiguation results can be improved.
3.
For the word-based or class-based models, case slots are judged
  independent, with the data size currently available in the Penn Tree
  Bank. This empirical finding verifies the independence assumption
  widely made in practice in statistical natural language processing.

We proposed to use dependency forests to represent case frame
patterns. It is possible that more complicated probabilistic
dependency graphs like Bayesian networks would be more appropriate for
representing case frame patterns. This would require even more data
and thus the problem of how to collect sufficient data would be a
crucial issue, in addition to the methodology of learning case frame
patterns as probabilistic dependency graphs. Finally the problem of
how to determine obligatory/optional cases based on dependencies
acquired from data should also be addressed.

We thank Mr.K.Nakamura, Mr.T.Fujita, and Dr.K.Kobayashi of NEC CC
Res. Labs. for their constant encouragement. We thank Mr.R.Isotani of
NEC Information Technology Res. Labs. for his comments. We thank Ms. 
Y.Yamaguchi of NIS for her programming effort.

Hiyan Alshawi and David Carter.
1995.
Training and scaling preference functions for disambiguation.
Computational Linguistics, 20(4):635-648.

Lalit R. Bahl, Frederick Jelinek, and Robert Mercer.
1983.
A maximum likelihood approach to continuous speech recognition.
IEEE Transaction on Pattern Analysis and Machine Intelligence,
  5(2):179-190.

Eric Brill and Philip Resnik.
1994.
A rule-based approach to prepositional phrase attachment
  disambiguation.
Proceedings of the 15th COLING, pages 1198-1204.

Jing-Shin Chang, Yih-Fen Luo, and Keh-Yih Su.
1992.
GPSM: A generalized probabilistic semantic model for ambiguity
  resolution.
Proceedings of the 30th ACL, pages 177-184.

C.K. Chow and C.N. Liu.
1968.
Approximating discrete probability distributions with dependence
  trees.
IEEE Transactions on Information Theory, 14(3):462-467.

Michael Collins and James Brooks.
1995.
Prepositional phrase attachment through a backed-off model.
Proceedings of the 3rd Workshop on Very Large Corpora.

Thomas M. Cover and Joy A. Thomas.
1991.
Elements of Information Theory.
John Wiley  Sons Inc.

Williams A. Gale and Kenth W. Church.
1990.
Poor estimates of context are worse than none.
Proceedings of the DARPA Speech and Natural Language Workshop,
  pages 283-287.

Ralph Grishman and John Sterling.
1994.
Generalizing automatically generated selectional patterns.
Proceedings of the 15th COLING, pages 742-747.

Donald Hindle and Mats Rooth.
1991.
Structural ambiguity and lexical relations.
Proceedings of the 29th ACL, pages 229-236.

Hang Li and Naoki Abe.
1995.
Generalizing case frames using a thesaurus and the MDL principle.
Proceedings of Recent Advances in Natural Language Processing,
  pages 239-248.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993.
Building a large annotated corpus of English: The penn treebank.
Computational Linguistics, 19(1):313-330.

Judea Pearl.
1988.
Probabilistic Reasoning in Intelligent Systems: Networks of
  Plausible Inference.
Morgan Kaufmann Publishers Inc.

J. Ross Quinlan and Ronald L. Rivest.
1989.
Inferring decision trees using the minimum description length
  principle.
Information and Computation, 80:227-248.

Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994.
A maximum entropy model for prepositional phrase attachment.
Proceedings of ARPA Workshop on Human Language Technology,
  pages 250-255.

Philip Resnik.
1992.
Semantic classes and syntactic ambiguity.
Proceedings of ARPA Workshop on Human Language Technology.

Jorma Rissanen.
1978.
Modeling by shortest data description.
Automatic, 14:37-38.

Jorma Rissanen.
1983.
A universal prior for integers and estimation by minimum description
  length.
The Annals of Statistics, 11(2):416-431.

Jorma Rissanen.
1984.
Universal coding, information, predication and estimation.
IEEE Transaction on Information Theory, 30(4):629-636.

Jorma Rissanen.
1986.
Stochastic complexity and modeling.
The Annals of Statistics, 14(3):1080-1100.

Jorma Rissanen.
1989.
Stochastic Complexity in Statistical Inquiry.
World Scientific Publishing Co.

Satoshi Sekine, Jeremy J. Carroll, Sofia Ananiadou, and Jun'ichi Tsujii.
1992.
Automatic learning for semantic collocation.
Proceedings of the 3rd Conference on Applied Natural Language
  Processing, pages 104-110.

Frank Smadja.
1993.
Retrieving collocations from text: Xtract.
Computational Linguistics, 19(1):143-177.

Joe Suzuki.
1993.
A construction of bayesian networks from databases based on an MDL
  principle.
Proceedings of Uncertainty in AI '93.

  Real World Computing Partnership
  One may argue that `fly' has different word senses
  in these sentences and for each of these word senses there is no
  dependency between the case frame slots. Word senses are in general
  difficult to define precisely, however, and in language processing,
  they would have to be disambiguated from the context anyway, which
  is essentially equivalent to assuming that the dependencies between
  case slots exist. Thus, our proposed method can in effect `discover'
  implicit word senses from corpus data.
  Optional case slots are not
  necessarily independent, but if two optional case slots are randomly
  selected, it is likely that they are independent of one another.
  The probabilities in this
  table are estimated by using the so-called Expected Likelihood
  Estimator, i.e., by adding 0.5 to actual frequencies (c.f.
   ). 
  We refer the interested reader to
   , for an introduction to MDL. 
  The `test data perplexity', which
  is a measure of testing how well an estimated probabilistic model
  predicts some hitherto unseen data, is defined as 

,
 where PM(x)  denotes the probability function of the estimated model, PT(x)  the distribution function of the data . 
  We uniformly treat the head of a noun phrase
  immediately after a verb as `arg2' (including, for example `30%' in
  `rise 30% from billion').
  The KL distance (KL divergence or relative
  entropy) which is widely used in information theory and statistics,
   is a measure of `distance' between two distributions .   It is always non-negative and is zero if and only if the two
  distributions are identical, but is asymmetric and hence not a
  metric (the usual notion of distance).
