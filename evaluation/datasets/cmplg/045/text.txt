
Categorial Grammar (CG) and in particular Lambek Categorial Grammar
(LCG) have their well-known benefits for the formal treatment of
natural language syntax and semantics. The most outstanding of these
benefits is probably the fact that the specific way, how the complete
grammar is encoded, namely in terms of `combinatory potentials' of its
words, gives us at the same time recipes for the construction of
meanings, once the words have been combined with others to form larger
linguistic entities. Although both frameworks are equivalent in weak
generative capacity -- both derive exactly the context-free
languages --, LCG is superior to CG in that it can cope in a 
natural way with extraction and unbounded dependency phenomena. For
instance, no special category assignments need to be stipulated to
handle a relative clause containing a trace, because it is analyzed,
via  hypothetical reasoning, like a traceless clause with the trace
being the hypothesis to be discharged when combined with the relative
pronoun.
  Figure  illustrates this proof-logical behaviour. Notice that this natural-deduction-style proof in the type
logic corresponds very closely to the phrase-structure tree one
would like to adopt in an analysis with traces. We thus can derive
Bill misses 
as an s from the hypothesis that there
is a ``phantom'' np in the place of the trace. Discharging the
hypothesis, indicated by index 1, results in Bill misses being
analyzed as an s/np from zero hypotheses. Observe, however, that
such a bottom-up synthesis of a new unsaturated type is only required,
if that type is to be consumed (as the antecedent of an
implication) by another type. Otherwise there would be a simpler proof
without this abstraction. In our example the relative pronoun has such
a complex type triggering an extraction. 

A drawback of the pure Lambek Calculus LMBK is
that it only allows for so-called `peripheral extraction', i.e., in
our example the trace should better be initial or final in the
relative clause. 

This inflexibility of Lambek Calculus is one of the reasons why many
researchers study richer systems today.  
 For instance, the recent work by Moortgat  gives a systematic in-depth study of mixed Lambek systems, which integrate the
systems LMBK, NL, NLP, and LP. These ingredient systems are
obtained by varying the Lambek calculus along two dimensions: adding
the permutation rule (P) and/or dropping the assumption that the type
combinator (which forms the sequences the systems talk about) is
associative (N for non-associative).

Taken for themselves these variants of LMBK are of little use in
linguistic descriptions. But in Moortgat's mixed system all the
different resource management modes of the different systems are left
intact in the combination and can be exploited in different parts of
the grammar. The relative pronoun which would, for instance,
receive category 


with llimp being
implication in LP,
  i.e., it requires as an argument ``an s lacking an npsomewhere''.
 . 

The present paper studies the computational complexity of a variant of
the Lambek Calculus that lies between LMBK and LP,
the Semidirectional Lambek Calculus SDL.
  Since LP derivability is known to be NP-complete, it is interesting
to study restrictions on the use of the LP operator llimp. A
restriction that leaves its proposed linguistic applications intact is
to admit a type 
only as the argument type in functional
applications, but never as the functor. Stated prove-theoretically
for Gentzen-style systems, this amounts to disallowing the left rule
for llimp. Surprisingly, the resulting system SDL can be stated
without the need for structural rules, i.e., as a monolithic system
with just one structural connective, because the ability of the
abstracted-over formula to permute can be directly encoded in the
right rule for llimp.

Note that
our purpose for studying SDL is not that it might be in any sense
better suited for a theory of grammar (except perhaps, because of its
simplicity), but rather, because it exhibits a core of logical
behaviour that any richer system also needs to include, at least if it
should allow for non-peripheral extraction. The sources of complexity
uncovered here are thus a forteriori present in all these richer
systems as well.

The semidirectional Lambek calculus (henceforth SDL) is a variant of
 J. Lambek's original  calculus of syntactic types. We start by defining the Lambek calculus and extend it to obtain SDL.

Formulae (also called ``syntactic types'') are built 
from a set of propositional variables (or ``primitive types'') 


and the three binary connectives ,

,
/, called product, left implication, and   right implication. We use generally capital letters A, B, C,
...to denote formulae and capitals towards the end of the alphabet
T, U, V, ...to denote sequences of formulae. The
concatenation of sequences U and V is denoted by (U,V).

The (usual) formal framework of these logics is a Gentzen-style
sequent calculus. Sequents are pairs (U,A), written as ,
where A is a type and U is a sequence of types.
  The claim embodied by sequent 
can be read as ``formula A  is derivable from the structured database U''.
 Figure  shows Lambek's original calculus LMBK.  

First of all, since we don't need products to obtain our results and
since they only complicate matters, we eliminate products from
consideration in the sequel.

In Semidirectional Lambek Calculus we add as additional connective the
LP implication llimp, but equip it only with a right rule.



Let us define the polarity of a subformula of a sequent 

as follows: A has positive polarity, each of Ai have negative
polarity and if B/C or 
has polarity p, then B also has
polarity p and C has the opposite polarity of p in the sequent.

A consequence of only allowing the 


rule, which is
easily proved by induction, is that in any derivable sequent llimp
may only appear in positive polarity. Hence, llimp may not occur in
the (cut) formula A of a 


application and any
subformula 
which occurs somewhere in the prove must also
occur in the final sequent. When we assume the final sequent's RHS to
be primitive (or llimp-less), then the 


rule will
be used exactly once for each (positively) occuring
llimp-subformula. In other words, 


may only do
what it is supposed to do: extraction, and we can directly read off
the category assignment which extractions there will be. 

We can show Cut Elimination for this calculus by a straight-forward
adaptation of the Cut elimination proof for LMBK. We omit the proof
for reasons of space.


The cut-free system enjoys, as usual for Lambek-like logics, the   Subformula Property: in any proof only subformulae of the goal
sequent may appear.

In our considerations below we will make heavy use of the well-known
 count invariant for Lambek systems , which is an expression of the resource-consciousness of these logics. 
Define 
(the b-count of A), a function counting
positive and negative occurrences of primitive type b in an
arbitrary type A, to be 



The invariant now states that for any primitive b, the b-count of
the RHS and the LHS of any derivable sequent are the same. By noticing
that this invariant is true for Ax and is preserved by the
rules, we immediately can state:



Let us in parallel to SDL consider the fragment of it in which


and 


are disallowed. We call this
fragment SDLM. Remarkable about this fragment is that any positive
occurrence of an implication must be llimp and any negative one
must be / or .



We extend the lexical map l to nonempty strings of terminals by
setting 


for 

.

The language generated by a Lambek grammar


is defined as the set of all strings 


for which there exists a sequence of types


and 

.
We denote this language by L(G).

An SDL-grammar is defined exactly like a Lambek grammar, except that

replaces .

Given a grammar G and a string 

,
the parsing (or recognition) problem asks the 
question, whether w is in L(G).

It is not immediately obvious, how the generative capacity of
SDL-grammars relate to Lambek grammars or nondirectional Lambek
grammars (based on calculus LP). Whereas Lambek grammars generate
exactly the context-free languages (modulo the missing empty word)
 , the latter generate all permutation closures of  context-free languages . This excludes many context-free or even regular languages, but includes some
context-sensitive ones, e.g., the permutation closure of 

a[n] b[n]
c[n]. 

Concerning SDL, it is straightforward to show that all context-free
languages can be generated by SDL-grammars.


We can use a the standard transformation of an arbitrary cfr. grammar 


to a categorial grammar G'. Since llimp does
not appear in G' each SDL-proof of a lexical assignment must be
also an LMBK-proof, i.e. exactly the same strings are judged grammatical
by SDL as are judged by LMBK.

Note that since 
the 


subset of LMBK already
accounts for the cfr. languages, this observation extends to SDLM.

Moreover, some languages which are not context-free can also be
generated. 

Example.
Consider the following grammar G for the language 

a[n] b[n] c[n].
We use primitive types 


and define the lexical map
for 


as follows:



The distinguished primitive type is x. To simplify the
argumentation, we abbreviate types as indicated above.

Now, observe that a sequent ,
where U is the image of some
string over ,
only then may have balanced primitive counts, if
U contains exactly one occurrence of each of A2, B2 and C2(accounting for the one supernumerary x and balanced y and zcounts) and for some number ,
n occurrences of each of
A1, B1, and C1 (because, resource-oriented speaking, each
Bi and Ci ``consume'' a b and c, resp., and each Ai``provides'' a pair b, c). Hence, only strings containing the same
number of a's, b's and c's may be produced. Furthermore, due to
the Subformula Property we know that in a cut-free proof of ,
the main formula in abstractions (right rules) may only be either


or 
,
where 

,
since all other implication types
have primitive antecedents. 
Hence, the LHS of any sequent in the proof must be a subsequence of
U, with some additional b types and c types interspersed. But
then it is easy to show that U can only be of the form



since any / connective in U needs to be introduced via

.

It remains to be shown, that there is actually a proof for such a
 sequent. It is given in Figure . 

The sequent marked with 
is easily seen to be derivable without
abstractions.

A remarkable point about SDL's ability to cover this language is
that neither LMBK nor LP can generate it. Hence, this example
 substantiates the claim made in  that the inferential capacity of mixed Lambek systems may be greater than the sum of its
component parts. Moreover, the attentive reader will have noticed that
our encoding also extends to languages having more groups of nsymbols, i.e., to languages of the form 

.

Finally, we note in passing that for this grammar the rules


and 


are irrelevant, i.e. that it is at
the same time an SDLM grammar.

We show that the Parsing Problem for SDL-grammars is NP-complete by a
reduction of the 3-Partition Problem to it.

Here is our reduction. Let 


be a given 3-Partition
instance. For notational convenience we abbreviate


by 


and similarly 


by 

,
but note that this is just an
abbreviation in the product-free fragment. Moreover the notation A[k]stands for



We then define the SDL-grammar 


as
follows: 



The word we are interested in is 

.
We
do not care about other words that might be generated by 

.
Our claim now is that a given 3-Partition problem 
is solvable
if and only if 


is in 

.
We consider each direction in turn.


We have to show, when given a solution to ,
how to choose a
type sequence 


and construct an SDL proof
for .
Suppose 

.
From a given
solution (set of triples) 


we can compute in
polynomial time a mapping k that sends the index of an element to
the index of its solution triple, i.e., k(i)=j iff 

.
To
obtain the required sequence U, we simply choose for the witerminals the type 

(resp. 


for w3m).
Hence the complete sequent to solve is:



Let 


be a shorthand for (*), and let
X stand for the sequence of primitive types 



Using rule 


only, we can obviously prove 

.
Now, applying 


3m+Nm times we can obtain

,
since there are in total, for each i, 3 bi and N ci in X.
As final step we have



which completes the proof.


Let 


and 



be a witnessing derivable sequent, i.e., for 

,

.
Now, since the counts of this sequent must be balanced, the
sequence 


must contain for each 


exactly 3 bj and exactly N cj as subformulae.
Therefore we 
can read off the solution to 
from this sequent by including
in 
(for 

)
those three ai for which Bihas an occurrence of bj, say these are aj(1), aj(2) and
aj(3). We verify, again via balancedness of the primitive counts,
that 

s(aj(1))+s(aj(2))+s(aj(3))=N holds, 
because these are the numbers of positive and negative occurrences of
cj in the sequent. This completes the proof.

The reduction above proves NP-hardness of the parsing problem. We need
strong NP-completeness of 3-Partition here, since our 
reduction uses a unary encoding. Moreover, the
parsing problem also lies within NP, since for a given grammar Gproofs are linearly bound by the length of the string and hence, we
can simply guess a proof and check it in polynomial time. Therefore we
can state the following:



Finally, we observe that for this reduction  the rules


and 


are again irrelevant and that we can
extend this result to SDLM.

We have defined a variant of Lambek's original calculus of types that
allows  abstracted-over categories to freely permute. Grammars based
on SDL can generate any context-free language and more than that.
The parsing problem for SDL, 
however, we have shown to be NP-complete. This result indicates that
efficient parsing for grammars that allow for large numbers of
unbounded dependencies from within one node may be problematic, even
in the categorial framework. 
Note that the fact, that this problematic case doesn't show up in
the correct analysis of normal NL sentences, doesn't mean that a
parser wouldn't have to try it, unless some arbitrary bound to that
number is assumed. 
For practical grammar engineering one can devise the motto avoid
  accumulation of unbounded dependencies by whatever means.

On the theoretical side we think that this result for SDL is
also of some importance, since SDL exhibits a core of
logical behaviour that any (Lambek-based) logic must have which accounts
for non-peripheral extraction by some form of permutation. And hence,
this result increases our understanding of the necessary computational
properties of such richer systems. To
  our knowledge the question, whether the Lambek 
  calculus itself or its associated parsing problem are NP-hard, are
  still open.

J. van Benthem.
The Lambek Calculus.
In R. T. O. et al. (Ed.), Categorial Grammars and Natural
  Language Structures, pp. 35-68. Reidel, 1988.

M. R. Garey and D. S. Johnson.
Computers and Intractability--A Guide to the Theory
  of NP-Completeness.
Freeman, San Francisco, Cal., 1979.

J.-Y. Girard.
Linear Logic.
Theoretical Computer Science, 50(1):1-102, 1987.

E. Knig.
LexGram - a practical categorial grammar formalism.
In Proceedings of the Workshop on Computational Logic for
  Natural Language Processing. A Joint COMPULOGNET/ELSNET/EAGLES Workshop,
  Edinburgh, Scotland, April 1995.

J. Lambek.
The Mathematics of Sentence Structure.
American Mathematical Monthly, 65(3):154-170, 1958.

P. Lincoln and T. Winkler.
Constant-Only Multiplicative Linear Logic is NP-Complete.
Theoretical Computer Science, 135(1):155-169, Dec. 1994.

M. Moortgat.
Residuation in Mixed Lambek Systems.
In M. Moortgat (Ed.), Lambek Calculus. Multimodal and
  Polymorphic Extensions, DYANA-2 deliverable R1.1.B. ESPRIT, Basic Research
  Project 6852, Sept. 1994.

G. Morrill.
Type Logical Grammar: Categorial Logic of Signs.
Kluwer, 1994.

M. Pentus.
Lambek grammars are context free.
In Proceedings of Logic in Computer Science, Montreal,
  1993.

  The Lambek calculus with permutation LP is also called
 the ``nondirectional Lambek calculus'' . In it the leftward and rightward implication collapse.
   Morrill  achieves the same effect with a   permutation modality 
applied to the np gap:


  This name was coined by Esther Knig-Baumer, who employs a
   variant of this calculus in her LexGram system  for   practical grammar development.
   It should be pointed out
 that the resource management in this calculus is very closely
 related to the handling and interaction of local valency and
 unbounded dependencies in HPSG.
 The latter being handled with set-valued features  SLASH, 
   QUE and  REL essentially emulates the permutation
 potential of abstracted categories in semidirectional Lambek
 Grammar. A more detailed analysis of the relation between HPSG and
  SDL is given in . 
   In contrast to Linear Logic  the order of   types in U is essential, since the structural rule of permutation
  is not assumed to hold. Moreover, the fact that only a single
  formula may appear on the right of ,
make the Lambek calculus
  an intuitionistic fragment of the multiplicative fragment of
  non-commutative propositional Linear Logic.
   A similar reduction has been used in  to show   that derivability in the multiplicative fragment of propositional
  Linear Logic with only the connectives llimp and lltensor
  (equivalently Lambek calculus with permutation LP) is
  NP-complete.
