
Recently various methods for automatically constructing a thesaurus
(hierarchically clustering words) based on corpus data have been
 proposed ,,,. The realization of such an automatic construction method would make it possible to a)
save the cost of constructing a thesaurus by hand, b) do away with
subjectivity inherent in a hand made thesaurus, and c) make it easier
to adapt a natural language processing system to a new domain. In this
paper, we propose a new method for automatic construction of thesauri.
Specifically, we view the problem of automatically clustering words as
that of estimating a joint distribution over the Cartesian product of
a partition of a set of nouns (in general, any set of words) and a
partition of a set of verbs (in general, any set of words), and
propose an estimation algorithm using simulated annealing with an
energy function based on the Minimum Description Length (MDL)
Principle. The MDL Principle is a well-motivated and theoretically
sound principle for data compression and estimation in information
theory and statistics. As a strategy of statistical estimation MDL is
guaranteed to be near optimal.

We empirically evaluated the effectiveness of our method. In
particular, we compared the performance of an MDL-based simulated
annealing algorithm in hierarchical word clustering against that of
one based on the Maximum Likelihood Estimator (MLE, for short). We
found that the MDL-based method performs better than the MLE-based
method. We also evaluated our method by conducting pp-attachment
disambiguation experiments using a thesaurus automatically constructed
by it and found that disambiguation results can be improved.

Since some words never occur in a corpus, and thus cannot be reliably
classified by a method solely based on corpus data, we propose to
combine the use of an automatically constructed thesaurus and a hand
made thesaurus in disambiguation. We conducted some experiments in
order to test the effectiveness of this strategy. Our experimental
results indicate that combining an automatically constructed thesaurus
 and a hand made thesaurus widens the   coverage of our disambiguation method, while maintaining  high accuracy. 

A method of constructing a thesaurus based on corpus data usually
consists of the following three steps: (i) Extract co-occurrence data
(e.g. case frame data, adjacency data) from a corpus, (ii) Starting
from a single class (or each word composing its own class), divide (or
merge) word classes based on the co-occurrence data using some
similarity (distance) measure. (The former approach is called
`divisive,' the latter `agglomerative.') (iii) Repeat step (ii) until
some stopping condition is met, to construct a thesaurus (tree). The
method we propose here consists of the same three steps.

 Suppose available to us are data like those in Figure , which are frequency data (co-occurrence data) between verbs and their
objects extracted from a corpus (step (i)). We then view the problem
of clustering words as that of estimating a probabilistic model
(representing probability distribution) that generates such data. We
assume that the target model can be defined in the following way.
First, we define a noun partition 
over a given set of nouns

and a verb partion 
over a given set of verbs .
A noun partition is any set 
satisfying 

,


and

.
A verb
partition 


is defined analogously. In this paper, we call
a member of a noun partition a `noun cluster,' and a member of a verb
partition a `verb cluster.' We refer to a member of the Cartesian
product of a noun partition and a verb partition ( 


) simply as a `cluster.' We then define a
probabilistic model (a joint distribution), written 

P(Cn,Cv),
where random variable Cn assumes a value from a fixed noun
partition ,
and Cv a value from a fixed verb
partition .
Within a given cluster, we assume that each
element is generated with equal probability, i.e.,



 Figure  shows two example models which might have given  rise to the data in Figure . 

In this paper, we assume that the observed data are generated by a
model belonging to the class of models just described, and select a
model which best explains the data. As a result of this, we obtain
both noun clusters and verb clusters. This problem setting is based on
the intuitive assumption that similar words occur in the same context
with roughly equal likelihood, as is made explicit in equation
 (). Thus selecting a model which best explains the given data is equivalent to finding the most appropriate classification of
words based on their co-occurrence.

We now turn to the question of what strategy (or criterion) we should
employ for estimating the best model. Our choice is the MDL
(Minimum Description Length) principle
 ,,,,, a well-known principle of data compression and statistical estimation
from information theory. MDL stipulates that the best probability
model for given data is that model which requires the least code
length for encoding of the model itself, as well as the given data
 relative to it. We refer to the code length for the model as the `model description length' and that for the data the
`data description length.'

We apply MDL to the problem of estimating a model consisting of a pair
of partitions as described above. In this context, a model with less
 clusters, such as Model 2 in Figure , tends to be simpler (in terms of the number of parameters), but also tends to have
a poorer fit to the data. In contrast, a model with more clusters,
 such as Model 1 in Figure , is more complex, but tends to have a better fit to the data. Thus, there is a trade-off
relationship between the simplicity of clustering (a model) and the
goodness of fit to the data. The model description length quantifies
the simplicity (complexity) of a model, and the data description
length quantifies the fit to the data. According to MDL, the model
which minimizes the sum total of the two types of description lengths
should be selected.

In what follows, we will describe in detail how the description length
is to be calculated in our current context, as well as our simulated
annealing algorithm based on MDL.

We will now describe how the description length for a model is
calculated. Recall that each model is specified by the Cartesian
product of a noun partition and a verb partition, and a number of
parameters for them. Here we let kn denote the size of the noun
partition, and kv the size of the verb partition. Then, there are


free parameters in a model.

Given a model M and data S, its total description length
 L(M) is computed as the sum of the model description length 

Lmod(M), the description length of its parameters 

Lpar(M),
and the data description length 

Ldat(M). (We often refer to

Lmod(M) + Lpar(M) as the model description length). Namely,



We employ the `binary noun clustering method,' in which kv is fixed
at 


and we are to decide whether kn=1 or kn=2, which
is then to be applied recursively to the clusters thus obtained. This
is as if we view the nouns as entities and the verbs as features and
cluster the entities based on their features. Since there are


subsets of the set of nouns ,
and for each
`binary' noun partition we have two different subsets (a special case
of which is when one subset is 
and the other the empty set
), the number of possible binary noun partitions is

.
Thus for each binary noun
partition we need 

 bits to describe it. Hence  
 Lmod(M) is calculated as 




Lpar(M) is calculated by 



where
  |S| denotes the input data size, and 


is the
  number of (free) parameters in the model. It is known that using




bits to describe each of
  the parameters will (approximately) minimize the description length
   .  Finally,  
Ldat(M) is calculated by



where f(n,v) denotes the observed frequency of the
noun verb pair (n,v), and 


the estimated probability
of (n,v), which is calculated as follows. 





where

f(Cn,Cv) denotes the observed frequency of the noun verb pairs
  belonging to cluster (Cn,Cv).

With the description length of a model defined in the above manner, we
wish to select a model having the minimum description length and
output it as the result of clustering. Since the model description
length Lmod is the same for each model, in practice we only need
to calculate and compare 

L'(M) = Lpar(M) + Ldat(M).

 The description lengths for the data in Figure  using  the two models in Figure  are shown in  Table . (Table  shows some values  needed for the calculation of the description
length for Model 1.)  These calculations indicate that
according to MDL, Model 1 should be selected over Model 2.

We could in principle calculate the description length for each model
and select a model with the minimum description length, if computation
time were of no concern. However, since the number of probabilistic
models under consideration is exponential, this is not feasible in
practice. We employ the `simulated annealing technique' to deal with
 this problem. Figure  shows our (divisive)  clustering algorithm. 

Although there have been many methods of word clustering proposed to
 date, their objectives seem to vary. In Table  we exhibit a simple comparison between our work and related work. Perhaps the
 method proposed by  is the most relevant in our  context. In , they proposed a method of `soft clustering,' namely, each word can belong to a number of distinct
classes with certain probabilities. Soft clustering has several
desirable properties. For example, word sense ambiguities in input
data can be treated in a unified manner. Here, we restrict our
attention on `hard clustering' (i.e., each word must belong to exactly
one class), in part because we are interested in comparing the
thesauri constructed by our method with existing hand-made thesauri.
(Note that a hand made thesaurus is based on hard
 clustering.) 

In this section, we elaborate on the merits of our method. 

In statistical natural language processing, usually the number of
parameters in a probabilistic model to be estimated is very large, and
therefore such a model is difficult to estimate with a reasonable data
size that is available in practice. (This problem is usually referred
to as the `data sparseness problem.')  We could smooth the estimated
probabilities using an existing smoothing technique (e.g.,
 ,), then calculate some similarity measure using the smoothed probabilities, and then cluster words according to it.
There is no guarantee, however, that the employed smoothing method is
in any way consistent with the clustering method used subsequently.
Our method based on MDL resolves this issue in a unified fashion. By
employing models that embody the assumption that words belonging to a
same cluster occur in the same context with equal likelihood, our
method achieves the smoothing effect as a side effect of the
clustering process, where the domains of smoothing coincide with the
clusters obtained by clustering. Thus, the coarseness or fineness of
clustering also determines the degree of smoothing. All of these
effects fall out naturally as a corollary of the imperative of `best
possible estimation,' the original motivation behind the MDL
principle.

In our simulated annealing algorithm, we could alternatively employ
the Maximum Likelihood Estimator (MLE) as criterion for the best
probabilistic model, instead of MDL. MLE, as its name suggests,
selects a model which maximizes the likelihood of the data, that is,

.
This is equivalent to
minimizing the `data description length' as defined in Section 3, i.e. 

.
We can see easily
that MDL generalizes MLE, in that it also takes into account the
complexity of the model itself. In the presence of models with varying
complexity, MLE tends to overfit the data, and output a model that is
too complex and tailored to fit the specifics of the input data. If we
employ MLE as criterion in our simulated annealing algorithm, it will
result in selecting a very fine model with many small clusters, most
of which will have probabilities estimated as zero. Thus, in contrast
to employing MDL, it will not have the effect of smoothing at all.

Purely as a method of estimation as well, the superiority of MDL over
MLE is supported by convincing theoretical findings (c.f.
 ,). For instance, the speed of convergence of the models selected by MDL to the true model is known to be near
optimal. (The models selected by MDL converge to the true model
approximately at the rate of 1/s where s is the number of
parameters in the true model, whereas for MLE the rate is 1/t, where
t is the size of the domain, or in our context, the total number of
elements of 

.)  `Consistency' is another 
desirable property of MDL, which is not shared by MLE. That is, the
number of parameters in the models selected by MDL converge to that of
 the true model . Both of these properties of MDL are empirically verified in our present context, as will be shown in the
next section. In particular, we have compared the performance of
employing an MDL-based simulated annealing against that of one based
on MLE in word clustering.

We describe our experimental results in this section.

We compared the performance of employing MDL as a criterion in our
simulated annealing algorithm, against that of employing MLE by
simulation experiments. We artificially constructed a true model
of word co-occurrence, and then generated data according to its
distribution. We then used the data to estimate a model (clustering
 words), and measured the KL distance between the true model and the estimated model. (The algorithm used for MLE was the same as that
 shown in Figure , except the `data description length' replaces the (total) description length' in Step 2.)  Figure
 (a) plots the relation between the number of obtained noun clusters (leaf nodes in the obtained thesaurus tree) versus the
input data size, averaged over 10 trials. (The number of noun
 clusters in the true model is 4.)  Figure (b) plots the KL distance versus the data size, also averaged over the same 10trials.  The results indicate that MDL converges to the true model
faster than MLE. Also, MLE tends to select a model overfitting the
data, while MDL tends to select a model which is simple and yet fits
the data reasonably well. We conducted the same simulation experiment
for some other models and found the same tendencies.
 (Figure (a) and Figure (b) show the analogous results when the number of noun clusters in the true model
is 2). We conclude that it is better to employ MDL than MLE in word
clustering.

We extracted roughly 180,000 case frames from the bracketed WSJ
 (Wall Street Journal) corpus of the Penn Tree Bank  as co-occurrence data. We then constructed a number of thesauri based on
 these data, using our method. Figure  shows an example thesaurus for the 20 most frequently occurred nouns in the
data, constructed based on their appearances as subject and object of
roughly 2000 verbs. The obtained thesaurus seems to agree with human
intuition to some degree. For example, `million' and `billion' are
classified in one noun cluster, and `stock' and `share' are classified
together. Not all of the noun clusters, however, seem to be meaningful
in the useful sense. This is probably because the data size we had was
not large enough. This general tendency is also observed in another
example thesaurus obtained by our method, shown in
 Figure . Pragmatically speaking, however, whether the obtained thesaurus agrees with our intuition in itself is only of
secondary concern, since the main purpose is to use the constructed
thesaurus to help improve on a disambiguation task.

We also evaluated our method by using a constructed thesaurus in a
pp-attachment disambiguation experiment.

We used as training data the same 180,000 case frames in Experiment
1. We also extracted as our test data 172

(verb,noun1,prep,noun2) patterns from the data in the same corpus,
which is not used in the training data. For the 150 words that
appear in the position of noun2, we constructed a thesaurus based
on the co-occurrences between heads and slot values of the frames in
the training data. This is because in our disambiguation test we only
need a thesaurus consisting of these 150 words. We then applied the
 learning method proposed in  to learn case frame patterns with the constructed thesaurus as input using the same training data.
That is, we used it to learn the conditional distributions 

P(Class1
| verb,prep), 

P(Class2 | noun1, prep), where Class1 and
Class2 vary over the internal nodes in a certain `cut' in the
 thesaurus tree shows some example case frame patterns obtained  by this method, and Figure     shows the leaf nodes dominated by the internal nodes appearing in the case frame patterns
 of Table . 

We then compare 


and 

,
which are estimated based on the case frame patterns,
to determine the attachment site of 

 (prep, noun2). More specifically, if the former is larger than the latter, we attach it to verb, and if the latter
is larger than the former, we attach it to noun1, and otherwise
(including when both are 0), we conclude that we cannot make a
 decision. Table  shows the results of our pp-attachment disambiguation experiment in terms of `coverage' and
`accuracy.'  Here `coverage' refers to the proportion (in percentage)
of the test patterns on which the disambiguation method could make a
decision. `Base Line' refers to the method of always attaching

(prep,noun2) to noun1. `Word-Based,' `MLE-Thesaurus,' and
`MDL-Thesaurus' respectively stand for using word-based estimates,
using a thesaurus constructed by employing MLE, and using a thesaurus
constructed by our method. Note that the coverage of `MDL-Thesaurus'
significantly outperformed that of `Word-Based,' while basically
maintaining high accuracy (though it drops somewhat), indicating that
using an automatically constructed thesaurus can improve
disambiguation results in terms of coverage.

 We also tested the method proposed in  of learning case frames patterns using an existing thesaurus. In particular, we used
 this method with WordNet  and using the same training data, and then conducted pp-attachment disambiguation experiment using
the obtained case frame patterns. We show the result of this
 experiment as `WordNet' in Table . We can see that in terms of `coverage,' `WordNet' outperforms `MDL-Thesaurus,' but in
terms of `accuracy,' `MDL-Thesaurus' outperforms `WordNet.' These
results can be interpreted as follows. An automatically constructed
thesaurus is more domain dependent and captures the domain dependent
features better, and thus using it achieves high accuracy. On the
other hand, since training data we had available is insufficient, its
coverage is smaller than that of a hand made thesaurus. In practice,
it makes sense to combine both types of thesauri. More specifically,
an automatically constructed thesaurus can be used within its
coverage, and outside its coverage, a hand made thesaurus can be used.
Given the current state of the word clustering technique (namely, it
requires data size that is usually not available, and it tends to be
computationally demanding), this strategy is practical. We show the
result of this combined method as `MDL-Thesaurus + WordNet' in
 Table . Our experimental result shows that employing the combined method does increase the coverage of disambiguation. We
also tested `MDL-Thesaurus + WordNet + LA + Default,' which stands for
using the learned thesaurus and WordNet first, then the lexical
 association value proposed by , and finally the default (i.e. always attaching 

prep, noun2 to noun1). Our best
disambiguation result obtained using this last combined method
 somewhat improves the accuracy reported in  (). 

We have proposed a method of clustering words based on large corpus
data. We conclude with the following remarks.
1.
Our method of hierarchical clustering of words based on the MDL
principle is theoretically sound. Our experimental results show that
  it is better to employ MDL than MLE as estimation criterion in word
  clustering.
2.
Using a thesaurus constructed by our method can improve
  pp-attachment disambiguation results.
3.
At the current state of the art in statistical natural language
  processing, it is best to use a combination of an automatically
  constructed thesaurus and a hand made thesaurus for disambiguation
  purpose. The disambiguation accuracy obtained this way was .

In the future, hopefully with larger training data size, we plan to
construct larger thesauri as well as to test other clustering
algorithms.

We thank Mr.K.Nakamura, Mr.T.Fujita, and Dr.K.Kobayashi of NEC CC
Res. Labs. for their constant encouragement. We thank Dr.K.Yamanishi
of CC Res. Labs. for his comments. We thank Ms.Y.Yamaguchi of NIS 
for her programming effort.

Andrew R. Barron and Thomas M. Cover.
1991.
Minimum complexity density estimation.
IEEE Transaction on Information Theory, 37(4):1034-1054.

Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and
  Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):283-298.

Thomas M. Cover and Joy A. Thomas.
1991.
Elements of Information Theory.
John Wiley  Sons Inc.

Ido Dagan, Shaul Marcus, and Shaul Makovitch.
1992.
Contextual word similarity and estimation from sparse data.
Proceedings of the 30th ACL, pages 164-171.

Williams A. Gale and Kenth W. Church.
1990.
Poor estimates of context are worse than none.
Proceedings of the DARPA Speech and Natural Language Workshop,
  pages 283-287.

Donald Hindle and Mats Rooth.
1991.
Structural ambiguity and lexical relations.
Proceedings of the 29th ACL, pages 229-236.

Donald Hindle.
1990.
Noun classification from predicate-argument structures.
Proceedings of the 28th ACL, pages 268-275.

Hang Li and Naoki Abe.
1995.
Generalizing case frames using a thesaurus and the MDL principle.
Proceedings of Recent Advances in Natural Language Processing,
  pages 239-248.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993.
Building a large annotated corpus of English: The penn treebank.
Computational Linguistics, 19(1):313-330.

George A. Miller, Richard Beckwith, Chirstiane Fellbaum, Derek Gross, and
  Katherine Miller.
1993.
Introduction to WordNet: An on-line lexical database.
Anonymous FTP: clarity.princeton.edu.

Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.
Distributional clustering of english words.
Proceedings of the 31st ACL, pages 183-190.

J. Ross Quinlan and Ronald L. Rivest.
1989.
Inferring decision trees using the minimum description length
  principle.
Information and Computation, 80:227-248.

Jorma Rissanen.
1978.
Modeling by shortest data description.
Automatic, 14:37-38.

Jorma Rissanen.
1983.
A universal prior for integers and estimation by minimum description
  length.
The Annals of Statistics, 11(2):416-431.

Jorma Rissanen.
1984.
Universal coding, information, predication and estimation.
IEEE Transaction on Information Theory, 30(4):629-636.

Jorma Rissanen.
1986.
Stochastic complexity and modeling.
The Annals of Statistics, 14(3):1080-1100.

Jorma Rissanen.
1989.
Stochastic Complexity in Statistical Inquiry.
World Scientific Publishing Co.

Andreas Stolcke and Stephen Omohundro.
1994.
Inducing probabilistic grammars by bayesian model merging.
Proceedings of ICGI'94.

Takenobu Tokunaga, Makoto Iwayama, and Hozumi Tanaka.
1995.
Automatic thesaurus construction based-on grammatical relations.
Proceedings of IJCAI'95.

Kenji Yamanishi.
1992.
A learning criterion for stochastic rules.
Machine Learning, 9:165-203.

  Real World Computing Partership
  `Coverage' refers to the proportion (in
  percentage) of test data for which the disambiguation method can
  make a decision.
  `Accuracy' refers to the success rate,
  given that the disambiguation method makes a decision.
   We refer the interested reader to    for explanation of rationals behind using the MDL principle in
  natural language processing.
  L(M) depends on S, but we will leave S  implicit.
  Throughout the paper `' denotes the logarithm to
  the base 2.
  For further explanation, see
   . 
  The
  exact formulation of 

Lmod(M) is subjective, and it depends on
  the exact coding scheme used for the description of the models.
  As we noted earlier, an alternative
  would be to employ an agglomerative algorithm.
  We wish to investigate the possibility of
  employing MDL in soft clustering in the near future. Since MDL is a
  general criterion for statistical estimation, it can be used in
  other problem settings of word clustering. For example, recently,
  Stolcke  Omohundro proposed to use a Bayesian model merging
   technique , which is similar to MDL, in the problem   setting of word clustering in the context of estimating `n-gram
   models,' proposed by . 
  The KL distance
  (relative entropy), which is widely used in information theory and
  statistics, is a measure of `distance' between two distributions
   . It is always non-negative and is zero iff the two   distributions are identical, but is asymmetric and hence not a
  metric (the usual notion of distance).
  Each `cut' in a thesaurus tree defines a
   different noun partition. See  for detail. 
  For
   further detail see . 
