<?xml version='1.0'?>
<!DOCTYPE MINIMAL-DOC SYSTEM "mini.dtd">
<MINIMAL-DOC>
<TITLE>On Descriptive Complexity, Language Complexity, and GB1</TITLE>
<ABSTRACT>
<P>
We introduce  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
a monadic second-order language for reasoning
about trees which characterizes the strongly Context-Free Languages in the
sense that a set of finite trees is definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff it is (modulo a
projection) a Local Set--the set of derivation trees generated by a CFG.
This provides a flexible approach to establishing language-theoretic complexity
results for formalisms that are based on systems of well-formedness constraints
on trees.  We demonstrate this technique by sketching two such results for
Government and Binding Theory.  First, we show that free-indexation,
the mechanism assumed to mediate a variety of 
agreement and binding relationships in GB, is not definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and
therefore not enforcible by CFGs.  Second, we show how, in spite of this
limitation, a reasonably complete GB account of English can be defined in
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Consequently, the language licensed by that account is strongly context-free.
We illustrate some of the issues involved in establishing this result by
looking at the definition, in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
of chains.  The limitations of this
definition provide some insight into the types of natural linguistic principles
that correspond to higher levels of language complexity.  We close with some
speculation on the possible significance of these results for generative
linguistics.
</P>
</ABSTRACT>
<BODY>
<DIV ID="1" DEPTH="1" R-NO="1"><HEADER>  Introduction
</HEADER>
<P>
One of the more significant developments in generative linguistics over
the last decade has been the development of constraint-based formalisms--grammar formalisms that define languages
not in terms of the derivations of the strings in the language, but
rather in terms 
of well-formedness conditions on the structures analyzing their syntax.
Because traditional notions 
of language complexity are generally defined in terms of rewriting mechanisms,
complexity of the languages licensed by these formalisms can be
difficult to determine.
</P>
<P>
A particular example, one that will be a focus of this paper, is
Government and Binding Theory.  While this is often modeled as a
specific range of Transformational Grammars, the connection between
the underlying grammar mechanism and the language a given GB theory
licenses is quite weak.  In an extreme view, one can take the underlying
mechanism simply to generate the set of all finite trees (labeled with
 some alphabet of symbols)  while the grammatical theory is actually 
embodied in a set of principles that filter out the ill-formed analyses.
As a result, it has been difficult to establish language
complexity results for GB theories, even at the level of the
 recursive <REF/>,<REF/>  or context-sensitive <REF/> languages. 
</P>
<P>
That language complexity results for GB should be difficult to come by
is hardly surprising.  The development of GB coincided with the
abandonment, by GB theorists, of the presumption that the traditional
language complexity classes would provide any useful characterization of
the human languages.  This followed, at least in part, from the recognition of
the fact that
the structural properties that characterize natural languages as a class may
well not be those that can be distinguished by existing language complexity
classes. 
There was a realization that the theory needed to be driven
by the regularities identifiable in natural languages, rather than
those suggested by abstract mechanisms.  Berwick characterized this
approach as aiming to ``discover the properties of natural languages
 first, and then characterize them formally.'' <REF/>, pg. 100] 
</P>
<P>
But formal language theory still has much to offer to generative
linguistics.  Language complexity provides one of the most useful
measures with which to compare languages and language formalisms. 
We have an array of results establishing the
boundaries of these classes, and, while many of the results do not seem
immediately germane to natural languages,
even seemingly artificial diagnostics
(like the copy language 
<!-- MATH: $\set{ww\mid w\in (ab)^*}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 )
can provide the basis
for useful classification results (such as Shieber's argument for the
 non-context-freeness of Swiss-German <REF/>). More importantly, characterization results for language
complexity classes tend to be in terms of the structure
of languages, and the structure of natural language, while hazy, is
something that can be studied more or less directly.  Thus there is a
realistic expectation of finding empirical evidence falsifying a given
hypothesis.  (Although such evidence may well be difficult to find, as
witnessed by the history of less successful attempts to establish results such
 as Shieber's <REF/>,<REF/>.) Further,
language complexity classes characterize, along one dimension, the
types
of resources necessary to parse or recognize a language.  Results of
this type for the class of human languages, then, make specific
predictions about the nature of the human language faculty, predictions
that, at least in principle, can both inform and be informed by progress
in uncovering the physical nature of that faculty.
</P>
<P>
In this paper we discuss a flexible and quite powerful approach to
establishing language complexity results for formalisms based on systems
 of constraints on trees.  In Section <CREF/> we introduce a logical language, 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
capable
of encoding such constraints lucidly.  The key merit of such an encoding
is the fact that sets of trees are definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
if and only if
they are strongly context-free. Thus definability in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 characterizes the strongly context-free languages.  This is our primary
 result, and we develop it in Section <CREF/>. 
</P>
<P>
We have used this technique to establish both inclusion and exclusion
results for a variety of linguistic principles within the GB
 framework <REF/>.  In the remainder of the paper we demonstrate some of these.  In
 Section <CREF/> we sketch a proof of the non-definability of free-indexation, a mechanism that is nearly ubiquitous in GB theories. 
The consequence of this result is that languages that are licensed by
theories that necessarily employ free-indexation are outside of the
class of CFLs.  Despite the unavailability of free-indexation, we are
able to capture a mostly standard GB account of English within 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Thus we are able to show that the language licensed by this particular
 GB theory is strongly context-free. In Section <CREF/> we illustrate some of the issues 
involved in establishing this result, particularly in light of the
non-definability of free-indexation.  We
close, finally, with some
speculation on the possible significance of these results for generative
linguistics.
</P>
</DIV>
<DIV ID="2" DEPTH="1" R-NO="2"><HEADER>  
<!-- MATH: $L^2_{{K},{P}}$ -->
L[2]K,P
</HEADER>
<P>
The idea of employing mathematical logic to provide a precise formalization of
GB theories is a natural one.  This has been done, for instance, by
 Johnson <REF/>   and Stabler <REF/> using first-order logic (or the Horn-clause  fragment of first-order logic) and by Kracht <REF/> using a fragment of dynamic logic.  What distinguishes the formalization we discuss
is the fact that it is
carried out in a language which can only define strongly context-free sets.
The fact that the formalization is possible, then, establishes a relatively 
strong language complexity result for the theory we capture.
</P>
<P>
We have, then, two conflicting criteria for our language.  It must be
expressive enough to capture the relationships that define the trees licensed
by the theory, but it must be restricted sufficiently to be no more
expressive than Context-Free Grammars.
In keeping with the first of these our language is intended to support, as
transparently as possible, 
the kinds of reasoning about trees typical of linguistic applications.
It includes binary predicates for  the usual structural relationships between
the nodes in the trees--parent (immediate domination), 
domination (reflexive), proper domination (irreflexive), left-of (linear
precedence) and equality.  In addition, it includes an arbitrary array
of monadic predicate constants--constants naming specific subsets of
the nodes in the tree.  These can be thought of as atomic labels.
The formula 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
for instance, is true at
every node labeled N.  It includes, also, a similar array of
individual constants--constants naming specific individuals in the
tree--although these prove to be of limited usefulness.  There are two
sorts of variables as well--those that range over nodes in the tree and those
that range over arbitrary subsets of those nodes (thus this is is monadic
second-order language).  Crucially, though, this is all the language includes.
By restricting ourselves to this language we restrict ourselves to working with
properties that can be expressed in terms of these basic predicates.
</P>
<P>
To be precise, the actual language we use in a given situation depends on the
sets of constants in use in that context.  We are concerned then with a
family of languages, parameterized by the sets of individual and set
constants they employ.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
We use infix notation for the fixed predicate constants
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We use lower-case for
individual variables and constants, and upper-case for set variables and
predicate constants.  Further, we will say X(x) to assert that the individual
assigned to the variable x is included in the set assigned to the
variable X.  So, for instance,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
asserts that the set assigned to X includes every node dominated by the node
assigned to x.
</P>
<P>
Truth, for these languages, is defined relative to a specific class of models.
The basic models are just ordinary structures interpreting the individual
and predicate constants.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
If the domain of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is empty (i.e., the model is for a language
<!-- MATH: $L_{\emptyset,\bms{P}}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 )
we will generally omit it.  Models for
<!-- MATH: $L_{\emptyset,\emptyset}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
then, are tuples 
<!-- MATH: $\tup{\sU,\sP,\sD,\sL}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
</P>
<P>
The intended class of these models are, in essence, labeled tree domains.
A tree domain is the set of node addresses generated by giving
the address 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
to the root and giving the children of the node
at address w addresses (in order, left to right)
<!-- MATH: $w\cdot 0, w\cdot 1,\ldots$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
where the centered dot denotes
 concatenation. Tree domains, then, are particular subsets of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
(
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is the
set of natural numbers.)
</P>
<IMAGE TYPE="FIGURE"/>
<P>
>
</P>
<P>
Every tree domain has a natural interpretation as a model for
<!-- MATH: $L_{\emptyset,\emptyset}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
(which interprets only the fixed predicate symbols.)
</P>
<IMAGE TYPE="FIGURE"/>
<P>
>
</P>
<P>
The structures of interest to us are just those models that are the natural
interpretation of a tree domain, augmented with interpretations of
 additional individual and predicate constants. 
</P>
<P>
In general, satisfaction is relative to an assignment mapping each
individual variable into a member of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and each predicate variable
into a subset of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We use
</P>
<IMAGE TYPE="FIGURE"/>
<P>
to denote that a model M satisfies a formula 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
with an
assignment s.  The notation
</P>
<IMAGE TYPE="FIGURE"/>
<P>
asserts that M models 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
with any assignment.  When 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a
sentence (has no unquantified variables) we will usually use this form.
</P>
<P>
Proper domination is a defined predicate:
</P>
<IMAGE TYPE="FIGURE"/>
<DIV ID="2.1" DEPTH="2" R-NO="1"><HEADER>  Definability in L[2]K,P
</HEADER>
<P>
We are interested in the subsets of the class of intended
models which are definable
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
using any sets K and P.  
If  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a set of
sentences in a language 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
we will use the notation 
<!-- MATH: $\Mod(\Phi)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
to denote
the set of trees, i.e., 
intended models, that satisfy all of the sentences in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We are
interested, then, in the sets of trees that are 
<!-- MATH: $\Mod(\Phi)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
for some such
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
In developing our definitions we can use individual and monadic
predicates freely (since K and P can always be taken to
be the sets that actually occur in our definitions) and we can quantify over
individuals and sets of individuals.  We will also use
non-monadic predicates and even higher-order predicates,
e.g., properties of subsets, but only those that can be explicitly defined, that is, those which can be eliminated by a simple
syntactic replacement of the predicate by its definition.
</P>
<P>
This use of explicitly defined predicates is crucial to the transparency of
definitions in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We might, for instance, define a simplified version of
government in three steps:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in words, x governs y iff it c-commands y and no barrier
intervenes between them.  It 
c-commands y iff neither x nor y dominates the other and every
branching node that properly dominates x also properly dominates y.
<!-- MATH: $\f{Branches}(x)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is just a monadic predicate; it is within the language of
</P>
<IMAGE TYPE="FIGURE"/>
<P>
(for suitable P) and its definition is simply a biconditional 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 formula. 
In contrast, C-Command and Governs are non-monadic and do
not occur in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Their definitions, however, are ultimately in
terms of monadic predicates and the fixed predicates (parent,
etc.) only. One can replace each of their occurrences in a formula
with the right hand side of their definitions and eventually derive a
formula that is in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We will reserve the use of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
(in
contrast to 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 )
for explicit definitions of non-monadic predicates.
</P>
<P>
Definitions can also use predicates expressing properties of sets and relations
between sets, as long as those properties can be explicitly defined.  The 
subset relation, for instance can be defined:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
We can also
capture the stronger notion of one set being partitioned by a collection
of others:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Here 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a some sequence of set variables and 
<!-- MATH: $\bigvee_{X\in\vec{X}} X(x)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is shorthand for the disjunction 
<!-- MATH: $X_0(x)\lor
X_1(x)\cdots$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
for all Xi in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
etc.  There is a distinct
instance of 
<!-- MATH: $\f{Partiton}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
for each sequence 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
although we can ignore
distinctions between sequences of the same length.
Finally, we note that finiteness is a definable property of subsets in
our intended models.  This follows from the fact that these models are
linearly ordered by the  lexicographic order relation: 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and that every non-empty subset of such a model has a least element with
respect to that order.  
A set of nodes, then, is finite iff
each of its non-empty subsets has an upper-bound with respect to lexicographic
order as well.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
These three second-order relations will play a role in the next section.
</P>
</DIV>
</DIV>
<DIV ID="3" DEPTH="1" R-NO="3"><HEADER>  Characterizing the Local Sets
</HEADER>
<P>
We can now give an example of a class of sets of trees that is definable in
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 --the local sets (i.e., the sets of derivation trees generated by
Context-Free Grammars).  The idea behind the definition is simple.  Given an
arbitrary Context-Free Grammar, we can treat its terminal and
non-terminal symbols as monadic predicate constants.  The productions of the
grammar, then, relate the label of a node to the number and labels of its
children.  If the set of productions for a non-terminal A, for instance, is
</P>
<IMAGE TYPE="FIGURE"/>
<P>
we can translate this as
</P>
<IMAGE TYPE="FIGURE"/>
<P>
where
</P>
<IMAGE TYPE="FIGURE"/>
<P>
We can collect such translations of all the productions of the grammar together
with sentences requiring nodes labeled with terminal symbols to have no
children, requiring the root to be labeled with the start symbol, requiring
the sets of nodes labeled with the terminal and non-terminal symbols to
partition the set of all nodes in the tree, and requiring that set of nodes to
be finite.  It is easy to show that the models of this set of sentences
 are all and only the derivation trees of the grammar.  In this way we get the first half of our characterization of the local sets.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
It is, perhaps, not surprising that we can define the local sets with 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
This is superficially quite a powerful language, allowing, as it does, a
certain amount of second-order quantification.  It is maybe more remarkable
that, modulo a projection, the only sets of finite trees (with
bounded branching)
that are definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
are the local sets.  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
The proof hinges on the fact that one can translate formulae in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
into the
language of SnS--the monadic second-order theory of multiple successor
functions.  This is the monadic second-order theory of the structure
</P>
<IMAGE TYPE="FIGURE"/>
<P>
a generalization of the
natural numbers with successor and less-than.  The universe, Tn, is the
complete n-branching tree domain.  The relation 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is domination,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is lexicographic order, and the functions 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
are the
successor functions, each taking nodes into their 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
child (
<!-- MATH: $w\mapsto
wi$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
).  Rabin <REF/> showed that SnS is decidable for any  <!-- MATH: $n\leq
\omega$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
One way of understanding his proof is via the observation that 
satisfying assignments for
a formula 
<!-- MATH: $\phi(\vec{X})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
 with free variables among  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
can be understood as trees
labeled with (subsets of) 
the variables in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
A node is in the set assigned to
Xi in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff it is labeled with Xi.  Rabin showed that, for any
<!-- MATH: $\phi(\vec{X})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in the language of SnS, the set of trees encoding the
satisfying assignments for 
<!-- MATH: $\phi(\vec{X})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is accepted by a
particular type of finite-state automaton on infinite trees.  We say that the
set is Rabin recognizable.  He goes on to show that emptiness of these
sets is decidable.  It follows that satisfiability of these
formulae, and hence the theory SnS, is decidable.
</P>
<P>
For us, the key point is the fact that the sets encoding satisfying assignments
are Rabin recognizable.  It is not difficult to exhibit a syntactic
transformation which, given any 
<!-- MATH: $\psi(\vec{X})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
produces a
formula 
<!-- MATH: $\phi(X_U,\vec{X}_P,\vec{X})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in the language of SnS,
where XU is a new variable and
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a sequence of new variables (one for each of the finitely many
predicates in P that occur in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 )
such that,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that is, the set AU and 
the sequences of sets 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
form
a satisfying assignment for 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff the structure consisting of
the universe AU along with the natural interpretation of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
on AU, and the sets 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
satisfies 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
with the assignment taking 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 into 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
It follows that a set of trees is definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff
they are Rabin recognizable.
</P>
<P>
If we restrict our attention to sets of finite trees, we can take Rabin's
automata
 to be ordinary finite-state automata over finite trees <REF/>, that is, 
the sets of finite trees that are definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
are simply recognizable.  One can think of these automata as traversing the tree, top
down, assigning states to the children of a node on the basis of a transition
function that depends on the state of the node, its label, and the position of
the child among its siblings.  A tree is accepted if it can be labeled by the
automaton in such a way that the root is labeled with a start state and the
set of states labeling the leaves is one of a set of accepting sets of
states. Every set of trees that is accepted in this way is the projection of a
 local set.   To see this, suppose that 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a tree accepted by a tree automaton.  Then there is some assignment
of states to the nodes in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that witnesses this fact.  
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
Suppose, for
instance, 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 is the tree of Figure <CREF/>, labeled as shown. Consider the tree 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in which each node is labeled with a pair consisting
of the label from 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and the state assigned to that node.  It is easy to show that, given a
recognizable set of trees, one can construct a CFG to generate the
corresponding set of trees labeled with pairs as in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
In the example,
for instance, this would include, among others, the productions
</P>
<IMAGE TYPE="FIGURE"/>
<P>
The original set of trees is then the first projection of the set generated by
the CFG.
</P>
<P>
Together, these two theorems give us our primary result.
</P>
<IMAGE TYPE="FIGURE"/>
</DIV>
<DIV ID="4" DEPTH="1" R-NO="4"><HEADER>  Non-Definability of Free Indexation
</HEADER>
<P>
This characterization provides a powerful tool for establishing strong
context-freeness of  classes of languages that are
defined by constraints on the structure of the trees analyzing the strings in
the language.  If one can show that the
constraints defining such a set, or perhaps that any constraints in the class
employed by a given formalism, can be defined within
</P>
<IMAGE TYPE="FIGURE"/>
<P>
then the corresponding language or class of languages is
strongly context-free.  Much of the value of standard language complexity
classes, on the other hand, comes from results that allow one to show that a
given language or class of languages is not included in a particular complexity
class.  Such
results are available here as well, in the form of non-definability results for
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
One relatively easy way of establishing such results is by employing
 the contrapositive of Theorem <CREF/>.  If one can show that a given predicate, when added to 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
allows definition of known
non-CF languages, then clearly that predicate properly extends the power of the
language and cannot be definable.  In this way, one can show that the predicate
<!-- MATH: $\f{YieldsEq}_P(x,y)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
which holds between two nodes iff the yields of the
subtrees rooted at those nodes are labeled identically wrt P is not definable
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
for if it were one could define the copy language 
<!-- MATH: $\set{ww\mid w\in
(ab)^*}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
</P>
<P>
In this section we will explore an approach that is more difficult but is
one of the most  general--reduction from the monadic second-order theory of
the grid--and will use it to demonstrate non-definability of
free-indexation--a mechanism which shows up in a number of modules of GB.
</P>
<P>
The grid is the structure
<!-- MATH: $G=\tup{\Nat^2,\f{O},\f{r}_0,\f{r}_1}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
where
</P>
<IMAGE TYPE="FIGURE"/>
<P>
This is the structure of the (discrete) first quadrant.  Note the similarity to
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
the structure of two successor functions.  The key distinction is the
fact that G satisfies the property 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that is, the horizontal successor of the vertical successor of a point is the
same as the vertical successor of its horizontal successor.  Let 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
be
 the monadic second-order theory of G.  Lewis <REF/> showed that this theory is undecidable by showing how one could define the set of
terminating computations of an arbitrary Turing machine within it.
</P>
<P>
Now, the monadic second-order theory of any of our intended structures is
decidable (by reduction to SnS), as is the monadic second-order theory of any
of our intended structures augmented with any predicate that is definable in
</P>
<IMAGE TYPE="FIGURE"/>
<P>
(since we can reduce this to the theory of the original structure via
that definition).  Our approach to showing that a predicate is not
definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is to show that the theory of one of our structures
augmented with that predicate is not decidable.  In particular, we will show
that the theory of such a structure includes an undecidable fragment of the
monadic second-order theory of the grid.
</P>
<P>
Our focus, in this section, is the mechanism known as free-indexation.
In the Government and Binding Theory framework this is the mechanism that
is generally assumed
to mediate issues like agreement, co-reference of nominals, and identification
of moved elements with their traces.  In its most general form this operates by
assigning indices to the nodes of the tree randomly and then filtering out
those assignments that do not meet various constraints on agreement,
co-reference, etc.  In essence, the indexation is an equivalence relation, one
that distinguishes  
unboundedly many equivalence classes among the nodes of the tree.  That is,
each value of the index identifies an equivalence class and there is no a
priori bound on its maximum value.  Free-indexation views constraints on the
indexation as a filter that admits only those equivalence relations that meet
specific conditions on the relationships between the individuals in these
classes.
</P>
<P>
To see that we cannot define such equivalence relations in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
consider the
class of structures
</P>
<IMAGE TYPE="FIGURE"/>
<P>
where T2 is the complete binary-branching tree domain, 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and
</P>
<IMAGE TYPE="FIGURE"/>
<P>
are the natural interpretations of parent, domination, and left-of on
that domain, and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is any arbitrary equivalence relation.   Let S2S+CI
be the monadic second-order theory of this class of structures.
Our claim is that
 this is an undecidable theory. 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Lewis's proof of the non-decidability of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is based on a construction
that takes any given Turning Machine M into a formula 
<!-- MATH: $\phi_M(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
such that 
<!-- MATH: $G\models(\exists \vec{P})[\phi_M(\vec{P})]$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff M halts (when
started, say, on the empty tape).  
The idea behind our proof of the non-decidability of S2S+CI is that there is a
natural correspondence between points in T2 and those in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that is
induced by interpreting node addresses in T2 as paths (non-decreasing in
both x and y) from the origin in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Of course, in general, there
will be many points in T2 that correspond to the same point in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
but
we can restrict the interpretation of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in such a way that all points in
T2 that correspond to the same point in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
will be co-indexed.  We
then restrict the interpretation of the variables in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in such a
way that it does not break the classes of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
In more typically
linguistic terms, we require 
co-indexed nodes to agree on the features in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
</P>
<P>
The formula
<!-- MATH: $\phi_M(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
of Lewis' proof
involves only the constant 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
the successor functions
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
some set of (bound) individual variables, the (free)
monadic predicate variables in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and the logical connectives.
</P>
<P>
Let
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Then 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is true only at the root,
<!-- MATH: $\f{r}_0(x,y)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is true iff y is the leftmost child of x and
<!-- MATH: $\f{r}_1(x,y)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is true iff y is the rightmost child of x.  These
translations are sufficient for us to translate 
<!-- MATH: $\phi_M(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
into a
formula 
<!-- MATH: $\psi_M(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that, when combined with an axiom
<!-- MATH: $\Phi_G(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 constraining the interpretation of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
as sketched
above, will be satisfiable by a model in the class 
<!-- MATH: $\sT_\f{CI}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff
<!-- MATH: $\phi_M(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is satisfied by G.  That is:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff
</P>
<IMAGE TYPE="FIGURE"/>
<P>
This in turn implies that
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Decidability of S2S+CI, then, would imply decidability of the halting problem.
</P>
<P>
It remains only to define 
<!-- MATH: $\Phi_G(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Let
  ALIGN="right" 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
where
</P>
<IMAGE TYPE="FIGURE"/>
<P>
This requires that
every node is co-indexed with itself,
that the left children of co-indexed nodes are
co-indexed as are the right children of co-indexed nodes,
and that the left child of the right child and right child of the left child of
co-indexed nodes are co-indexed.  Finally all co-indexed nodes are
forced, by 
<!-- MATH: $\f{Agree}_{\vec{P}}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
to agree on all predicates in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
That this is sufficient to carry the reduction of the halting
problem to membership in S2S+CI depends on the fact that
<!-- MATH: $\Phi_G(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
forces all points in T2 equivalent in the sense that
they correspond to the same
point in G as sketched above, to agree on the predicates in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Thus we (roughly) can take the quotient with respect to
this equivalence without affecting satisfiability of
<!-- MATH: $\psi_M(\vec{P})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
The resulting structure is isomorphic to G and
satisfies 
<!-- MATH: $(\exists \vec{P})[\psi_M(\vec{P})]$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
iff G satisfies 
<!-- MATH: $(\exists \vec{P})[\phi_M(\vec{P})]$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
 The proof is carried out in detail in <REF/>. 
</P>
<P>
The non-definability of free-indexation is a significant obstacle to
capturing GB accounts of language in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
As it turns out,  other constraints employed in GB theories are not generally
difficult to define.  Our ability to capture these accounts,
then, depends directly on the degree to which they necessarily employ
free-indexation.  
The common practice, in GB, is to simply assume co-indexation
almost whenever there is a 
need to identify components of the tree in some way.  Unfortunately, we
cannot capture directly accounts that are defined in these terms.
Rather, we are compelled to restate them
without reference to indices.  On the other hand, it is not at all
clear that accounts that appeal to free-indexation actually require so
general a mechanism.  On the contrary, it seems that indices are frequently
only a conceptually simple way of encoding more complicated, but less
general relationships.
There has been a tendency, in the more recent GB literature, to avoid
free-indexation in favor of these more specific relationships.  Chomsky, 
for instance, comments:
A theoretical apparatus that takes indices seriously as entities...
is questionable on more general grounds. Indices are basically the
expression of a relationship, not entities in their own right.  They
should be replaceable without loss by a structural account of the
 relation they annotate. <REF/>, pg. 49, note 52] 
</P>
<P>
This quote comes in the context of a suggestion for a re-interpretation
of the standard account of Binding Theory in a manner that avoids use
 of indices.    Rizzi, in <REF/>, motivated by an examination of a wide variety of extraction phenomena,
offers a re-interpretation of the Empty Category Principle and the
theory of chains that restricts the role of indices to a relatively
small class of movements.
As we will see in the next section,
Rizzi's theory provides us with the
foundation we need to capture a largely complete GB account of English
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We thus establish that this account licenses a strongly
context-free language.  It seems noteworthy that GB theorists have been led,
by purely linguistic considerations, to precisely the kind of re-interpretation
of the theory we require in order to establish our language-theoretic
results. 
</P>
</DIV>
<DIV ID="5" DEPTH="1" R-NO="5"><HEADER>  Defining Chains
</HEADER>
<P>
We turn now to an example that is particularly relevant to the issue of
capturing a Government and Binding Theory account of English in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and in
particular capturing it without use of 
indices.  This is our definition of chains--the core notion in
contemporary GB accounts of movement.  Our exposition is intended to be
accessible without prior familiarity with GB, although possibly
mysterious in some of its details.  It will necessarily be
somewhat meager both in the details of the definition and in the details of the
underlying theory.  A more complete treatment can be found 
 in <REF/>. 
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<DIV ID="5.1" DEPTH="2" R-NO="1"><HEADER>  Identifying Antecedents of Traces </HEADER>
<P>
Government and Binding Theory analyzes sentences with four distinct
syntactic representations which are related by the general transformation
move-
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
These are D-Structure--corresponding
to the deep-structure of earlier transformational theories, S-Structure--roughly corresponding to the surface-structure of those
theories, Phonetic Form--the actual phonetic structure of the sentence,
and Logical Form--a more or less direct representation of the sentence's
semantic content.  The principles embodying a GB theory of
language are collected into modules which apply at various levels of
this analysis. 
The principles
we capture include basic X-bar Theory, Theta Theory, the Case Filter, 
Binding Theory, Control Theory and various constraints on movement, in
particular the Empty Category Principle.
In this section we focus on the Empty Category Principle and the
definition of chains.
</P>
<P>
As we noted in the introduction, we prefer to regard GB theories as a
set of constraints on structures rather than a mechanism for
constructing them.  We take this a step further by assuming that those
constraints apply to a single tree which includes S-Structure and
 D-Structure as submodels, rather than having some constraints apply to one structure, others to the other, and others still to the relationship
between them.  In this view, D-Structure and move-
</P>
<IMAGE TYPE="FIGURE"/>
<P>
are best
understood as perspicuous means of stating constraints which are
obscured in a single-level representation (see, for instance,
 Koster <REF/> and Brody <REF/>). One argument against such a view is that in some cases (such as
head-raising) chains formed by one movement can be disrupted by
subsequent movement.  Indeed, representational accounts, such as ours,
frequently appeal to a notion of reconstruction--effectively
derivation in reverse--to resolve such difficulties.  In fact, at
least if one can employ indices to identify the elements of chains,
there is no need for such a retreat.  Even limiting oneself to the
language of L[2]K,P, if one restricts attention to languages, like
English, in which head-movement is strictly limited, it is possible to
get a purely declarative (and reasonably clear) account of the issues
usually treated by reconstruction.  Details of such an account are
 given in <REF/>. 
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
 Figure <CREF/> gives the S-Structure of a more or less typical GB analysis of the
sentence:
Whom do you think Alice will invite.
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
 In the D-Structure (Figure <CREF/>) the element carrying the inflection is positioned between the subject and the predicate and Whom is in its standard position as the object of invite.
Move-
</P>
<IMAGE TYPE="FIGURE"/>
<P>
transforms this structure by cutting out the subtrees rooted at
Ij and Ni, leaving phonetically empty traces (tj and
ti), and re-attaching them a higher positions in the tree.
In the case of Whom the movement occurs in two steps, with
traces being left at each intermediate position.  The original position
of the moved element is referred to as the base position, and its
final resting place is the target position.  The moved element is
identified with its traces by co-indexation.  Together, an element and
the traces co-indexed with it form a chain.  Chains can be broken
up into a sequence of links each consisting of a trace and its
antecedent--the next higher element of the chain.
</P>
<P>
The fundamental issue we must address in defining chains within 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 is how to identify the antecedent of a trace without reference to
indices.  Our key idea is that, if we can limit the portion of the tree
in which an antecedent can occur, then we can possibly bound the number
of potential antecedents a trace may have.  Such a bound would suffice
since, while we cannot capture indexations with an unbounded range of
index, we can capture any indexation in which there is a constant bound on 
the total number of distinct indices.
</P>
<P>
 In the standard GB account of movement, that of Barriers <REF/>, there are two principles that tend to bound the length of links.  The
first is n-subjacency, which, roughly, limits the number of
phrasal boundaries that a link can cross.  This is exactly the kind of
constraint we need.  Unfortunately it is responsible only for weak
effects; there are many sentences that violate n-subjacency that
are only of degraded acceptability rather than outright ungrammatical.
The second principle that might do is the Empty Category
Principle.  This puts specific constraints on the structural
relationship between a trace and its antecedent.  Indices, however, play a
significant role in Chomsky's formulation of this principle.
</P>
<P>
There is a formulation of ECP, due to Rizzi and
 based on his notion of Relativized Minimality <REF/>, in which the role of indexation is largely eliminated.  In Rizzi's theory,
this is a conjunctive principle with two components, a Formal Licensing
requirement and an Identification requirement:
ECP (Rizzi):
<ITEMIZE>
<ITEM>A non-pronominal empty category must be properly head-governed.
(Formal Licensing)
</ITEM>
<ITEM>Operators must be identified with their variables. (Identification)
</ITEM>
</ITEMIZE>
We are interested in the identification requirement, which, incidently,
is responsible for most of the effects attributed to ECP in the Barriers
account.  This constraint requires every trace (variable) to be identified with
its target (operator).  This can be done in one of two ways,  either by 
a particular class of index, the referential indices,
or by a sequence of antecedent-government
links.
In the latter case the role of indices in identifying chains can be
taken over by the antecedent-government relation.
</P>
<P>
To a first approximation, government is simply a relation between an
element and those elements occurring in a specifically limited region of
the tree dominated by the phrase in which that element (the governor)
occurs.  Its definition
 has three components.  First, for the class of government
relations we are considering here, the governor must c-command the
elements it governs, that is, those elements must be dominated by a sibling 
of the governor.  Second, there must be no intervening barrier.  For
Rizzi, the notion of barrier is much weaker than it is in the Barriers
account.  Here, this constraint simply forbids the government relation
from crossing certain phrasal boundaries (in particular specifiers,
adjuncts and complements of nouns or prepositions).  The final component
of the government relation requires a governor to be the minimal
potential governor of the elements it governs, that is, no potential
governor can fall properly between a governor and the elements it
governs.  There are a range of types of government relations that fall under
this general category.  In Rizzi's theory 
only potential governors of the same type count for the minimality
requirement.  (This is the relativized aspect of his theory.)  For
antecedent-government there is an additional requirement that the
governor be co-indexed with the trace.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
As we will see, we can drop the co-indexation
requirement on the grounds that, when it
exists, the antecedent-governor is unique.
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
 As an example of these relationships, consider, in Figure <CREF/>, the trace in the specifier of the lower I, that is, the trace of Who falling immediately under the I.  The elements
c-commanding this trace include the (empty) C, the ti
in the specifier of C, the V, etc.  This is a Wh-Trace
which means that, by the principles of Binding Theory,
its antecedent must fall in a non-argument
position.  In the example, the non-argument positions c-commanding the
trace are just the specifiers of the Cs.  By minimality, no
potential antecedent of the trace beyond the closest specifier of
C can govern it.  Thus the only possible antecedent-governor
of the trace in question is the trace in the specifier of
the lower C, which is, in fact, its antecedent.
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
In contrast, if we fill that position with a moved adverbial, as in the
 example of Figure <CREF/>, there is a problem.  The element why cannot be the antecedent of the trace in the specifier of the
lower I, but it blocks government by all other potential
antecedents.  Thus the trace ti cannot be identified with its
antecedent, and the sentence is ruled ungrammatical on the grounds that
it violates ECP.
</P>
<P>
In this way, minimality suffices to pick out the unique antecedent of
traces in chains that are identified by antecedent-government.  But
under Rizzi's criteria chains can also be identified by
referential indices.  These are just indices assigned to elements that
receive what are termed referential Theta roles.  Again to a
first approximation, we can take these simply to be elements that are
the objects of verbs.  
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
 In Figure <CREF/> Who is extracted from the embedded subject.
If we return to our original example, in which we
extract from the object, we find that filling the specifier of the lower
 C with a moved adverbial (Figure <CREF/>) has a less dramatic effect.  While antecedent government of the trace in the
complement of the lower V is  blocked, that trace can now be
identified with its target by the referential index they share.  The
fact that this example is not judged to be as bad as the example from
 Figure <CREF/> is attributed, then, to the fact that it is only a 1-subjacency violation rather than an ECP violation.
</P>
<P>
In general, we could be forced to resort to a mechanism equivalent to
indexation in order to distinguish such referential chains.  It turns out,
however, that in English, at least, chains of this type do not overlap.
 Manzini <REF/>, in fact, argues for an account of A-movement (movements, like these we have been considering, to non-argument positions)
which implies that no more than two such chains--one 
referential and one non-referential--may ever overlap.
Thus, we need to identify only a single referential antecedent in any
single context.
</P>
</DIV>
<DIV ID="5.2" DEPTH="2" R-NO="2"><HEADER>  Defining Antecedent-Government, Links, and Chains </HEADER>
<P>
Relativized Minimality theory distinguishes a number of distinct
varieties of antecedent-government, one for each class of movement.  
We look at one representative case
-antecedent-government.  This is defined, in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
as follows:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
In words, this says simply that x is an -antecedent-governor of y iff
x is in a non-argument () position, it c-commands y, no barrier
intervenes between x and y, and no non-argument specifier falls between xand y.  The actual definitions of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
<!-- MATH: $\f{Intervening-Barrier}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and 
<!-- MATH: $\f{Intervenes}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is unimportant
here.   The predicate 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is used to check the compatibility of the
features of the trace with those of its antecedent.
</P>
<P>
Using this, we can define the link relation. 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
This is just antecedent-government with certain additional configurational
requirements.  We can extend the notion of
links based on Rizzi's antecedent-government to include antecedents and
traces that Rizzi  identifies with a referential index
(which we refer to as -referential links), and links formed by
rightward movement.  This gives us five distinct link relations.  As they
are mutually exclusive, we can take their disjunction to form a single link
relation which must be satisfied by every trace and its antecedent.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
The idea, now, is to define chains as any set of nodes that are
linearly ordered by 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Before we can do this, though, we have one
more issue to resolve.  The problem is that, while we can identify a unique
antecedent for each trace, nothing assures us that there will be a unique trace
for each antecedent, that is, nothing prevents us from identifying the same
node as the antecedent of more than one trace.
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
As an example, we might license
 the tree in Figure <CREF/>.  This is the conflation of two sentences: 
Whoi has ti told you Alice invited him.
Whoi has Alice told you ti ti invited him.
In the first we have extracted Who from the subject of the matrix
clause and in the second we have extracted it from the subject of the embedded
clause.  We can find a link relation between Who and the trace in the
specifier of the matrix I and a link relation between Who and the
trace in the specifier of the embedded C, but clearly it cannot have moved
from both positions. 
</P>
<P>
We rule out such structures by requiring that chains not only be linearly
ordered by 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
but that they are also closed under the link relation,
that is,  every chain includes every node that is related by 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
to any
 node in the chain.  Trees like the the one in Figure <CREF/> are ruled out on the grounds that any chain that contains either of the traces in
question must include both of them, and will therefore not be linearly ordered.
</P>
<P>
Formalizing this, we get:
</P>
<IMAGE TYPE="FIGURE"/>
</DIV>
<DIV ID="5.3" DEPTH="2" R-NO="3"><HEADER>  Defining the ECP </HEADER>
<P>
We can now capture Rizzi's version of the Empty Category Principle:
</P>
<P>
Licensing
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Identification
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Note, in particular, that in our definition the identification requirement
is reduced simply to a requirement that every trace is a member of some
well-formed chain.  As we admit the notion of trivial chains--chains
with a single element, formed by zero movements--we can generalize this to a
global requirement that every element of the tree is a member of a (possibly
trivial) well-formed chain.
</P>
<P>
Identification (Generalized)
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Recall that identification is the component of Rizzi's definition  that
accounts for most of the effects attributed to ECP in the Barrier's account of
movement.  Thus we have reduced a variety of effects to a single simple global
principle.  Of course we have paid for this with a complex
definition of chains, but much of this complexity lies in the definition
of antecedent-government and Rizzi argues, on
linguistic grounds, for essentially this definition in
any case.  It is satisfying that we can recover its added complexity in the
form of a greatly simplified ECP.
</P>
</DIV>
<DIV ID="5.4" DEPTH="2" R-NO="4"><HEADER>  Limits of the Definition </HEADER>
<P>
The fact that we can exhibit a definition in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
of the class of trees
licensed by a specific GB account of English provides a strong complexity
result for that class of trees--it is strongly context-free.  We don't, on the
other hand, expect this formalization to work for GB theories in general, and,
in particular we don't expect it to work for a GB account of Universal Grammar.
</P>
<IMAGE TYPE="TABLE"/>
<IMAGE TYPE="TABLE"/>
<P>
A more or less typical account of head-raising in Dutch, for instance, is given
 in Figure <CREF/>.  This is the type of movement presumed to be responsible for the cross-serial dependencies that form the basis of Shieber's
 claim that Swiss-German is non-context-free <REF/>.  Bresnan, et  al., <REF/> have pointed out that analyses such as these form a non-recognizable set.  Consequently, it cannot be possible to capture this
account within 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and, in fact, the definition we give fails to license
these structures.  Examining why this is the case provides some insight into
the kinds of natural properties of linguistic structures that correspond to
increased language-theoretic complexity.
</P>
<P>
In order to rule out the possibility of ``forking'' chains--of some nodes
participating in the licensing of multiple gaps--we have required chains to be
maximal in the sense that they include every node that is related by link to
any node in the chain.  Consequently, we can license overlapping chains only if
they are distinguished in some way.  The account works for English because we
can 
classify chains in English into a bounded set of types in such a way that no
two chains of the same type ever cross.  (This fact depends to a great
extent on the minimality requirement in the antecedent-government relation.)
This property can be stated as a principle:
The number of chains which overlap at any single position in the
tree is bounded by a constant.
Our approach to chains will work for any account of language that satisfies
this principle.  Once again, the linguistics literature provides arguments that
such bounds exist, at least in some cases.  As we have already noted, Manzini's
 Locality Theory <REF/> implies that no more than two  A-chains ever overlap.  Stabler <REF/> makes the stronger claim that such bounds exist for all linguistically relevant relationships in all
languages.  
</P>
<P>
Leaving aside the possibility that it may be possible to account for
cross-serial dependencies in Dutch in other ways, we can note that accounts
 employing structures such as the one in Figure <CREF/> fail to meet the bound on overlapping chains.  This is despite the fact that, if one orders
the movements bottom-up, each movement meets the strictest conceivable locality
constraint--each head moves to the closest possible position (often stated as
the Head Movement Constraint).  The problem is that, even if the
movements are ordered in this way, each movement carries the target
positions of the prior movements along with it.  Thus, in the final structure
all chains of 
head-movement overlap.  Given that the number of heads participating in these
structures is arbitrary, there can be no a priori bound on the number of
overlapping chains.  Note that in the example the two helpen chains
(
<!-- MATH: $[\f{V}_3,t_3]$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and 
<!-- MATH: $[\f{V}_5,t_5]$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 )
are indistinguishable.  Any attempt to
form a chain including any of these nodes will be required to include all four
and the result will not be linearly ordered.
</P>
</DIV>
</DIV>
<DIV ID="6" DEPTH="1" R-NO="6"><HEADER>  Conclusion
</HEADER>
<P>
In this paper we have 
introduced a kind of descriptive complexity result for the
strongly Context-Free Languages--a language is strongly context-free iff
the set of trees analyzing the syntax of its strings is definable in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 (modulo a projection).  Using this result we have sketched a couple of language
complexity results relevant to GB, namely, that free-indexation cannot, in
general, be 
enforced by CFGs, and that a specific GB account of English licenses
a strongly context-free language.  The first of these results is not likely to
come as a surprise to the GB community.  The appropriateness of free-indexation
as a fundamental component in linguistic theories has been questioned in the
more recent GB literature on purely linguistic (rather than complexity
theoretic) grounds.
</P>
<P>
The second result is more surprising.  We don't expect it to extend to
the whole range of human languages, that is, to any theory of Universal
 Grammar.  Shieber <REF/> and  Miller <REF/> (to cite two examples) give fairly strong evidence that there are 
constructions that occur in human languages that are beyond the CFLs, and hence
not possible to capture in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
As expected, our definitions fail for these
constructions.  The fact that the
definition works for English is a consequence of the fact that, in the account
of English we capture, it is 
possible to classify chains into finitely many categories in such a way that
no two 
chains from a given category ever overlap. 
  GB-style analyses of the constructions studied by Shieber and by
Miller include positions in which an unbounded number of chains can overlap.
Our definition is unable to identify any well-formed
chains including these positions; indeed, there is unlikely to be any way to
distinguish these chains without the 
equivalent of unbounded indices.
</P>
<P>
As it stands, this result speaks only of the particular account of English we
capture.  The fact that this is context-free says nothing about the nature of
human language faculty, since the principle it depends upon is unlikely to be a
principle of Universal Grammar.  It does, however, raise the prospect of wider
results.  Extensions of our descriptive
complexity result to larger 
language complexity classes
could provide formal restrictions on the principles employed by GB theories
that would be sufficient to provide non-trivial generative capacity results for
those theories
without losing the ability to capture the full range of human language.  With
such extended characterizations
one might establish upper bounds on the complexity of human language in
general. 
The possibility that such results might be obtainable is suggested by the fact
that we find numerous cases in which the issues arising in our studies for
definability reasons, and ultimately for language complexity reasons, have
parallels that arise in the GB literature 
motivated by more purely linguistic
concerns.  This suggests that the regularities of human languages that are the
focus of the linguistic studies are perhaps reflections of properties of the
human language faculty that can be characterized, at least to some extent, by
language complexity classes.
</P>
<DIV ID="6.1" DEPTH="2" R-NO="1"><HEADER>Bibliography </HEADER>
<P>
Robert C. Berwick.
Strong generative capacity, weak generative capacity, and modern
  linguistic theories.
Computational Linguistics, 10:189-202, 1984.
</P>
<P>
Joan Bresnan, Ronald M. Kaplan, Stanley Peters, and Annie Zaenen.
Cross-serial dependencies in Dutch.
Linguistic Inquiry, 13:613-635, 1982.
</P>
<P>
Michael Brody.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 -theory and arguments.
Linguistic Inquiry, 24(1):1-23, 1993.
</P>
<P>
Robert C. Berwick and Amy S. Weinberg.
The Grammatical Basis of Linguistic Performance.
MIT Press, 1984.
</P>
<P>
Noam Chomsky.
Barriers.
MIT Press, 1986.
</P>
<P>
Noam Chomsky.
A minimalist program for linguistic theory.
In The View from Building 20, pages 1-52. MIT Press, 1993.
</P>
<P>
John Doner.
Tree acceptors and some of their applications.
Journal of Computer and System Sciences, 4:406-451, 1970.
</P>
<P>
Ferenc Gcseg and Magnus Steinby.
Tree Automata.
Akadmiai Kiad, Budapest, 1984.
</P>
<P>
Mark Johnson.
The use of knowledge of language.
Journal of Psycholinguistic Research, 18(1):105-128, 1989.
</P>
<P>
Jan Koster.
Domains and Dynasties.
Foris Publications, Dordrecht, Holland, 1987.
</P>
<P>
Marcus Kracht.
Syntactic codes and grammar refinement.
Journal of Logic, Language, and Information, to appear.
</P>
<P>
Steven G. Lapointe.
Recursiveness and deletion.
Linguistic Analysis, 3(3):227-265, 1977.
</P>
<P>
Harry R. Lewis.
Unsolvable Classes of Quantificational Formulas.
Addison-Wesley, 1979.
</P>
<P>
Maria Rita Manzini.
Locality: A Theory and Some of Its Empirical Consequences.
MIT Press, Cambridge, Ma, 1992.
</P>
<P>
Philip H. Miller.
Scandinavian extraction phenomena revisited: Weak and strong
  generative capacity.
Linguistics and Philosophy, 14:101-113, 1991.
</P>
<P>
Geoffrey K. Pullum and Gerald Gazdar.
Natural language and context-free languages.
Linguistics and Philosophy, 4:471-504, 1982.
</P>
<P>
Geoffery K. Pullum.
On two recent attempts to show that English is not a CFL.
Computational Linguistics, 10:182-188, 1984.
</P>
<P>
Michael O. Rabin.
Decidability of second-order theories and automata on infinite trees.
Transactions of the American Mathematical Society, 141:1-35,
  July 1969.
</P>
<P>
Luigi Rizzi.
Relativized Minimality.
MIT Press, 1990.
</P>
<P>
James Rogers.
Studies in the Logic of Trees with Applications to Grammar
  Formalisms.
Ph.D. dissertation, Univ. of Delaware, 1994.
</P>
<P>
Stuart M. Shieber.
Evidence against the context-freeness of natural language.
Linguistics and Philosophy, 8:333-343, 1985.
</P>
<P>
Edward P. Stabler, Jr.
The Logical Approach to Syntax.
Bradford, 1992.
</P>
<P>
Edward P. Stabler.
The finite connectivity of linguistic structure.
In C. Clifton, L. Frazier, and K. Rayner, editors, Perspectives
  on Sentence Processing, chapter 13, pages 303-336. Lawrence Erlbaum,
  Hillsdale, NJ, 1994.
</P>
<P>
J. W. Thatcher.
Characterizing derivation trees of context-free grammars through a
  generalization of finite automata theory.
Journal of Computer and System Sciences, 1:317-322, 1967.
</P>
<DIV ID="6.1.1" DEPTH="3" R-NO="1"><HEADER>Footnotes</HEADER>
<P>
  To Appear:
Specifying Syntactic Structures (papers from the Logic, Structures, and Syntax
workshop, Amsterdam, Sept. 1994)
  Or, following
a strictly derivational approach, the set of all structures consisting of a
triple of finite trees along with a representation of PF.
  We will usually dispense with the dot and denote
concatenation by juxtaposition.
  A partial
 axiomatization of this class of models is given in <REF/>. 
  A more complete
 proof is given in <REF/>. 
  We
will assume, for simplicity, that only set variables occur free.  Since
individual variables can be re-interpreted as variables ranging over
singleton sets, this is without loss of generality.
  This
 proof is evidently originally due to Thatcher <REF/>.  In addition,  Theorem <CREF/> is implicit in the proof of a related theorem due to  Doner <REF/>. 
  Since the property of being an
equivalence relation--being reflexive, symmetric, and transitive--is definable
in 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
our result is one way of showing that 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
augmented with a
single arbitrary binary relation has a non-decidable monadic second-order
theory.
  While we don't treat Logical Form,
there is no reason this cannot be incorporated into our structures in
much the same way.
  It is
 interesting that Johnson, in <REF/> initially defines all four levels of structure, but then, through a series of standard
program transformations, optimizes away everything except PF and LF.
</P>
</DIV>
</DIV>
</DIV>
</BODY>
</MINIMAL-DOC>
