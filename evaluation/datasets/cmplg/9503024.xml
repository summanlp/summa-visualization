<?xml version='1.0'?>
<!DOCTYPE MINIMAL-DOC SYSTEM "mini.dtd">
<MINIMAL-DOC>
<TITLE>From compositional to systematic semantics</TITLE>
<ABSTRACT>
<P>
We prove a theorem stating that any semantics can be encoded as a
compositional semantics, which means that, essentially, the standard
definition of compositionality is formally vacuous. We then show that when
compositional semantics is required to be "systematic"
(that is, the meaning
function cannot be arbitrary, but must belong to some class),
it is possible to
distinguish between compositional and non-compositional semantics.
As a result, we believe that the paper clarifies the
concept of compositionality and opens a possibility of making systematic
formal comparisons of different systems of grammars.
</P>
</ABSTRACT>
<BODY>
<DIV ID="1" DEPTH="1" R-NO="1"><HEADER>  Introduction </HEADER>
<P>
Compositionality is defined as the property that the
meaning of a whole is a function of the meaning of its parts (cf. e.g.
 <REF/>, pp.24-25). (A slightly less general definition,  e.g. <REF/>, postulates the existence of a homomorphism from syntax to semantics). However,
we can prove a theorem stating that any semantics can be encoded as a
compositional semantics, which means that, essentially, the standard
definition of compositionality is formally vacuous.
Thus, although intuitively clear, the definition is not restrictive
enough. We illustrate the power of the theorem by showing how
to assign compositional semantics to idioms and to a very
counterintuitive semantics of coordination (Section 4).
</P>
<P>
Given a class of functions F, we say that the
compositional semantics is systematic  if the
meaning function belongs to the class F. We show that when
compositional semantics is required to be systematic
we can distinguish between grammars with
compositional and non-compositional semantics;
we present an example of a simple grammar for which there is no
"systematic" compositional semantics (Section 3).
</P>
<P>
As a result, we believe that the paper clarifies the
concept of compositionality and opens a possibility of making systematic
comparisons of different systems of grammars and
natural language understanding programs.
Furthermore, the concept of systematicity that we introduce in
the paper might be useful in extracting the formal meaning behind
various versions of compositionality as a philosophical principle,
 cf.<REF/>. But in this paper we restrict ourselves to the mathematics of
compositionality.
</P>
<P>
Compositional semantics is usually defined  as  a functional
dependence of the meaning of an expression on the meanings of its  parts.
One   of  the  first  natural  questions  we  might want to ask is whether a
set of natural language expressions, i.e. a language,
can have some compositional semantics. This question has
been answered positively by
 <REF/>. However his result says nothing about what kinds of things should be assigned e.g. to nouns, where,
obviously,
we would like nouns to be mapped into sets of entities, or something like
that. That is, we want semantics to encode some basic
intuitions, e.g. that
nouns denote sets of entities, and verbs denote relations between entities,
and so on; in other words, we would like to have
a compositional semantics that agrees with intuitions.
More formally, the questions is whether after deciding what
sentences and
their parts mean, we can find a function that would compose the meaning  of a
whole from the meanings of its parts.
</P>
<P>
The answer to this question is somewhat disturbing. It turns out  that
whatever we decide that some language expressions should mean, it is always
possible to produce a function that would give compositional semantics
to it (see below for a more
precise formulation of this fact). The upshot is that compositionality, as
commonly defined, is not a strong constraint on a semantic theory.
</P>
</DIV>
<DIV ID="2" DEPTH="1" R-NO="2"><HEADER>  Proving the existence of compositional semantics </HEADER>
<P>
Let S be  any collection of expressions (intuitively, sentences and their
parts). Assume that elements of S (e.g. s.t)
are composed from other elements of
S (that is, s and t)  by concatenation (".").
We do not assume that concatenation is associative, that is
<!-- MATH: $(a.(b.c)) = ((a.b).c)$ -->
(a.(b.c)) = ((a.b).c). Intuitively, this means that we assign
semantics to parse trees,  and not to strings of words.
</P>
<P>
Let M be a set  of meanings, and let for any 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
<!-- MATH: $m(s) \in
M$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
be the meaning of s. We want to show that there
is  a compositional semantics for S which agrees with the function massociating s with m(s).
</P>
<P>
Since elements of Mcan be of any type, we do not automatically have
<!-- MATH: $m(s.t)  =  m(s)\sharp m(t)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
where 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is some
operation on the meanings. To
get that kind of homomorphism we have to perform a type raising operation
that would map elements of Sinto functions  and  then the functions into the
required meanings.
Note that such a type raising operation is quite common
both in mathematics  (e.g. 1 being a function equal to 1 for all
values) and in mathematical linguistics.
The  meaning function 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that we want to define will provide
compositional semantics for S by mapping it into
a set of functions in such a way that 
<!-- MATH: $\mu(s.t) = \mu(s) ( \mu(t) )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
for all elements s.t of S.
</P>
<P>
Secondly, we want that the original semantics be
easily decoded from  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
There is more than one way of
doing this.
One can trivially extend the language S by adding
to   it an "end of expression" character 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
which may appear only as the
last element of any expression. The purpose of it is to encode the function
m(x) in the following way:
<!-- MATH: $\mu(s.\$) =  m(s)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
for all s in S. Intuitively, the
character 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is like the period at the end of a sentence, or the pause
marking the end of an utterance. In effect, we will be treating all
sentences as idioms, or garden path sentences, where the meanings are
clear only once the sentence is completed (Theorem 2).
But, as we are going to show now, the original
semantics can be encoded in a different way, without extending the original
language, e.g. by assuring 
<!-- MATH: $\mu(s)(s) =  m(s)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
for all s in S (Theorem 1).
</P>
<P>
To make the notation simple, we have assumed that there is only one
way of composing elements  of S, by concatenation,
but  all our arguments work for languages with many operators as well.
We show an example of how such operators can be handled in Section 4.
</P>
<P>
 THEOREM 1.
Let M be an arbitrary set.
Let A be an arbitrary alphabet. Let "." be a binary operation,
and let S be the set closure of A under ".". Let 
<!-- MATH: $m : S
\rightarrow M$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
be an arbitrary function.
</P>
<P>
Then there is a set of functions M[*] and a unique
map 
<!-- MATH: $\mu : S \rightarrow M^*$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
such that for all s, 
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
 COROLLARY 2.  Theorem 1 is also true when
the binary operation "." is partial.
 PRELIMINARIES TO THE PROOF: THE SOLUTION LEMMA 
Our results will be proved in set theory with the
anti-foundation axiom.
This set theory, ZFA,  is equiconsistent with
the standard system of ZFC, thus the theorem does not assume
anything more than what is needed for "standard mathematical
practice". Furthermore, ZFA is better suited as foundations for
semantics of natural language  than ZFC
 (<REF/>). 
</P>
<P>
We need only one (but fundamental) theorem of ZFA:
 the solution lemma (<REF/> and  <REF/>), which says any (well-formed) collection of equations that define sets
has a unique solution.
For the reader who is not familiar with set theory, the meaning of the
solution lemma can be explained as follows:
We have a universe of sets V, and
a set of variables 
<!-- MATH: $X= \{  x_1 , x_2 , ... \}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
which may be infinite (countable or uncountable). We can form equations
of the form
</P>
<IMAGE TYPE="FIGURE"/>
<P>
where 
<!-- MATH: $a\_set(X,V)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a set expression involving the variables and
elements of V, for instance, if 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
we can write the following equations:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
We say that such a set is well-formed if each variable appear only
once on the left, and each left hand side is a variable.
The solution lemma says that any set of such equations (finite or
infinite) has a unique
solution. That is, there is a unique
collection of sets that satisfy them.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 PROOF OF THEOREM 1 AND COROLLARY 2 
</P>
<P>
Proof of Theorem 1.
It is enough to ensure that for all 
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
Clearly,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a function, because it is a collection of pairs.
The proof is complete once we check that
for  s's and   t's in S we have
(i)  
<!-- MATH: $\mu(s . t ) = \mu(s ) ( \mu(t ))$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
, and
(ii) 
<!-- MATH: $\mu(s)(s) = m(s )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
.
Using the above equality we check (i): If 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
then
<!-- MATH: $f(\mu(t )) = \mu(s . t )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Similarly for (ii).
</P>
<P>
It remains to show that using the solution lemma we can make the
above equation true for all 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We begin by
introducing a set variable
Xs for every  
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and observing that
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a well-formed set equation for any 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
(The pair [ a , b ] is set theoretically defined as
<!-- MATH: $\{ \{ a \} , \{ a , b \} \})$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Hence the solution
lemma applies, and there are unique sets 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that satisfy
each equation. But each such 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is a collection of pairs, i.e.
a function. Furthermore, since each 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is unique, and S is
a set, the mapping 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
associating 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
with each 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is
a function. This completes the proof of Theorem 1. 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Proof of Corollary 2.
It is enough to observe that we can add an extra condition in the
main equation of Theorem 1, and the proof still works:
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
 NOTE. Notice that we can view using the solution lemma in the
above proofs as
an extreme example of defining a function by cases. To see it more
clearly, one can make the main equation of the proof of Theorem 1
explicit.
Let  
<!-- MATH: $t(0) , t(1) ,   \ldots  , t(\alpha)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 enumerate  S.
We can
create a big table specifying meaning values for all strings and  their
combinations.  Then the  conditions on the meaning functions 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 can be written as the set of equations below
</P>
<IMAGE TYPE="FIGURE"/>
<P>
>
</P>
<P>
In ordinary mathematics, this would correspond to saying that if
x is 1 then f(x)=32,
if x is 2 then 
<!-- MATH: $f(x)=14732$ -->
f(x)=14732,
if x is 3 then f(x)=1, and so on. Clearly, such a process
defines the
function f, but, intuitively, it is not a definition we would
care much for. Before showing that requiring a better description
of an f than as a set of pairs makes sense, we want to observe that
the encoding of the original meaning function
can be uniform in the following sense: 
</P>
<P>
 PROPOSITION 3.
In addition to the assumptions of Theorem 1, let 
<!-- MATH: $\$ \notin  A$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and
let 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
be the language obtained by the mapping
<!-- MATH: $s \rightarrow s.\$$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
for all 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Then there is a set of functions M[*] and a unique
map 
<!-- MATH: $\mu : S\$ \rightarrow M^*$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
such that for all s, 
</P>
<IMAGE TYPE="FIGURE"/>
<IMAGE TYPE="FIGURE"/>
<P>
Proof. As in the proof of
Corollary 2, we can change the set of equations to
</P>
<IMAGE TYPE="FIGURE"/>
<P>
To finish the construction of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
we make sure that the equation
<!-- MATH: $\mu(\$) = \$$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 holds. Formally, this requires adding the pair 
<!-- MATH: $< \$ , \$ >$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
into
the graph of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that was obtained from the solution lemma.
Also, we have to extend the domain of function 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
to include
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
This is easily done by adding to the already constructed
part of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
the set of pairs
<!-- MATH: $\{ < s.\$ , m(s) > : s \in S \}$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
The proof is complete once we check that for  s's  in S we have
<!-- MATH: $\mu(s . \$ ) = m(s )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and that
<!-- MATH: $\mu(s . \$ ) = \mu( s ) ( \mu(\$) )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 (because 
<!-- MATH: $\mu ( \$ ) = \$$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and, according to the equation,
<!-- MATH: $\mu ( s ) ( \$ ) = m (s )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ). 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
Note that, as in Corollary 2,
if a certain string does not belong to the language, we can assume that
the corresponding value in this table is undefined; thus 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is not
necessarily defined for all possible concatenations of strings of
S.
</P>
<P>
In view of the above theorems, any semantics is equivalent
to a compositional
semantics, and hence it would be meaningless to keep the definition of
compositionality as the existence of a homomorphism from syntax to semantics
without imposing some conditions on this homomorphism.
Notice that requiring
the computability of the meaning function will not do.
In mathematics, where semantics obviously is
compositional, we can talk about noncomputable functions, and it
is usually clear what we postulate about them. Also, we have the
following proposition. 
</P>
<P>
 PROPOSITION 4.
If the set of expressions S and the original meaning
function m(x) are computable, then so is the meaning
function 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
</P>
<P>
Proof. One can easily check that
the table defining the meaning functions 
<!-- MATH: $\mu(t(\alpha))$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 in the note above is effectively
computable from the functions m(x) and 
<!-- MATH: $t( \alpha )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Hence
so is the function 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 NOTE. What does it mean that the table is effectively
computable from the functions m(x) and 
<!-- MATH: $t( \alpha )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ?
It means that given a Turing machine, T1, that prints all
elements of S, and another Turing machine, T2, that takes
an element s on the output tape of T1 as input and produces
as output m(s), we can construct a third Turing machine, T3, that
produces the successive elements of the table, i.e. enumerates all the
equations (e.g. for any pair [m, n] gives the nth value pair of the
mth equation).
But these equations define our function 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Hence
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is effectively computable.
Also, notice that the proposition holds true for generalized
 computability, in the sense of <REF/>. 
</P>
</DIV>
<DIV ID="3" DEPTH="1" R-NO="3"><HEADER>  Systematic semantics. I. Examples </HEADER>
<P>
  <REF/>, pp.27, talks about compositionality, postulating that the meaning of a whole should
be a "systematic" function of the
meanings of the parts. He does not define the word
"systematic" except as being an antonym to "idiosyncratic".
What it could mean is that we want to avoid such meaning functions
as the ones defined in
the proof of Theorem 1 and the subsequent propositions.
We suggest a simple way of doing it --
by requiring that the meaning function belong to a certain class.
By Proposition 4 this does not work if we merely postulate that
the function be computable. However it does work for
smaller classes of functions as the following examples show.
 A SIMPLE GRAMMAR WITHOUT A SYSTEMATIC SEMANTICS 
If meanings have to be expressed using certain natural,
but restricted, sets
of operations, it may turn out that even simple grammars do not have a
compositional semantics.
Consider two grammars of numerals in base 10: 
</P>
<P>
Grammar ND 
    N 
<!-- MATH: $\leftarrow$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
N D 
    N 
<!-- MATH: $\leftarrow$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
D  
    D 
<!-- MATH: $\leftarrow$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
0 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
1 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
2 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
3 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
4 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
5 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 6 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
7 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
8 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
9 
</P>
<P>
Grammar DN   
    N 
<!-- MATH: $\leftarrow$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
D N 
    N 
<!-- MATH: $\leftarrow$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
D 
    D 
<!-- MATH: $\leftarrow$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
0 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
1 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
2 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
3 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
4 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
5 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 6 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
7 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
8 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
9 
</P>
<P>
 PROPOSITION 5.
For the grammar ND, the meaning of any numeral can be expressed in the model
<!-- MATH: $( Nat, + , \ast , 10 )$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
as
</P>
<IMAGE TYPE="FIGURE"/>
<P>
that is, a polynomial
in two variables with coefficients in natural numbers.
</P>
<IMAGE TYPE="FIGURE"/>
<P>
On the other hand, for the grammar DN,
we can prove that no such a polynomial exists:
</P>
<P>
 THEOREM 6.
There is no polynomial p in two variables x, y such that
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and such that the value of 
<!-- MATH: $\mu ( D  N
)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
is the number expressed by the string  D  N in base 10.
</P>
<P>
Proof.
We are looking for
</P>
<IMAGE TYPE="FIGURE"/>
<P>
where the function pmust be a polynomial in these two variables.
If such a polynomial exists, it would have to be equal to
</P>
<IMAGE TYPE="FIGURE"/>
<P>
for 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in the interval 0..9, and to
<!-- MATH: $\mu(D) \ast 10 + \mu(N)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 for 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in 10..99, and to 
<!-- MATH: $\mu(D) \ast 100 + \mu(N)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
for
</P>
<IMAGE TYPE="FIGURE"/>
<P>
in 100..999, and
so on. Let the degree of this polynomial be less
than n , for
some n. Let us consider the interval
<!-- MATH: $10^{n} .. 10^{(n+1)}- 1$ -->
10[n] .. 10[(n+1)]- 1. On this interval the polynomial
would have to be equal identically
to 
<!-- MATH: $\mu(D) \ast 10 ^{ n}+ \mu(N)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Now, if two polynomials of degrees less than n agree on
n different values, they must be identical. Hence,
<!-- MATH: $p (\mu(N), \mu(D)) = \mu(D) \ast 10 ^{ n}+ \mu(N)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
But this
would give wrong values for other intervals, e.g 10..99.
Contradiction. 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
But notice that there is a
compositional semantics  for the grammar DN that
does not agree with intuitions:
<!-- MATH: $\mu( D  N ) = 10 \ast \mu(N) + \mu(D)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
which
corresponds to reading the number backwards. And there are
many other semantics
corresponding to all possible polynomials in
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
Also observe that (a) if we specify enough values of the meaning function we
can exclude any particular polynomial; (b) if we do not restrict the degree
of the polynomial, we can write one that would give any values we want on a
finite number of words in the grammar.
The moral is that not only it is natural to restrict meaning functions to,
say, polynomials, but to further restrict them.
E.g. if we restrict the meaning functions to polynomials of degree 1,
then by specifying only three values  of the meaning function we can (a)
have a unique compositional semantics for the first grammar; and
(b) show that
there is no compositional semantics for the second grammar
(directly from the
proof of the above theorem).
</P>
</DIV>
<DIV ID="4" DEPTH="1" R-NO="4"><HEADER>  Some linguistic examples </HEADER>
<P>
In this section we want to explain how the theorems we proved in
Section 2
apply to typical linguistic examples. In the process
we also explain how languages with operators can be handled by our
method. We begin by discussing the simple case of idioms.
</P>
<P>
 IDIOMS.
Intuitively, the meaning of "high seas" is not compositional,
because "high" refers to length or distance, and not to open
spaces; moreover one could even argue that although "seas" is
plural, "high seas" is semantically singular, for it means
an "open sea". (However, the precise semantics of the expression
is not important at this
point). We want to show how we can assign compositional semantics
to such non-compositional examples.
</P>
<P>
Let the language S consist of 
<!-- MATH: $wall, seas,  high$ -->
wall, seas,  high,
high.wall, and high.seas. The equations we need to ensure
the compositionality of semantics have the familiar form:
</P>
<IMAGE TYPE="FIGURE"/>
<P>
>
</P>
<P>
Notice, that we could add building and other words
to the language and easily
extend this set of equations. The intuition we associate with
compositionality would be captured by the uniformity of
the meanings of high.X as 
<!-- MATH: $high(m(X))$ -->
high(m(X)), where X ranges over
<!-- MATH: $wall, building, ...$ -->
wall, building, ....
However the formal expression
of this intuition as the principle of compositionality does not work,
which can be seen by noticing that the meaning of high.seas is
a composition of the meaning functions for high and seas.
What is happening has to do with the fact that, by
definition, functions defined by cases are as good as any others.
And what we have done is to have defined the meaning of high by
cases.
</P>
<P>
 COORDINATION.
We now turn our attention to a slightly more complicated example.
Consider disjunction and conjunction, +and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
We plan to prove the following results:
</P>
<P>
 PROPOSITION 7. Let + and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
denote "or" and "and".
Then: 
  (A). It is possible to assign compositionally the "natural"
semantics of 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 to expressions of type 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and preserve the original
meanings of a+b and 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
  (B). (A) is not possible if the meaning functions have to be Boolean
polynomials. 
Proof.
To keep things as simple as possible, consider language S consisting of
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
b, c, and a. To apply directly the
solution lemma we should represent the operators in their prefix form;
e.g. a+b becomes +.a.b. Then our language S becomes
<!-- MATH: $+.a.\.b.c$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
+, 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
b, c, and a. (And for the sake
of completeness we can add to it 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 ,
and b.c).
As before we write our equations (this time using the version
with $):
</P>
<IMAGE TYPE="FIGURE"/>
<P>
>
</P>
<P>
It can be easily checked that as before 
<!-- MATH: $\mu(s.t)=\mu(s)(\mu(t))$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
for all
<!-- MATH: $s , t \in S$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
The meaning m(a) of a is arbitrary, but we
would typically identify it with its logical value (true or false).
Also, notice that without loss of generality those  a, b, c(and hence the language S)
can be "expanded" to sets of variables
<!-- MATH: $a_i , b_j , c_k$ -->
ai , bj , ck, resulting in a slight change in the equations,
but not changing the content of the theorem. Then,
for all pairs of variables disjunction and conjunction would behave as
usual; however for a combination of a variable with a
Boolean formula they could behave arbitrarily.
This proves the first part.
</P>
<P>
We now prove the second part, i.e. that if the meaning functions are
restricted to Boolean polynomials, it is impossible to assign
compositional semantics given by the "natural" semantics of
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 to expressions of the type 
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and preserve the original
meanings of the connectives. (The "natural" semantics is of
course the conjunction of the Boolean value of a + b with the
Boolean value of c).
</P>
<P>
The proof consists in observing that
<!-- MATH: $(m(a) + m(b)) \  m(c)$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
cannot be obtained as a Boolean polynomial
(function) 
<!-- MATH: $p( m(a) , m(b)\m(c))$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
 .
To see it, it is enough to
construct a truth table showing the values of 
<!-- MATH: $(a+b) \ c$ -->
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and
</P>
<IMAGE TYPE="FIGURE"/>
<P>
and observing that there cannot be a functional
dependence of the former on the latter and the value of a(compare the values for the triples 
<!-- MATH: $< a=1 , b =0 , c=1 >$ -->
[ a=1 , b =0 , c=1 ] and
<!-- MATH: $< a=1 , b =1 , c=0 >$ -->
[ a=1 , b =1 , c=0 ]).
</P>
<IMAGE TYPE="FIGURE"/>
</DIV>
<DIV ID="5" DEPTH="1" R-NO="5"><HEADER>  Systematic semantics. II. Discussion </HEADER>
<P>
Above, we have argued that the existence of a homomorphism from
syntax to semantics does not restrict the grammar, but if we put
some constraints on such a homomorphism, they actually might restrict
grammars of languages. We have called such
homomorphisms (F-)systematic. However the nature of systematicity
seems to be very much an open problem. In this section we discuss
some of the most obvious issues, and propose some research
possibilities in this area.
</P>
<P>
The first
natural question that arises is: What should be this class F? We have
shown that for a grammar of numbers, and a grammar of two Boolean
connectives the natural classes are polynomials. Clearly, this cannot
always be the case. For instance, it seems natural to map verbs
into predicates and nouns into their arguments. But we know that
if we want to provide compositional semantics for more than the
simplest case of subject-verb-object construction we need other
mechanism, e.g. type raising. On the other hand, unrestricted type
raising leads to the results we have just discussed. We arrive then
at the following variant of the natural question: How should we restrict
type raising? (So that we can account e.g. for ellipsis, but at the
same time constrain the grammars).
</P>
<P>
Many grammatical constructions express meanings that go beyond
expressing predicate-arguments assignments. For example, "the X-er,
the Y-er" construction
 (<REF/>, <REF/>), as in "the more you dive, the better you swim", expresses a proportional dependence.
Other constructions can express speech acts, various kind of conflict,
etc., hence creating rich sets of meanings. The next question we
can ask is whether we can express constraints on the syntax-semantics
interface by saying that the homomorphism should be simple,
relative to the the class of
meaning functions. A mathematical analogy would be
to say that we are given
many different functions, e.g. 
<!-- MATH: $sin(x), cos(x), e^x, ...$ -->
sin(x), cos(x), e[x], ...,  but
only simple ways of composing them, e.g. only by the +.
</P>
<P>
In essence, when we say that a function is F-systematic, we view
F as a measure of complexity and/or expressive power.
Hence the natural association with polynomials. But there are other
measures of complexity.
 <REF/> discusses grammars and languages in terms of Kolmogoroff complexity, suggesting that more compact grammars
are better even if they overgeneralize (e.g. by approximating
a finite language by an infinite one).
We believe that his work is relevant for systematicity, but
we do not have any formal argument supporting that claim, except
the observation that polynomials are more compact than functions
defined by cases. So perhaps this might be a beginning of a formal
connection.
</P>
<P>
One of the referees has pointed out two other ideas. First,
there could be other more natural notions of systematicity, in cases
when meanings are specified
by means of constraint solving, as is implicit in unification-based
formalisms, and even in Theorem 1, where meanings are extracted as
solutions to equations. The second idea,
the differences between natural and formal languages notwithstanding,
is a programming language semantics
concept which actually restricts the ability of a semantics to be
compositional. This concept is "full abstraction", i.e. the
equivalence between the operational and denotational semantics,
 (cf. <REF/>), which can be viewed as a general constraint on compositionality.
</P>
<P>
A radically different approach to the interaction of syntax and semantics
 has been presented in <REF/> and <REF/>. Language is modeled there as a set of constructions (cf. also
 <REF/> and <REF/>). In that model there is no separate syntax, since constructions encode both
form and meaning. Each construction
explicitly defines the meaning function taking the meanings of
its subconstructions as arguments.
Intuitively, in that model we assume that
each word sense requires a separate semantic description;
the same is true about each
 idiom, open idiom (<REF/>), and a phrasal construction. This means that we make each
semantic function as complicated as linguistically
necessary, but their mode of combination is restricted.
(Continuing the above mathematical analogy, we would say that
the only mode of combination is substitution for an argument).
In the construction-based model semantics is "compositional"
and "systematic" (with respect to the set of all these
meaning functions), but there is no homomorphism from syntax
to semantics, because there is no syntax to speak of.
It is "compositional", because the meaning of a construction is
a function of the meanings of its parts and their mode of
combination. (Note that such a function
is different for different constructions, and each construction
defines its own mode of combination). And it is systematic, because
the modes of combination are not arbitrary, as they have to be
linguistically justified.
But since only few formal aspects
of constructions have been worked out, we can only speak of that model
as of yet another possibility.
</P>
<P>
The last point we want to make is that while it is true that the
homomorphism condition is too weak (in general) to count as
systematicity, the semanticists (e.g. Montague)
have not been using arbitrary homomorphisms. Thus a careful
examination of their work should lead to some characterization
of "good" homomorphisms; and this seems to be another interesting
avenue of research. (As suggested by one of the referees, a technique
from universal algebra might also prove helpful,
where one first gives a class
of algebras and then specifies meanings as homomorphisms
from the initial algebra of the class).
</P>
</DIV>
<DIV ID="6" DEPTH="1" R-NO="6"><HEADER>  Conclusions </HEADER>
<P>
In this paper we have shown the formal vacuity of the compositionality
principle. That is, we have shown that the property that the meaning
of the whole is a function of the meanings of its parts does not put any
material constraints on syntax or semantics.
Theorem 1 (and its corollaries) explain formally why
the postulate of a
homomorphism between syntax and semantics is not restrictive
enough: the syntactic operator of concatenation
"." can be mapped into functional
composition operating on functions that encode arbitrary semantics
of an arbitrary language.
As we have seen in the examples, the
presence of other operators does not change the result, because
they can be treated as yet another letter of the alphabet, and
one can still produce a homomorphism between syntax and semantics.
</P>
<P>
The problem of the vacuity of compositional semantics
arises, because in the formal definition of compositionality
meaning functions can be completely arbitrary. Therefore we have
proposed that the meaning functions should be systematic, i.e.,
non-arbitrary.
We have shown that this notion makes sense formally; that is,
we presented examples
of semantic classes of functions, for which there are grammars with
meaning functions in that class, as well as we have shown that there
are grammars that cannot have a meaning function in that class.
As we noted in the previous section, both
the formal and the linguistic nature of systematicity
remains an open problem, but with a few promising avenues of research.
</P>
<P>
In this paper we have restricted ourselves to the mathematics of
compositionality, and many important issues have not been discussed.
For instance, the main result is relevant for
theories of grammar and for the thesis about
the reduction of syntax to lexical meanings (cf. e.g.
 T. Wasow on pp.204-205 in <REF/>). Also, systematicity of semantics should give us a handle in
constraining the power of the semantic as well as the syntactic
 components of a grammar (cf. <REF/>). Furthermore, our results have implications for
computational linguistics (they are briefly discussed in
 <REF/>). 
</P>
<P>
Finally, the reader should note that one of the more bizarre
consequences of Theorem 1 is that we do not have to start building
compositional semantics for natural language beginning
with assigning of the meanings to words. We can do equally well by
assigning meanings to phonems  or even LETTERS, assuring
that, for any sentence, the intuitive meaning we associate with
it would be a function of the meanings of
the letters from which that sentence is composed. But then
the cabalists had always known it.
</P>
<P>
 ACKNOWLEDGEMENTS. I would like to thank Alexis Manaster
Ramer for our many
discussions of compositionality, and the referees for their
insightful comments.
</P>
<DIV ID="6.1" DEPTH="2" R-NO="1"><HEADER>Bibliography </HEADER>
<P>
P. Aczel.
Lectures on Non-Well-Founded Sets.
CSLI Lecture Notes, Stanford, CA, 1987.
</P>
<P>
Jon Barwise and John Etchemendy.
The Liar.
Oxford University Press, New York, NY, 1987.
</P>
<P>
Jon Barwise.
Admissible Sets and Structures.
Springer, New York, NY, 1975.
</P>
<P>
L. Bloomfield.
Language.
The University of Chicago Press, Chicago, IL, 1933.
</P>
<P>
Charles J. Fillmore, Paul Kay, and Mary Catherine O'Connor.
Regularity and idiomaticity in grammatical constructions.
Language, 64(3):501-538, 1988.
</P>
<P>
C.A. Gunter.
Semantics of Programming Languages.
MIT Press, Cambridge, MA, 1993.
</P>
<P>
Graeme Hirst.
Semantic interpretation and the resolution of ambiguity.
Cambridge University Press, Cambridge, Great Britain, 1987.
</P>
<P>
D. Jurafsky.
An On-line Computational Model of Sentence Interpretation.
PhD thesis, University of California, Berkeley, 1992.
Report No. UCB/CSD 92/676.
</P>
<P>
Edward L. Keenan and Leonard M. Faltz.
Boolean Semantics for Natural Language.
D Reidel, Dordrecht, Holland, 1985.
</P>
<P>
Alexis Manaster-Ramer and Wlodek Zadrozny.
Systematic semantics.
in preparation, 1994.
</P>
<P>
Barbara H. Partee, Alice ter Meulen, and Robert E. Wall.
Mathematical Methods in Lingusitics.
Kluwer, Dordrecht, The Netherlands, 1990.
</P>
<P>
Barbara H. Partee.
The logic of semantics.
In Fred Lanman and Frank Veltman, editors, Varieties of Formal
  Semantics, pages 281-312. Foris, Dordrecht, Holland, 1982.
</P>
<P>
Walter J. Savitch.
Why it might pay to assume that languages are infinite.
Annals of Mathematics and Artificial Intelligence,
  8(1,2):17-26, 1993.
</P>
<P>
P. Sells.
Lectures on Contemporary Syntactic Theories.
CSLI Lecture Notes (3), Stanford, CA, 1985.
</P>
<P>
Johan van Benthem.
The logic of semantics.
In Fred Lanman and Frank Veltman, editors, Varieties of Formal
  Semantics, pages 55-80. Foris, Dordrecht, Holland, 1982.
</P>
<P>
Wlodek Zadrozny and Alexis Manaster-Ramer.
The significance of constructions.
submitted to Computational Linguistics, 1994.
</P>
<P>
Wlodek Zadrozny.
On compositional semantics.
Proc. Coling'92, pages 260-266, 1992.
</P>
<DIV ID="6.1.1" DEPTH="3" R-NO="1"><HEADER>Footnotes</HEADER>
<P>
  Linguistics and Philosophy(17):329-342, 1994
</P>
</DIV>
</DIV>
</DIV>
</BODY>
</MINIMAL-DOC>
