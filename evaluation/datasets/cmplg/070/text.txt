
Generative grammar and formal language theory share a common origin in
a procedural notion of grammars: the grammar formalism provides
a general mechanism for recognizing or generating languages while the
grammar itself specializes that mechanism for a specific language.  At least
initially there was hope that this relationship would be informative
for linguistics, that by characterizing the natural languages in terms
of language-theoretic complexity one would gain insight into the
structural regularities of those languages.  Moreover, the fact
that language-theoretic complexity classes have dual automata-theoretic
characterizations offered the prospect that such results might provide abstract
models of the human language faculty, thereby  not just identifying these
regularities, but actually accounting for them.

Over time, the two disciplines have
gradually become estranged, principally due to 
a realization that the structural properties of languages
that characterize natural languages may well not be those that can be
distinguished by existing language-theoretic complexity classes.  Thus the
insights offered by formal language theory might actually be misleading in
guiding theories of syntax.  As a
result, the emphasis in generative grammar has turned from formalisms
with restricted generative capacity to those that
support more natural expression of the observed regularities of
languages.  While a variety of distinct approaches have developed, most
of them can be characterized as constraint based--the formalism
(or formal framework) provides a class of structures and a means of
precisely stating constraints on their form, the linguistic theory is
then expressed as a system of constraints (or principles) that characterize the
 class of well-formed analyses of the strings in the language. 

As the study of the formal properties of classes of structures defined in such
a way falls within domain of Model Theory, it's not surprising that
treatments of the meaning of these systems of constraints are typically couched
in terms of formal 
 logic ,,,,,,,,,. 

While this provides a model-theoretic interpretation of the systems of
constraints produced by these formalisms, those systems are
typically built by derivational processes that employ extra-logical mechanisms
to combine constraints.
More recently, it has become clear that in many cases these mechanisms can
be replaced with ordinary logical operations.  (See, for
instance: 
, , , , , , , ,
and, anticipating all of these, .) 
This approach abandons
the notions of grammar mechanism and derivation in favor of defining 
languages as classes of more or less ordinary mathematical structures
axiomatized by sets of more or less ordinary logical formulae.  A grammatical
theory expressed within such a framework is just 
the set of logical consequences of those axioms.  This step completes the
detachment of generative grammar from its procedural roots.  Grammars, in this
approach, are purely declarative definitions of a class of structures,
completely independent of mechanisms to generate or check them.
While it is unlikely that every theory of syntax with an explicit derivational
 component can be captured in this way, for those that can the logical re-interpretation frequently offers
a simplified statement of the theory and clarifies its consequences.

But the accompanying loss of language-theoretic complexity results is
unfortunate.  While such results may not be useful in guiding syntactic theory,
they are not irrelevant.  The nature of language-theoretic complexity
hierarchies is to classify languages on the basis of their structural
properties.  The languages in a class, for instance, will typically exhibit
certain closure properties (e.g., pumping lemmas) and the classes themselves
admit normal forms (e.g., representation theorems).  While the linguistic
significance of individual 
results of this sort is open to debate, they at least loosely parallel typical
linguistic concerns:  closure properties state regularities that are exhibited
by the languages in a class, normal forms express generalizations about their
structure.  So while these may not be the right results, they are not entirely
the wrong kind of results.  Moreover, since these classifications are
based on structural properties and the structural properties of natural
language can be studied more or less directly, there is a reasonable
expectation of finding empirical evidence falsifying a hypothesis about
language-theoretic complexity of natural languages if such evidence exists.

Finally, the fact that these complexity classes have automata-theoretic
characterizations means that results concerning the complexity of
natural languages will have implications for the nature of the human language
faculty.  These automata-theoretic characterizations determine, along one axis,
the types of resources required to generate or recognize the languages in a
class.  The regular languages, for instance, can be characterized by
finite-state (string) automata--these languages can be processed
using a fixed amount of memory.  The context-sensitive languages, on the other
had, can be characterized by linear-bounded automata--they can be
processed using 
an amount of memory proportional to the length of the input.  The context-free
languages are probably best characterized by finite-state tree
automata--these 
correspond to recognition by a collection of processes, each with a fixed
amount of memory,  where the number of processes is linear in the length of the
input and all communication between processes is completed at the time they are
spawned.  As a result, while these results do not necessarily offer abstract
models of the human language faculty (since the complexity results do not
claim to characterize the human languages, just to classify them), they do
offer lower bounds on certain abstract properties of that faculty.  In this
way, generative grammar in concert with formal language theory offers
insight into a deep aspect of human 
cognition--syntactic processing--on the basis of observable behavior--the
structural properties of human languages.

In this paper we discuss an approach to defining theories of syntax based on

  , a monadic second-order  language that has well-defined generative capacity: sets of finite trees are
definable within 


iff they are strongly context-free in a particular
sense.  While originally introduced as a means of establishing
language-theoretic complexity results for constraint-based theories, this
language has much to recommend it as a general framework for theories
of syntax in its own right.  Being a monadic second-order language it can
capture the (pure) modal languages of much of the existing model-theoretic
syntax literature directly; 
having a signature based on the traditional linguistic relations of domination,
immediate domination, linear precedence, etc. it can express most linguistic
principles transparently; and having a clear characterization in terms of
generative capacity, it serves to re-establish the close connection between
generative grammar and formal language theory that was lost in the move away
from phrase-structure grammars.  Thus,
with this framework we get both the advantages of the model-theoretic
approach with respect to naturalness and clarity in expressing linguistic
principles and the advantages of the grammar-based approach with respect to
language-theoretic complexity results.  

We look, in particular, at the
definitions of a single aspect of each of GPSG and GB.  The first of these,
Feature Specification Defaults in GPSG, are widely assumed
to have an inherently dynamic character.  In addition to being purely
declarative, our reformalization is considerably simplified wrt the definition
 in , and does not share its   misleading dynamic flavor.  We offer this as an example of 
how re-interpretations of this sort can inform the original theory.  In the
second example we sketch a definition of chains in GB.  This, again,
captures a 
presumably dynamic aspect of the original theory in a static way.  Here,
though, the main significance of the definition is that it forms a component of
a full-scale treatment of a GB theory of English S- and D-Structure within

.
This full definition establishes that the theory we capture licenses a
strongly context-free language.  More importantly, by examining the limitations
of this definition of 
chains, and in particular the way it fails for examples of non-context-free
constructions, we develop a characterization of the context-free languages that
is quite natural in the realm of GB.  This suggests that the apparent mismatch
between formal language theory and natural languages may well have more to do
with the unnaturalness of the traditional diagnostics than a lack of relevance
of the underlying structural properties.

Finally, while GB and GPSG are fundamentally distinct, even antagonistic,
approaches to syntax, their translation into the model-theoretic terms of


allows us to explore the similarities between the theories they express
as well as to delineate actual distinctions between them.  We look briefly
at two of these issues.

Together these examples are chosen to illustrate the main strengths of
the model-theoretic approach, at least as embodied in 

,
as a
framework for studying theories of syntax: a focus on structural
properties themselves, rather than on mechanisms for specifying them
or for generating or checking structures that exhibit them, and a
language that is expressive enough to state most linguistically
significant properties in a natural way, but which is restricted
enough to have well-defined strong generative capacity.



is the monadic second-order language over the signature including a set
of individual constants (K), a set of monadic predicates (P), and binary
predicates for immediate domination (

), domination (

), linear
precedence (

)
and equality (

).  The predicates in P can be
understood both as picking out particular subsets of the tree and as
(non-exclusive) labels or features decorating the tree.  Models for the
 language are labeled tree domains  with the natural interpretation of the binary predicates.  In  we
have shown that 
this language is equivalent in descriptive power to SS--the monadic
second-order theory of the complete infinitely branching tree--in the sense
that sets of trees are definable in SS iff they are definable in

.
This places it within a hierarchy of results relating
language-theoretic complexity classes to the descriptive complexity of their
models: the sets of strings definable in S1S are exactly the regular
 sets , the sets of finite trees definable in SnS, for finite n, are the recognizable sets (roughly the sets of derivation trees of
 CFGs) , and, it can be shown, the sets of finite trees definable in SS are those generated by generalized CFGs in which regular
expressions may occur on the rhs of rewrite
 rules .  Consequently, languages are definable in 


iff they are strongly context-free in the mildly generalized sense of
GPSG grammars.

In restricting ourselves to the language of 


we are restricting ourselves
to reasoning in terms of just the predicates of its signature.  We can expand
this by defining new predicates, even higher-order predicates that express, for
instance, properties of or relations between sets, and in doing so we can use
monadic predicates and individual constants freely since we can interpret these
as existentially bound variables.  But the fundamental restriction of 


is
that all predicates other than monadic first-order predicates must be
explicitly defined, that is, their definitions must resolve, via syntactic
substitution, into formulae involving only the signature of 

.

We now turn to our first application--the definition of Feature Specification
 Defaults (FSDs) in GPSG.  Since GPSG is presumed to license (roughly) context-free languages, we are not concerned here with establishing
language-theoretic complexity but rather with clarifying the linguistic theory
expressed by GPSG.  FSDs specify conditions on feature values that must hold at
a node in a licensed tree unless they are overridden by some other component of
the grammar; in particular, unless they are incompatible with either a feature
specified by the ID rule licensing the node (inherited features) or a
feature required by one of the agreement principles--the Foot Feature
Principle (FFP), Head Feature Convention (HFC), or Control Agreement Principle
(CAP).  It is the fact that the default holds just in case it is incompatible
with these other components that gives FSDs their dynamic flavor.  Note,
though, in contrast to typical applications of default logics, a GPSG grammar
is not an evolving theory.  The exceptions to the defaults are fully determined
when the grammar is written.  If
we ignore for the moment the effect of the agreement principles, the defaults
are roughly the converse of the ID rules: a non-default feature occurs iff it
is licensed by an ID rule.

It is easy to capture ID rules in 

.
For instance the rule:



can be expressed:



where 


holds iff the set of nodes that are
children of x are just the yi and  VPVP  VP , 

,
etc. are
 all members of P.  A sequence of nodes will satisfy 


iff they form a local tree that, in the
terminology of GKPS, is induced by the corresponding ID rule.  Using
such encodings we can define a predicate 


which is true at a
node x iff the feature f is compatible with the inherited features of x.

The agreement principles require pairs of nodes occurring in certain
configurations in local trees to agree on certain classes of features.  Thus
these principles do not introduce features into the trees, but rather propagate
features from one node to another, possibly in many steps.  Consequently, these
principles cannot override FSDs by themselves; rather every violation of a
default must be licensed by an inherited feature somewhere in the tree.  In
order to account for this propagation of features, the definition of FSDs in
GKPS is based on 
identifying pairs of nodes that co-vary wrt the relevant features in all
possible extensions of the given tree.  As a result, although the treatment in
GKPS is actually declarative, this fact is far from obvious.

Again, it is not difficult to define the configurations of local trees in which
nodes are required to agree by FFP, CAP, or HFC in 

.
Let the predicate


hold for a pair of nodes x and y iff they are
required to agree on f by one of these principles (and are, thus, in the
same local tree).  Note that 


is symmetric.  Following the
terminology of GKPS, we can identify the set of nodes that are prohibited 
from taking feature f by the combination of the ID rules, FFP, CAP, and HFC
as the set of nodes that are privileged wrt f.  This includes all
nodes that are not Free for f as well as any node connected to such a node by
a sequence of 


links.  We, in essence, define this
inductively.  


is true of a set iff it includes all nodes not Free
for f and is closed wrt 

.


is true of
the smallest such set.





There are two things to note about this definition.  First, in any tree there
is a unique set satisfying 


and this contains exactly those
nodes not Free for f or connected to such a node by 

.
Second, while this is a first-order inductive property, the definition is a
second-order explicit definition.  In fact, the second-order quantification of


allows us to capture any monadic first-order inductively or implicitly
definable property explicitly.

Armed with this definition, we can identify individuals that are privileged wrt
f simply as the members of 

 . 



One can define 


which holds whenever x is
required to take the feature f along similar lines.

These, then, let us capture FSDs.  For the default 
  [_-INV] 


, for instance, we get:



For 


(which says that 
  [_Bar 0] 


nodes are, by default, not marked passive), we get:



The key thing to note about this treatment of FSDs is its simplicity relative
to the treatment of GKPS.  The second-order quantification allows us to
reason directly in terms of the sequence of nodes extending from the privileged
node to the local tree that actually licenses the privilege.  The immediate
benefit is the fact that it is clear that the property of satisfying a set of
FSDs is a static property of labeled trees and does not depend on the
particular strategy employed in checking the tree for compliance.

The key issue in capturing GB theories within 


is the fact that the
mechanism of free-indexation is provably non-definable.  Thus definitions of
principles that necessarily employ free-indexation have no direct
interpretation in 


(hardly surprising, as we expect GB to be capable of
expressing non-context-free languages).  In many cases, though, references to
indices can be eliminated in favor of the underlying structural relationships
 they express. The most prominent example is the definition of the chains
formed by move-.
The fundamental problem here is identifying each
trace with its antecedent without referencing their index.  Accounts of the
licensing of traces that, in many cases of movement, replace co-indexation with
government relations have been offered by both 
and .  The key element of these accounts, from our point 
of view, is that the antecedent of a trace must be the closest
antecedent-governor of the appropriate type.  These relationships are easy to
capture in 

.
For 



Having interpretations both of GPSG and of a GB account of English
in 


provides a certain amount of insight into the distinctions
between these approaches.  For example, while the explanations of
filler-gap relationships in GB and GPSG are quite
dramatically dissimilar, when one focuses on the structures these
accounts license one finds some surprising parallels.  In the light of
our interpretation of antecedent-government, one can understand the
role of minimality in Rizzi's and Manzini's accounts as eliminating
ambiguity from the sequence of relations connecting the gap with its
filler.  In GPSG this connection is made by the sequence of agreement
relationships dictated by the Foot Feature Principle.  So while both
theories accomplish agreement between filler and gap through marking a
sequence of elements falling between them, the GB account marks
as few as possible while the GPSG account marks every node of the
spine of the tree spanning them.  In both cases, the complexity of the
set of licensed structures can be limited to be strongly context-free
iff the number of relationships that must be distinguished in a given
context can be bounded.

One finds a strong contrast, on the other hand, in the way in which GB
and GPSG encode language universals.  In GB it is presumed that
all principles are universal with the theory being specialized to
specific languages by a small set of finitely varying parameters.
These principles are simply properties of trees.  In terms of models,
one can understand GB to define a universal language--the set of all
analyses that can occur in human languages.  The principles then
distinguish particular sub-languages--the head-final or the pro-drop
languages, for instance.  Each realized human language is just the
intersection of the languages selected by the settings of its
parameters.  In GPSG, in contrast, many universals are, in
essence, closure properties that must be exhibited by human
languages--if the language includes trees in which a particular
configuration occurs then it includes variants of those trees in which
certain related configurations occur.  Both the ECPO principle and
the metarules can be understood in this way.  Thus while
universals in GB are properties of trees, in GPSG they tend to be
properties of sets of trees.  This makes a significant
difference in capturing these theories model-theoretically; in the GB
case one is defining sets of models, in the GPSG case one is defining
sets of sets of models.  It is not at all clear what the linguistic
significance of this distinction is; one particularly interesting question is
whether it has empirical consequences.  It is only from the
model-theoretic perspective that the question even arises.

We have illustrated a general formal framework for expressing theories of
syntax based on axiomatizing classes of models in 

.
This approach has a
number of strengths.  First, as should be clear from our brief explorations of
aspects of GPSG and GB, re-formalizations of existing theories 
within 


can offer a clarifying perspective on those theories, and, in
particular, on the consequences of individual components of those theories.
Secondly, the framework is purely declarative and focuses on those aspects of
language that are more or less directly observable--their structural
properties.  It allows us to reason about the consequences of a theory without
hypothesizing a specific mechanism implementing it.  The abstract properties of
the mechanisms that might implement those theories, however, are not beyond our
reach.  The key virtue of descriptive complexity results like the
characterizations of language-theoretic complexity classes discussed here and
the more typical characterizations of computational complexity
 classes , is that  they allow us to determine the complexity of checking properties independently
of how that checking is implemented.  Thus we can use such descriptive
complexity results to draw conclusions about those
abstract properties of such mechanisms that are actually inferable from their
observable behavior.  Finally, by providing a uniform representation for a
variety of linguistic theories, it offers a framework for comparing their
consequences.  Ultimately it has the potential to reduce distinctions between
the mechanisms underlying those theories to distinctions between the properties
of the sets of structures they license.  In this way one might hope to
illuminate the empirical consequences of these distinctions, should any, in
fact, exist.

Blackburn, Patrick, Claire Gardent, and Wilfried Meyer-Viol.
1993.
Talking about trees.
In EACL 93, pages 21-29. European Association for
  Computational Linguistics.

Blackburn, Patrick and Wilfried Meyer-Viol.
1994.
Linguistics, logic, and finite trees.
Bulletin of the IGPL, 2(1):3-29, March.

Bchi, J. R.
1960.
Weak second-order arithmetic and finite automata.
Zeitschrift fr mathematische Logik und Grundlagen der
  Mathematik, 6:66-92.

Carpenter, Bob.
1992.
The Logic of Typed Feature Structures; with Applications to
  Unification Grammars, Logic Programs and Constraint Resolution.
Number 32 in Cambridge Tracts in Theoretical Computer Science.
  Cambridge University Press.

Cornell, Thomas Longacre.
1992.
Description Theory, Licensing Theory, and Principle-Based
  Grammars and Parsers.
Ph.D. thesis, University of California Los Angeles.

Dawar, Anuj and K. Vijay-Shanker.
1990.
An interpretation of negation in feature structure descriptions.
Computational Linguistics, 16(1):11-21.

Doner, John.
1970.
Tree acceptors and some of their applications.
Journal of Computer and System Sciences, 4:406-451.

Gazdar, Gerald, Ewan Klein, Geoffrey Pullum, and Ivan Sag.
1985.
Generalized Phrase Structure Grammar.
Harvard University Press.

Gazdar, Gerald, Geoffrey Pullum, Robert Carpenter, Ewan Klein, T. E. Hukari,
  and R. D. Levine.
1988.
Category structures.
Computational Linguistics, 14:1-19.

Gorn, Saul.
1967.
Explicit definitions and linguistic dominoes.
In John F. Hart and Satoru Takasu, editors, Systems and Computer
  Science, Proceedings of the Conference held at Univ. of Western Ontario,
  1965. Univ. of Toronto Press.

Gurevich, Yuri.
1988.
Logic and the challenge of computer science.
In E. Brger, editor, Current Trends in Theoretical Computer
  Science. Computer Science Press, chapter 1, pages 1-57.

Immerman, Neil.
1989.
Descriptive and computational complexity.
In Proceedings of Symposia in Applied Mathematics, pages
  75-91. American Mathematical Society.

Johnson, David E. and Paul M. Postal.
1980.
Arc Pair Grammar.
Princeton University Press, Princeton, New Jersey.

Johnson, Mark.
1988.
Attribute-Value Logic and the Theory of Grammar.
Number 16 in CSLI Lecture Notes. Center for the Study of Language and
  Information, Stanford, CA.

Johnson, Mark.
1989.
The use of knowledge of language.
Journal of Psycholinguistic Research, 18(1):105-128.

Kasper, Robert T. and William C. Rounds.
1986.
A logical semantics for feature structures.
In Proceedings of the 24th Annual Meeting of the Association for
  Computational Linguistics.

Kasper, Robert T. and William C. Rounds.
1990.
The logic of unification in grammar.
Linguistics and Philosophy, 13:35-58.

Keller, Bill.
1993.
Feature Logics, Infinitary Descriptions and Grammar.
Number 44 in CSLI Lecture Notes. Center for the Study of Language and
  Information.

Kracht, Marcus.
1995.
Syntactic codes and grammar refinement.
Journal of Logic, Language, and Information, 4:41-60.

Manzini, Maria Rita.
1992.
Locality: A Theory and Some of Its Empirical Consequences.
MIT Press, Cambridge, Ma.

Moshier, M. Drew and William C. Rounds.
1987.
A logic for partially specified data structures.
In ACM Symposium on the Principles of Programming Languages.

Rizzi, Luigi.
1990.
Relativized Minimality.
MIT Press.

Rogers, James.
1994.
Studies in the Logic of Trees with Applications to Grammar
  Formalisms.
Ph.D. dissertation, Univ. of Delaware.

Rogers, James.
1995.
On descriptive complexity, language complexity, and GB.
In Patrick Blackburn and Maarten de Rijke, editors, Specifying
  Syntactic Structures. In Press.
Also available as IRCS Technical Report 95-14. cmp-lg/9505041.

Rogers, James.
1996a.
A Descriptive Approach to Language-Theoretic Complexity.
Studies in Logic, Language, and Information. CSLI Publications.
To appear.

Rogers, James.
1996b.
The descriptive complexity of local, recognizable, and generalized
  recognizable sets.
Technical report, IRCS, Univ. of Pennsylvania.
In Preparation.

Rogers, James.
1996c.
Grammarless phrase-structure grammar.
Under Review.

Rogers, James and K. Vijay-Shanker.
1994.
Obtaining trees from their descriptions: An application to
  tree-adjoining grammars.
Computational Intelligence, 10:401-421.

Smolka, Gert.
1989.
A feature logic with subsorts.
LILOG Report 33, IBM Germany, Stuttgart.

Stabler, Jr., Edward P.
1992.
The Logical Approach to Syntax.
Bradford.

  This 
notion of constraint-based includes not only the obvious
formalisms, but the formal framework of GB as well.
  Whether there are theories
that cannot be captured, at least without explicitly encoding the derivations,
is an open question of considerable theoretical interest, as is the question of
what empirical consequences such an essential dynamic character might have.
  We will refer to  as
GKPS
  We should note that the definition of FSDs
in GKPS is, in fact, declarative although this is obscured by the fact that
it is couched in terms of an algorithm for checking models.
  There is reason to believe that this
hierarchy can be extended to encompass, at least, a variety of mildly
context-sensitive languages as well.
  A more complete treatment of GPSG in 

can be found in .
  We will not elaborate here on the encoding of
categories in 

,
nor on non-finite ID schema like the iterating
co-ordination schema.  These present no significant problems.
  We could, of course,
skip the definition of 


and define 


as

,
but we prefer to emphasize the inductive
nature of the definition.
  More detailed
expositions of the interpretation of GB in 


can be found in
, , and .
